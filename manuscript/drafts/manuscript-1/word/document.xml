<?xml version="1.0" encoding="UTF-8"?>
<w:document xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" xmlns:pic="http://schemas.openxmlformats.org/drawingml/2006/picture" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing"><w:body><w:p><w:pPr><w:pStyle w:val="Title" /></w:pPr><w:r><w:t xml:space="preserve">Active</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">learning</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">for</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">efficient</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">systematic</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">reviews</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">-</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Subtitle" /></w:pPr><w:r><w:t xml:space="preserve">Evaluating</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">models</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">across</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">research</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">areas</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Author" /></w:pPr><w:r><w:t xml:space="preserve">Gerbrich</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Ferdinands</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Date" /></w:pPr><w:r><w:t xml:space="preserve">11</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">May,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">2020</w:t></w:r></w:p><w:p><w:r><w:br w:type="page" /></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="20" w:name="abstract" /><w:r><w:t xml:space="preserve">Abstract</w:t></w:r><w:bookmarkEnd w:id="20" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading4" /></w:pPr><w:bookmarkStart w:id="21" w:name="background" /><w:r><w:t xml:space="preserve">Background</w:t></w:r><w:bookmarkEnd w:id="21" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading4" /></w:pPr><w:bookmarkStart w:id="22" w:name="methods" /><w:r><w:t xml:space="preserve">Methods</w:t></w:r><w:bookmarkEnd w:id="22" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading4" /></w:pPr><w:bookmarkStart w:id="23" w:name="results" /><w:r><w:t xml:space="preserve">Results</w:t></w:r><w:bookmarkEnd w:id="23" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading4" /></w:pPr><w:bookmarkStart w:id="24" w:name="conclusions" /><w:r><w:t xml:space="preserve">Conclusions</w:t></w:r><w:bookmarkEnd w:id="24" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading4" /></w:pPr><w:bookmarkStart w:id="25" w:name="keywords" /><w:r><w:t xml:space="preserve">Keywords</w:t></w:r><w:bookmarkEnd w:id="25" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Machine learning, Active learning, Systematic Review, Study Selection, Text classification, Text representation,</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="26" w:name="background-1" /><w:r><w:t xml:space="preserve">Background</w:t></w:r><w:bookmarkEnd w:id="26" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Systematic reviews are top of the bill in research. A systematic review brings together all studies relevant to answer a specific research question</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-PRISMA-PGroup2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Systematic reviews inform practice and policy</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Gough2002"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">2</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and are key in developing clinical guidelines</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Chalmers2007"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">3</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. However, systematic reviews are costly because they involve the manual screening of thousands of titles and abstracts to identify publications relevant to answering the research question.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Conducting a systematic review typically requires over a year of work by a team of researchers</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Borah2017"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">4</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Nevertheless, systematic reviewers are often bound to a limited budget and timeframe. Currently, the demand for systematic reviews exceeds the available time and resources by far</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Lau2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">5</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Especially when the need for guidelines is urgent - such as in the context of the current COVID-19 crisis - it is almost impossible to provide a review that is both timely and comprehensive.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">To ensure a timely review, reducing workload in systematic reviews is essential. With advances in Machine Learning (ML), there has been wide interest in tools to reduce workload in systematic reviews</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Harrison2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">6</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Various learning models have been proposed, aiming to predict whether a given publication is relevant or irrelevant to the systematic review. Findings suggest that such models potentially reduce workload with 30-70% at the cost of losing 5% of relevant publications, i.e. 95% recall</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-OMara-Eves2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">7</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">A well-established approach in increasing efficiency in title and abstract screening is screening prioritization</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2009"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">8</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Shemilt2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">9</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. In screening prioritization, the learning model reorders publications to be screened by their likeliness to be relevant. The model presents the reviewer with the publications which are most likely to be relevant first, thereby expediting the process of finding all of the relevant publications. Such an approach allows for substantial time-savings in the screening process as the reviewer can decide to stop screening after a sufficient number of relevant publications have been retrieved</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Moreover, reviewing relevant publications early facilitates a faster transition of those publications to the next steps in the review process</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2009"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">8</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Recent studies have demonstrated the effectiveness of screening prioritization by means of active learning models</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">–</w:t></w:r><w:hyperlink w:anchor="ref-Gates2018a"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">16</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. With active learning, the machine learning model can iteratively improve its predictions on unlabelled data by allowing the model to select the records from which it wants to learn</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Settles2012"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">17</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. The model queries these records to a human annotator who provides them with a label, from which the model then updates its predictions. The general assumption is that by letting the model select which records are labelled, the model can achieve higher accuracy while requiring the human annotator to label as few records as possible</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Settles2009"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">18</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Active learning has proven to be an efficient strategy in large unlabelled datasets where labels are expensive to obtain</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Settles2009"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">18</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, which makes the screening phase in systematic reviewing an ideal candidate for such models because labelling the typical large number of publications is very costly. When active learning is applied in the screening phase, the reviewer screens publications that are selected by an active learning model. Subsequently, the active learning model learns from the reviewers’ decision (</w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">relevant</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">irrelevant</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve">) and uses this knowledge to update its predictions and to select the next publication to be screened by the reviewer.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The application of active learning models in reducing workload of systematic reviews has been extensively studied</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">–</w:t></w:r><w:hyperlink w:anchor="ref-Miwa2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">12</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2010"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Gates2018a"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">16</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Whilst previous studies have evaluated active learning models in many forms and shapes</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">–</w:t></w:r><w:hyperlink w:anchor="ref-Miwa2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">12</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2010"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, all studies used the same classification technique, namely Support Vector Machine. Findings from outside the field of active learning show that different classification techniques can serve different needs in the retrieval of relevant publications, for example recall versus precision</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Kilicoglu2009"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">19</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Aphinyanaphongs2004"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">20</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Therefore, it is essential to evaluate different classification techniques in the context of active learning models.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Moreover, another component known to influence performance of the models is the way how the textual content of titles and abstracts are represented in a model, called the feature extraction strategy</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Le2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">21</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Zhang2011"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">22</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Previous studies all adopt the widely used</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">bag of words</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">strategy</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">–</w:t></w:r><w:hyperlink w:anchor="ref-Miwa2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">12</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2010"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. It is of interest to evaluate models using this effective but rather simplistic approach by comparing them to models adopting a more sophisticated strategy, called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">Doc2vec</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Le2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">21</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Lastly, previous studies have mainly focussed on reviews from a single scientific field, like medicine</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2010"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Gates2018a"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">16</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and software engineering</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Model replications on reviews from varying research contexts are essential to draw conclusions about the general effectiveness of active learning models</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-OMara-Eves2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">7</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Marshall2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">23</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. As far as known to the authors, Miwa et al</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Miwa2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">12</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">were the only researchers to make a direct comparison between two systematic reviews from different research areas, namely the social and the medical sciences. They found that active learning was more difficult on data from the social sciences due to the nature of the different vocabularies used. Therefore, it is of interest to evaluate model performance across different research contexts. Taken together, evaluations of active learning models in the context of systematic reviewing are required (1) across different classification techniques, (2) feature extraction strategies, and (3) review contexts. The current study aims to address these issues by answering the following research questions:</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">   </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">RQ1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">What is the performance of several active learning models across different classification techniques?</w:t></w:r><w:r><w:br /></w:r><w:r><w:t xml:space="preserve">   </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">RQ2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">What is the performance of several active learning models across different feature extraction strategies?</w:t></w:r><w:r><w:br /></w:r><w:r><w:t xml:space="preserve">   </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">RQ3</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Does the performance of active learning models differ across systematic reviews from different research areas?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The purpose of the current paper is to increase the evidence base on active learning models for reducing workload in title and abstract screening in systematic reviews. We adopt four different classification techniques (Naive Bayes, Linear Regression, Support Vector Machine, and Random Forest) and two different feature extraction strategies (TF-IDF and Doc2vec) for the purpose of maximizing the number of identified relevant publications, while minimizing the number of publications needed to screen. Model performance was assessed by conducting a retrospective simulation on six systematic review datasets. Datasets were collected from the fields of medicine</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2006"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">24</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Appenzeller-Herzog2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">25</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, software engineering</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, psychology</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-vandeSchoot2017"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">26</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, behavioural public administration</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Nagtegaal2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">27</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and virology</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Kwok2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">28</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to assess generalizability of the models across research contexts. The models, datasets and simulations are implemented in a pipeline of active learning for screening prioritization, called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>ASReview</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-ASReview2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">29</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>ASReview</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is an open source and generic tool such that users can adapt and add modules as they like, encouraging fellow researchers to replicate findings from previous studies. All scripts and data used are openly published to facilitate usability and acceptability of ML-assisted title and abstract screening in the field of systematic review.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The remaining part of this paper is organized as follows. The Technical details section elaborates on the characteristics of active learning models for identifying relevant publications in the context of systematic reviews. The Simulation study section describes the study that was designed to answer the research questions. The findings of the simulation study are reported in the Results section. The implications of the findings in context of previous research are discussed in the Discussion section, followed by this study’s main conclusions in the Conclusion section.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="27" w:name="technical-details" /><w:r><w:t xml:space="preserve">Technical details</w:t></w:r><w:bookmarkEnd w:id="27" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">What follows is a more detailed account of the active learning models. The structure and functions of the key components of the models are introduced to clarify the choices made in the design of the current study.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="28" w:name="task-description" /><w:r><w:t xml:space="preserve">Task description</w:t></w:r><w:bookmarkEnd w:id="28" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The screening process of a systematic review starts with all publications obtained in the search. The task is to identify which of these publications are relevant, by screening them at the title and abstract level. In active learning for screening prioritization, the screening process proceeds as follows:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">Start with the set of all unlabeled records (titles and abstracts),</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>U</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">The reviewer provides a label for a few, for example 5-10, records</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>x</m:t></m:r><m:r><m:t>∈</m:t></m:r><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>U</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">, creating an set of labeled records</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>L</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">. The label can be either Relevant</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>⟨</m:t></m:r><m:r><m:t>x</m:t></m:r><m:r><m:t>,</m:t></m:r><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>R</m:t></m:r><m:r><m:t>⟩</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or Irrelevant</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>⟨</m:t></m:r><m:r><m:t>x</m:t></m:r><m:r><m:t>,</m:t></m:r><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>I</m:t></m:r><m:r><m:t>⟩</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">The active learning cycle starts:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">A classifier,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>C</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">, is trained on the labeled records</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>L</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>C</m:t></m:r><m:r><m:t>=</m:t></m:r><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>train</m:t></m:r><m:r><m:t>(</m:t></m:r><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>L</m:t></m:r><m:r><m:t>)</m:t></m:r></m:oMath></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">The classifier predicts relevancy scores for all unlabelled records</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>U</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>C</m:t></m:r><m:r><m:t>(</m:t></m:r><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>U</m:t></m:r><m:r><m:t>)</m:t></m:r></m:oMath></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">Based on the predictions by</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>C</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">, the model selects the most relevant record</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:sSup><m:e><m:r><m:t>x</m:t></m:r></m:e><m:sup><m:r><m:t>*</m:t></m:r></m:sup></m:sSup><m:r><m:t>∈</m:t></m:r><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>U</m:t></m:r></m:oMath></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">The model queries the reviewer to screen this record,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>⟨</m:t></m:r><m:sSup><m:e><m:r><m:t>x</m:t></m:r></m:e><m:sup><m:r><m:t>*</m:t></m:r></m:sup></m:sSup><m:r><m:t>,</m:t></m:r><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>?</m:t></m:r><m:r><m:t>⟩</m:t></m:r></m:oMath></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">The reviewer screens the record and provides a label,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>⟨</m:t></m:r><m:sSup><m:e><m:r><m:t>x</m:t></m:r></m:e><m:sup><m:r><m:t>*</m:t></m:r></m:sup></m:sSup><m:r><m:t>,</m:t></m:r><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>R</m:t></m:r><m:r><m:t>⟩</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>⟨</m:t></m:r><m:sSup><m:e><m:r><m:t>x</m:t></m:r></m:e><m:sup><m:r><m:t>*</m:t></m:r></m:sup></m:sSup><m:r><m:t>,</m:t></m:r><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>I</m:t></m:r><m:r><m:t>⟩</m:t></m:r></m:oMath></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">The newly labeled record is added to the training data, such that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>x</m:t></m:r><m:r><m:t>∈</m:t></m:r><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>L</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>x</m:t></m:r><m:r><m:t>∉</m:t></m:r><m:r><m:rPr><m:sty m:val="p" /><m:scr m:val="script" /></m:rPr><m:t>U</m:t></m:r></m:oMath></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="1" /><w:numId w:val="1002" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">Back to step 1.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In this active learning cycle, the model can incrementally improve its predictions on the remaining unlabeled title and abstracts. The relevant titles and abstracts are identified as early in the process as possible. The reviewer and the model keep interacting until the reviewer decides to stop or until all records been labelled.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="29" w:name="class-imbalance-problem" /><w:r><w:t xml:space="preserve">Class imbalance problem</w:t></w:r><w:bookmarkEnd w:id="29" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">There are two classes in the dataset: relevant and irrelevant publications. Typically, the inclusion rate is low as only a fraction of the publications belong to the relevant class (2.94%</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Borah2017"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">4</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">). The class imbalance causes the classifier to miss relevant publications, because there are far fewer examples of relevant than irrelevant publications to train on</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-OMara-Eves2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">7</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Moreover, classifiers can achieve high accuracy but still fail to identify any of the relevant publications</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2010"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. This is evident in the case of a systematic review dataset where only three percent of publications are relevant. A model would achieve 97% accuracy when classifying all publications as irrelevant, even though none of the relevant papers would have been correctly identified.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Previous studies have addressed the class imbalance problem by rebalancing the training data in different ways</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-OMara-Eves2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">7</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. To decrease the class imbalance in the training data, the models in the current study rebalances the training set by Dynamic Resampling (DR). DR undersamples the number of irrelevant publications in the training data, whereas the number of relevant publications are oversampled such that the size of the training data remains the same. The ratio between relevant and irrelevant publications in the rebalanced training data is not fixed, but dynamically updated depending on the number of publications in the available training data, the number of publications in the total data, and the ratio between relevant and irrelevant publications in the available training data.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="30" w:name="classification" /><w:r><w:t xml:space="preserve">Classification</w:t></w:r><w:bookmarkEnd w:id="30" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">To make predictions on the unlabeled publications, a classifier is trained on features from the set of previously labeled publications.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">A technique widely used in classification tasks is the Support Vector Machine (SVM). SVMs separate the data into classes by finding a multidimensional hyperplane</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Tong2001"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">30</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Kremer2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">31</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. SVMs have been proven to be effective in active learning models for screening prioritization [</w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">; Miwa2014]. Moreover, SVMs are the currently the only classifier implemented in ready-to-use software tools implementing active learning for screening prioritization (Abstrackr</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2012b"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">32</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, Colandr</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cheng2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">33</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, FASTREAD</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, Rayyan</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Ouzzani2016"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">34</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, and RobotAnalyst</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Przybyla2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">35</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Whilst the performance of several classification techniques has been investigated in the ML-aided title-and-abstract screening field in general</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Kilicoglu2009"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">19</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Aphinyanaphongs2004"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">20</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, the relatively new subfield of active learning for screening prioritization has not yet studied the performance of classifiers other than SVMs</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">–</w:t></w:r><w:hyperlink w:anchor="ref-Wallace2010"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">The current study aims to address this gap by exploring performance of three classifiers besides SVM:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1003" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">L2-regularized Logistic Regression (LR) models the probabilities describing the possible outcomes by a logistic function. The L2 penalty is imposed on the coefficients to reduce the number of features upon which the given solution is dependent</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-scikit-learn"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">36</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1003" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">Naive Bayes (NB) is a supervised learning algorithm often used in text classification. Based on Bayes’ Theorem, with the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">naive</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">assumption that all features are independent given the class value</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Zhang2004"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">37</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1003" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">Random Forests (RF) is a supervised learning algorithm where a large number of decision trees are fit on bootstrapped samples of the original data. All trees cast a vote on the class, which are aggregated into a class prediction for each instance</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Breiman2001"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">38</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">These three classification techniques were selected because they are widely adopted methods in text classification</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Aggarwal2012"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">39</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Moreover, these techniques can be run on a personal computer as they require a relatively low amount of processing power.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="31" w:name="feature-extraction" /><w:r><w:t xml:space="preserve">Feature extraction</w:t></w:r><w:bookmarkEnd w:id="31" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">To predict publication class, the classifier uses information from the publications in the dataset. Examples of such information are titles and abstracts. However, a model cannot make predictions from the titles and abstracts as they are; their textual content needs to be represented numerically. The textual information needs to be mapped to feature vectors. This process of numerically representing textual content is referred to as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">feature extraction</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">A classical example of feature extraction is a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">bag of words</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(bow) representation. For each text in the data set, the term frequency - the number of occurrences of each word - is stored. This leads to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>n</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">features, where</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>n</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is the number of distinct words in the texts</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-scikit-learn"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">36</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. A serious weakness of this method is that it highly values often occurring but otherwise meaningless words such as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">the</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r><w:r><w:t xml:space="preserve">. A more sophisticated bow approach is Term-Frequency Inverse Document Frequency (TF-IDF), which circumvents this problem by adjusting the term frequency in a text with the inverse document frequency, the frequency of a given word in the entire data set</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Ramos2003"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">40</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. A downside of TF-IDF and other bow methods is that they do not take into account the ordering of words, thereby ignoring semantics. An example of an approach that aims to overcome this weakness is Doc2vec (D2V). Doc2vec represents texts by a neural network, capable of grasping semantics by learning to predict the words in the texts</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Le2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">21</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="32" w:name="query-strategy" /><w:r><w:t xml:space="preserve">Query strategy</w:t></w:r><w:bookmarkEnd w:id="32" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The active learning model can adopt different strategies in selecting the next publication to be screened by the reviewer. A strategy mentioned before is selecting the publication with the highest probability of being relevant. In the active learning literature this is referred to as certainty-based active learning</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Settles2012"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">17</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Another well-known strategy is uncertainty-based active learning, where the instances that are presented next are those instances on which the model’s classifications are the least certain, i.e. close to 0.5 probability</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Settles2012"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">17</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Traditionally, this strategy trains the most accurate model because the model can learn the most from instances it is uncertain about. However, a study comparing performance of both strategies in detecting relevant publications found that the accuracy gain of uncertainty-based screening was not significant</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Miwa2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">12</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Certainty-based active learning is the preferred strategy for the task at hand. Firstly, this strategy is far better suited to the goal of prioritizing relevant publications compared to uncertainty-based active learning, in which the publications are prioritized that the model is most uncertain about. Secondly, certainty-based active learning is far better equipped at dealing with imbalanced data in active learning</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Fu2011"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">41</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="33" w:name="simulation-study" /><w:r><w:t xml:space="preserve">Simulation study</w:t></w:r><w:bookmarkEnd w:id="33" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The section below describes the simulation study that was carried out to answer the research questions.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="34" w:name="set-up" /><w:r><w:t xml:space="preserve">Set-up</w:t></w:r><w:bookmarkEnd w:id="34" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">To address</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">RQ1</w:t></w:r><w:r><w:t xml:space="preserve">, four models combining every classifier with TF-IDF feature extraction were investigated:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">SVM + TF-IDF</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">NB + TF-IDF</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">RF + TF-IDF</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">LR + TF-IDF</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">To address</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">RQ2</w:t></w:r><w:r><w:t xml:space="preserve">, the classifiers were combined with Doc2vec feature extraction, leading to the following three models:</w:t></w:r><w:r><w:rPr><w:rStyle w:val="FootnoteReference" /></w:rPr><w:footnoteReference w:id="35" /></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1005" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">SVM + Doc2vec</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1005" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">RF + Doc2vec</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1005" /></w:numPr></w:pPr><w:r><w:t xml:space="preserve">LR + Doc2vec</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Performance of the seven models was evaluated by simulating every model on six systematic review datasets, addressing</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">RQ3</w:t></w:r><w:r><w:t xml:space="preserve">. Hence, 42 simulations were carried out, representing all model-dataset combinations. For every simulation, hyperparameters were optimized through a random search to arrive at maximum model performance. To account for variance, every simulation was repeated for 15 trials. Simulations were run using</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">’s simulation mode</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-ASReview2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">29</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. There was no need for a human reviewer as the model could query the labels in the data instead.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Every simulation started with an initial training set of one relevant and one irrelevant publication to represent a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">worst case scenario</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">where the reviewer has minimal prior knowledge on the publications in the data. To account for bias due to the content of the initial publications, the initial training set was randomly sampled from the dataset for every of the 15 trials. Although varying over trials, the initial training sets were kept constant over datasets to allow for a direct comparison of models within datasets. A seed value was set to ensure reproducibility. The classifier was retrained every time after a publication had been labeled. The simulation ended after all publications in the dataset had been labeled.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Simulations were run in ASReview, version 0.9.3</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-ASReview2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">29</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Analyses were carried out using R, version 3.6.1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-RCoreTeam2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">42</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. All scripts and data are stored in the supplementary material. This study was approved by the Ethics Committee of the Faculty of Social and Behavioural Sciences of Utrecht University, filed as an amendment under study 20-104. Due to their large number, the simulations were carried out on Cartesius, the Dutch national supercomputer. Access was granted by SURF via a grant (ID EINF-156).</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="38" w:name="datasets" /><w:r><w:t xml:space="preserve">Datasets</w:t></w:r><w:bookmarkEnd w:id="38" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The models were simulated on a convenience sample of six systematic review datasets. The data selection process was driven by two factors. Firstly, datasets were selected based on their background, given the need for datasets from diverse research areas. Secondly, datasets were selected by their availability, given the limited timespan of the current project. Thirdly, all original data files should be openly published with a CC-BY license, and are available through the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId39"><m:oMath><m:r><m:rPr><m:nor /><m:sty m:val="p" /><m:scr m:val="monospace" /></m:rPr><m:t>ASReview</m:t></m:r></m:oMath><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">GitHub page</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Datasets were collected from the fields of medicine, virology, software engineering, behavioural public administration, and pyschology to assess generalizability of the models across research contexts. The Wilson dataset</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Appenzeller-Herzog2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">43</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is on a review on effectiveness and safety of treatments of Wilson Disease, a rare genetic disorder of copper metabolism</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Appenzeller-Herzog2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">25</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. The Ace dataset contains publications on the efficacy of Angiotensin-converting enzyme (ACE) inhibitors, a drug treatment for heart disease</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2006"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">24</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. The Virus dataset is from a systematic review on studies that performed viral Metagenomic Next-Generation Sequencing (mNGS) in farm animals</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Kwok2020"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">28</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. From the field of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">software engineering</w:t></w:r><w:r><w:t xml:space="preserve">, the Software dataset contains publications from a review on fault prediction in source code</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Hall2012"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">44</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. The Nudging dataset</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Nagtegaal2019a"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">45</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">belongs to a systematic review on nudging healthcare professionals</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Nagtegaal2019"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">27</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, stemming from the area of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">behavioural public administration</w:t></w:r><w:r><w:t xml:space="preserve">. The PTSD dataset contains publications from the field of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">psychology</w:t></w:r><w:r><w:t xml:space="preserve">. The corresponding systematic review is on studies applying latent trajectory analyses on posttraumatic stress after exposure to trauma</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-vandeSchoot2017"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">26</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Of these six datasets, Ace, and Software have been used for model simulations in previous studies on ML-aided title and abstract screening, respectively</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2006"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">24</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Data were preprocessed from their original source into a test dataset, containing title and abstract of the publications obtained in the initial search. Candidate studies with missing abstracts and duplicate instances were removed from the data. Preprocessing scripts and resulting datasets can be found in the supplementary material. Test datasets were labelled to indicate which candidate studies were included in the systematic review, thereby indicating relevant publications. All test datasets consisted of thousands of candidate studies, of which only only a fraction was deemed relevant to the systematic review. For the Virus and the Nudging dataset, the inclusion rate was about 5 percent. For the remaining six datasets, inclusion rates were centered around 1-2 percent. (Table 1).</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="40" w:name="evaluating-performance" /><w:r><w:t xml:space="preserve">Evaluating performance</w:t></w:r><w:bookmarkEnd w:id="40" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Model performance was assessed by three different measures, Work Saved over Sampling (WSS), Relevant References Found (RRF), and Average Time to Discovery (ATD).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">WSS indicates the reduction in publications needed to be screened, at a given level of recall</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2006"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">24</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Typically measured at a recall level of 0.95</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2006"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">24</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">yields an estimate of the amount of work that can be saved at the cost of failing to identify 5% of relevant publications. In the current study, WSS is computed at 0.95 recall. RRF statistics are computed at 10%, representing the proportion of relevant publications that are found after screening 10% of all publications.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Both RRF and WSS are sensitive to random effects as these statistics are strongly dependent on the position of the cutoff value. Moreover, WSS makes assumptions about acceptable recall levels whereas this level might depend on the research question at hand</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-OMara-Eves2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">7</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. A statistic that is not dependent on some arbitrary cutoff value is the ATD, which is the average number of publications needed to screen to find a relevant publication.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Furthermore, model performance was visualized by plotting recall curves. Plotting recall as a function of the proportion of screened publications offers insight in model performance throughout the entire screening process</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Cormack2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">13</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. The curves give information in two directions. On the one hand they display the number of publications that need to be screened to achieve a certain level of recall (1-WSS), but on the other hand they present how many relevant publications are identified after screening a certain proportion of all publications (RRF). Moreover, the recall curves relate to the ATD in such a way that the area above the curve is equal to the ATD.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">For every simulation, the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, and ATD are reported as means over 15 trials. To indicate the spread of performance within simulations, the means are accompanied by an estimated standard devation</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:acc><m:accPr><m:chr m:val="̂" /></m:accPr><m:e><m:r><m:t>s</m:t></m:r></m:e></m:acc></m:oMath><w:r><w:rPr><w:rStyle w:val="FootnoteReference" /></w:rPr><w:footnoteReference w:id="43" /></w:r><w:r><w:t xml:space="preserve">. To compare overall performance across datasets, median performance is reported for every dataset, accompanied by the Median Absolute Deviation (MAD), indicating variability between models within a certain dataset. Recall curves are plot for every simulation, representing the average recall over 15 trials</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>±</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">the standard error of the mean.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="44" w:name="results-1" /><w:r><w:t xml:space="preserve">Results</w:t></w:r><w:bookmarkEnd w:id="44" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This section proceeds as follows. Firstly, the results of the Nudging dataset are discussed in detail to provide a basis for answering the research questions. Secondly, the results are presented for each research question over all datasets.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="45" w:name="evaluation-on-the-nudging-dataset" /><w:r><w:t xml:space="preserve">Evaluation on the Nudging dataset</w:t></w:r><w:bookmarkEnd w:id="45" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Figure 1a shows the recall curves for all simulations on the Nudging dataset. As desribed in the previous section, these curves plot recall as a function of the proportion of publications screened. The curves represent the average recall over 15 trials</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>±</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">the standard error of the mean in the direction of the y-axis. The x-axis is cut off at 40% since all for simulations, the models reached 95% recall after screening 40% of the publications. The dashed horizontal lines indicate the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values, the dashed vertical lines the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values. The recall curves relate to the ATD such that ATD is equal to the area above the curve. The dashed grey diagonal line corresponds to the expected recall curve when publications are screened in a random order. Desirable model performance was defined as maximizing recall while minimizing the number of publications needed to screen.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The recall curves were used to examine model performance throughout the entire screening process and to make a visual comparison between models within datasets. For example in Figure 1a, after screening about 30% of the publications all models had already found 95% of the relevant publications. Moreover, after screening 5% the green curve - representing the RF + TF-IDF model - splits away from the others and remains to be the lowest of all curves until about 30% of publications have been screened. Hence, from screening 5 to 30 percent of publications, the RF + TF- IDF model was the slowest in finding the relevant publications. The ordering of the remaining recall curves changes throughout the screening process, but maintain to show relatively similar performance at face value.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Figure 1b shows a subset of the recall curves in Figure 1a, namely the curves for the first four models only to allow for a visual comparison across classification techniques adopting the TF-IDF feature extraction strategy. Figure 1c shows recall curves for the remaining three models to compare the models using Doc2vec feature extraction. Figures 1d-f plot recall curves for models adopting the TF-IDF feature extraction strategy to recall curves for their Doc2vec-using counterparts to allow for a comparison between models adopting TF-IDF and models adopting Doc2vec feature extraction.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Nudging dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/nudging/inclusion_all_nudging.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId46" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Nudging dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/nudging/inclusion_tfidf_nudging.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId47" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Nudging dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/nudging/inclusion_d2v_nudging.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId48" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Nudging dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/nudging/inclusion_d2v_vs_tfidf_logistic_nudging.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId49" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Nudging dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/nudging/inclusion_d2v_vs_tfidf_rf_nudging.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId50" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Nudging dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/nudging/inclusion_d2v_vs_tfidf_svm_nudging.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId51" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">It can be seen from the data in the first column of Table 2 that in terms of ATD, the best performing models on the Nudging dataset were SVM + D2V and LR + D2V, both with an ATD of 8.9%. This indicates that the average proportion of publications needed to screen to find a relevant publication was 8.9% for both models. In the SVM + D2V model, the standard deviation was 0.33 , whereas for the LR + D2V model</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:acc><m:accPr><m:chr m:val="̂" /></m:accPr><m:e><m:r><m:t>s</m:t></m:r></m:e></m:acc><m:r><m:t>=</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">0.47. This indicates that for the SVM + D2V model, the ATD values of individual trials were closer to the overal mean compared to the LR + D2V model, meaning that the SVM + D2V model performed more stable across different initial training datasets. Median ATD for this dataset was 9.6% with an MAD of 1.06, indicating that for half of the models, the ATD was within 1.06 distance from the median ATD.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">As Table 3 shows, the highest</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">value on the Nudging dataset was achieved by the NB + TF-IDF model with a mean of 71.7, meaning that this model reduced the number of publications needed to screen with 71.7% at the cost of losing 5% of relevant publications. The estimated standard deviation of 1.37 indicates that in terms of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, this model performed the most stable across trials. The model with the lowest</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">value was RF + TF-IDF (</w:t></w:r><m:oMath><m:bar><m:barPr><m:pos m:val="top" /></m:barPr><m:e><m:r><m:t>x</m:t></m:r></m:e></m:bar></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">= 64.9,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:acc><m:accPr><m:chr m:val="̂" /></m:accPr><m:e><m:r><m:t>s</m:t></m:r></m:e></m:acc><m:r><m:t>=</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">2.50). Median</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of these models was 66.9%, with a MAD of 3.05%, indicating that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values of models varied the most within this dataset.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">As can be seen from the data in Table 4, LR + D2V was the best performing model in terms of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, with a mean of 67.5 indicating that after screening 10% of publications, on average 67.5% of all relevant publications had been identified, with a standard deviation of 2.59. The worst performing model was RF + TF-IDF (</w:t></w:r><m:oMath><m:bar><m:barPr><m:pos m:val="top" /></m:barPr><m:e><m:r><m:t>x</m:t></m:r></m:e></m:bar><m:r><m:t>=</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">53.6,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:acc><m:accPr><m:chr m:val="̂" /></m:accPr><m:e><m:r><m:t>s</m:t></m:r></m:e></m:acc><m:r><m:t>=</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">2.71). Median performance was 62.6, with an MAD of 3.89 indicating again that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values were most dispersed for models within this dataset.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="52" w:name="overall-evaluation" /><w:r><w:t xml:space="preserve">Overall evaluation</w:t></w:r><w:bookmarkEnd w:id="52" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Recall curves for the simulations on the five remaining datasets are presented in Figure 2. For the sake of brevity, recall curves are only plotted once per dataset, like in Figure 1a. Please refer to the supplementary material for figures presenting subsets of recall curves for the remaining datasets, like in Figure 1b-f.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for all seven models on (a) the PTSD, (b) Software, (c) Ace, (d) Virus, and (e) Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_all_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId53" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for all seven models on (a) the PTSD, (b) Software, (c) Ace, (d) Virus, and (e) Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/nolegend_inclusion_all_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId54" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for all seven models on (a) the PTSD, (b) Software, (c) Ace, (d) Virus, and (e) Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/nolegend_inclusion_all_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId55" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for all seven models on (a) the PTSD, (b) Software, (c) Ace, (d) Virus, and (e) Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/nolegend_inclusion_all_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId56" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for all seven models on (a) the PTSD, (b) Software, (c) Ace, (d) Virus, and (e) Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/nolegend_inclusion_all_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId57" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">First of all, for all datasets, the models were able to detect the relevant publications much faster compared to when screening publications at random order as the recall curves exceed the expected recall at screening at random order by far. Even the worst results outperform this reference condition. Across simulations, the ATD was at maximum 11.8% (in the Nudging dataset), the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">at least 63.9% (in the Virus dataset), and the lowest</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">was 53.6% (in the Nudging dataset). Interestingly, all these values were achieved by the RF + TF-IDF model.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Similar to the simulations on the Nudging dataset (Figure 1b), the ordering of recall curves changes throughout the screening process, indicating that model performance is dependent on the number of publications that have been screened. Moreover, the ordering of models in the Nudging dataset (Figure 1b) does not replicate on the remaining five datasets (Figure 2).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:bookmarkStart w:id="58" w:name="Xe2f790c41fef318331be70b9b82bdce2c530a7b" /><w:r><w:t xml:space="preserve">RQ1 - Comparison across classification techniques</w:t></w:r><w:bookmarkEnd w:id="58" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The first reserach question was aimed at evaluating the first four models adopting either the NB, SVM, LR or RF classification technique, combined with TF-IDF feature extraction. When comparing ATD-values of the models (Table 2), the NB + TF-IDF model ranked first in the Ace, Nudging, PTSD, Virus and Wilson dataset, and second in the PTSD and the Software dataset, in which the LR + TF-IDF model achieved the lowest ATD value. The RF + TF-IDF ranked last in all of the datasets except in the Ace dataset, where the SVM + TF-IDF model achieved the highest ATD-value.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Additionally, in terms of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(Table 3) the ranking of models was strikingly similar across all datasets. In the Ace, Nudging, Software, and Virus dataset, the highest</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">value was always achieved by the NB + TF-IDF model, followed by LR + TF-IDF, SVM + TF-IDF, and RF + TF-IDF. In the PTSD dataset this ranking applied as well, except that the LR + TF-IDF and NB + TF-IDF showed equal</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values. The ordering of the models for the Wilson dataset was NB + TF-IDF, RF + TF-IDF, LR + TF-IDF and SVM + TF-IDF.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Moreover, in terms of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(Table 4) the NB + TF-IDF model achieved the highest</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">value in the Ace, Nudging, and Wilson dataset. LR + TF-IDF ranked first in the PTSD dataset, SVM + TF-IDF was the best performing model within the Wilson dataset. The RF + TF-IDF model was again the worst performing model within all datasets, with on exception for the Software dataset. In this dataset, NB + TF-IDF ranked fourth, the remaining three models achieved an equal</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">score.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Taken together, these results show that while all four models perform quite well, the NB + TF-IDF shows high performance on all measures across all datasets, whereas the RF + TF-IDF model never performd best on any of the measures across all datasets.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:bookmarkStart w:id="59" w:name="X81e79a42d834b917c028219891866a99be62e33" /><w:r><w:t xml:space="preserve">RQ2 - Comparison across feature extraction techniques</w:t></w:r><w:bookmarkEnd w:id="59" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The following section is concerned with the question of how models using different feature extraction strategies relate to each other. The recall curves for the Nudging data (Figure 1d-f) show a clear trend of the models adopting Doc2vec feature extraction outperforming their TF-IDF counterparts. This trend also shows from the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values indicated by the vertical and horizontal lines in the figure. Likewise, the ATD values (Table 2) indicate that for the models adopting a particular classification technique, the model adopting Doc2vec feature extraction always achieved a smaller ATD-value than the model adoptin TF-IDF feature extraction.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In contrast, this pattern of models adopting Doc2vec outperforming their TF-IDF counterparts in the Nudging dataset does not replicate across other datasets. Whether evaluated in terms of recall curves,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or ATD, the findings were mixed. Neither one of the feature extraction strategies showed superior performance within certain datasets nor within certain classification techniques.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:bookmarkStart w:id="60" w:name="Xb15e90d7af3f7a7d269c04b5d534d74b17292f4" /><w:r><w:t xml:space="preserve">RQ3 - Comparison across research contexts</w:t></w:r><w:bookmarkEnd w:id="60" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">First of all, models showed much higher performance for some datasets than for others. While performance on the PTSD (Figure 2a) and the Software dataset (Figure 2b) was quite high, performance was much lower across models for the Nudging (Figure 1a) and Virus (Figure 2d) datasets. There does not seem to be a clear distinction between the datasets from the biomedical sciences (Ace, Virus, and Wilson) and datasets from other fields (Nudging, PTSD, Software). This result also holds in terms of the median ATD,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values for these models (Table 2, 3, and 4).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Secondly, variability of model performance differed across datasets. For the PTSD (Figure 2a), Software (Figure 2b), and the Virus (Figure 2d) datasets, recall curves form a tight group meaning that within these datasets, the models perform relatively similar. For the Nudging (Figure 1a), Ace (Figure 2c), and Wilson (Figure 2e) dataset, the recall curves are much further apart, indicating that model performace is much more dependent on the classification technique and feature extraction strategy. The MAD values of the ATD,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId42"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">RRF@10</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">confirm that within the PTSD, Software and Virus datasets, model performance is less spread out than within the Nudging, Ace and Wilson dataset.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Moreover, for some datasets the initial training data set seemed to have more impact on model performance than for others. the Ace and Wilson datasets model performance is more dependent on the initial training data, as these curves curves are wider compared to the other figures.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Overall these results indicate that first of all, model performance was mostly data dependent, however, the ordering of models is fairly similar.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="61" w:name="discussion" /><w:r><w:t xml:space="preserve">Discussion</w:t></w:r><w:bookmarkEnd w:id="61" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The current study set out to evaluate performance of active learning models for the purpose of identifying relevant publications in systematic review datasets.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">This study has been one of the first attempts to examine different classification strategies and feature extraction strategies in active learning models for systematic reviews. Moreover, this study has provided a deeper insight into the perforance of active learning models across research contexts.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Taken together, the findings confirm the great potential of active learning models in reducing workload for systematic reviewers.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">In a previous study, the Ace dataset was used to simulate a model that did not use active learning, finding a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">value of 56.61%</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Cohen2006"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">24</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">, whereas the models in the current study achieved far superior</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values varying from 68.6% to 82.9% in this dataset. Active learning models clearly outperformed a model who did not use active learning. In addition, the Software dataset was used to simulate an active learning model</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Yu2018"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">11</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and reached</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of 91%, strikingly similar the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WSS@95</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">values found in the current study which ranged from 90.5 to 92.3.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:bookmarkStart w:id="62" w:name="classification-techniques" /><w:r><w:t xml:space="preserve">Classification techniques</w:t></w:r><w:bookmarkEnd w:id="62" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The first research question in this study sought to evaluate models adopting different classification techniques. The most obvious finding to emerge from these evaluations was that the NB + TF-IDF model consistently performed as one of the best models. The results suggest that albeit the widely used SVM-classifier performed fairly well, LR and NB classification strategies are interesting, if not superior alternatives to the standard in this field.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:bookmarkStart w:id="63" w:name="feature-extraction-strategy" /><w:r><w:t xml:space="preserve">Feature extraction strategy</w:t></w:r><w:bookmarkEnd w:id="63" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The overall results on models adopting Doc2vec versus TF-IDF feature extraction strategy remain inconclusive. According to these findings, adopting Doc2vec instead of the well-established TF-IDF feature extraction strategy does not lead to better performing models. Given these results, although preliminary, preference goes out to teh TF-IDF feature extraction technique as this relatively simplistic technique will lead to more interpretable model.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:bookmarkStart w:id="64" w:name="research-contexts" /><w:r><w:t xml:space="preserve">Research contexts</w:t></w:r><w:bookmarkEnd w:id="64" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Simulating models on a heterogenous collection of systematic review datasets demonstrated that model performance was very data-dependent. Within some datasets, models achieved much higher overall performance than within other datasets. Moreover, for some datasets, differences between models were much larger than for other datasets. It has been suggested that active learning is more difficult for datasets from the social sciences compared to data from the medical sciences</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Miwa2014"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">12</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. This does not appear to be the case as performance within the biomedical datasets (Wilson, Virus, Ace) was not in any way superior to performance within the datasets from other resesarch areas (PTSD, Software and Nudging). An issue that emerges from these findings is that difficulty of active learning was not confined to any particular research area. A possible explanation for this is that difficulty of active learning could be attributed to factors more directly related to the systematic review at hand, such as the inclusion rate and the complexity of inclusion criteria used to identify relevant publications</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">[</w:t></w:r><w:hyperlink w:anchor="ref-Gates2018a"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">16</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r><w:hyperlink w:anchor="ref-Rathbone2015"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">46</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">]</w:t></w:r><w:r><w:t xml:space="preserve">. Although the current study did not investigate the inclusion criteria of systematic reviews, interestingly, the datasets on which the active learning models performed worst, Nudging and Virus, were also the datasets with the highest inclusion rates, 5.4% and 5.0%, respectively.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="65" w:name="limitations-and-future-research" /><w:r><w:t xml:space="preserve">Limitations and future research</w:t></w:r><w:bookmarkEnd w:id="65" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">When applied in systematic review practice, the succes of active learning models stands or falls down with the generalizability of model performance across unseen datasets. It is important to bear in mind that model hyperparameters were optimized for each model-dataset combination. Thus, the observed results reflect maximum model performance for the datasets at hand. The question remains whether model performance generalizes to datasets for which the hyperparameters were not optimized. Further research should be undertaken to determine the sensitivity of model performance to the hyperparameter values.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In systematic reviews, screening publications is typically a two-step process in which first, titles and abstracts are screened to identify potentially relevant publications, called abstract inclusions. Second, the fulltexts of these publications are read to identify the relevant publications. This implies that the relevant publications are selected based on information that the models do not have. To truly assess the added value of active learning models in title-and-abstract screening, models should be evaluated on their capability of detecting the abstract inclusions instead of relevant publications only. However, this data is typically not available. Hence, greater efforts are needed to provide information on the abstract inclusions in openly published systematic review datasets.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">One unanticipated finding was that the runtime of simulations varied widely across models, indicating that some models need more time to retrain after a publication has been labelled than other models. This finding has important implications for the practical application of such models, as an efficient model should be able to keep up with the decision-making speed of the reviewer. Further studies taking into account retraining time will need to be undertaken.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="66" w:name="conclusions-1" /><w:r><w:t xml:space="preserve">Conclusions</w:t></w:r><w:bookmarkEnd w:id="66" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Overall, the findings of this study confirm that active learning models show great potential of finding relevant publications in a systematic review dataset, while minimizing the number of publications needed to screen. The results shed new light on the performance of different classification techniques, indicating that the Naive Bayes classification technique is superior to the widely used SVM. As model performance differs vastly across datasets, this study raises the question what causes models to yield more workload savings for some systematic review datasets than for others. In order to gain a better understanding of the added value of active learning models in the screening process, it is essential to identify how dataset characteristics relate to model performance.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="67" w:name="list-of-abbreviations" /><w:r><w:t xml:space="preserve">List of abbreviations</w:t></w:r><w:bookmarkEnd w:id="67" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">ML - Machine learning</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="68" w:name="declarations" /><w:r><w:t xml:space="preserve">Declarations</w:t></w:r><w:bookmarkEnd w:id="68" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="69" w:name="Xd182db0490b2de20bd2b4e4cc8698599182035f" /><w:r><w:t xml:space="preserve">Ethics approval and consent to participate</w:t></w:r><w:bookmarkEnd w:id="69" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="70" w:name="consent-for-publication" /><w:r><w:t xml:space="preserve">Consent for publication</w:t></w:r><w:bookmarkEnd w:id="70" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Not applicable.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="71" w:name="availability-of-data-and-materials" /><w:r><w:t xml:space="preserve">Availability of data and materials</w:t></w:r><w:bookmarkEnd w:id="71" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">All systematic review datasets used during this study are</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="72" w:name="competing-interests" /><w:r><w:t xml:space="preserve">Competing interests</w:t></w:r><w:bookmarkEnd w:id="72" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The author declares that they has no competing interests&quot; in this section.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="73" w:name="funding" /><w:r><w:t xml:space="preserve">Funding</w:t></w:r><w:bookmarkEnd w:id="73" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Computing hours on the Cartesius supercomputer were funded by SURFsara. SURFsara had no role whatsoever in the design of the current study, nor in the data collection, analysis and interpretation, nor in writing the manuscript.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="74" w:name="authors-contributions" /><w:r><w:t xml:space="preserve">Authors’ contributions</w:t></w:r><w:bookmarkEnd w:id="74" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="75" w:name="acknowledgements" /><w:r><w:t xml:space="preserve">Acknowledgements</w:t></w:r><w:bookmarkEnd w:id="75" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">I am grateful for all researchers who have made great efforts to openly publish the data on their systematic reviews, special thanks go out to Rosanna Nagtegaal. I would also like to thank Caroline van Baal for supporting me in writing this thesis, and prof. dr. René Eijkemans, for being the second grader of this thesis. Finally, I would like to express my appreciation to my supervisors prof. dr. Rens van de Schoot, Jonathan de Bruin and dr. Raoul Schram. Your door was always open and your enthusiasm was contagious.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="76" w:name="references" /><w:r><w:t xml:space="preserve">References</w:t></w:r><w:bookmarkEnd w:id="76" /></w:p><w:bookmarkStart w:id="158" w:name="refs" /><w:bookmarkStart w:id="78" w:name="ref-PRISMA-PGroup2015" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[1] PRISMA-P Group, Moher D, Shamseer L, Clarke M, Ghersi D, Liberati A, et al. Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement. Syst Rev 2015;4:1.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId77"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1186/2046-4053-4-1</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="78" /><w:bookmarkStart w:id="80" w:name="ref-Gough2002" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[2] Gough D, Elbourne D. Systematic Research Synthesis to Inform Policy, Practice and Democratic Debate. Soc Policy Soc 2002;1:225–36.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId79"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/bdmp7h</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="80" /><w:bookmarkStart w:id="81" w:name="ref-Chalmers2007" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[3] Chalmers I. The lethal consequences of failing to make full use of all relevant evidence about the effects of medical treatments: The importance of systematic reviews. In:. Treating individuals—from randomised trials to personalised medicine., Lancet; 2007, pp. 37–58.</w:t></w:r></w:p><w:bookmarkEnd w:id="81" /><w:bookmarkStart w:id="83" w:name="ref-Borah2017" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[4] Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. BMJ Open 2017;7:e012545.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId82"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/f9tf57</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="83" /><w:bookmarkStart w:id="85" w:name="ref-Lau2019" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[5] Lau J. Editorial: Systematic review automation thematic series. Syst Rev 2019;8:70.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId84"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/ggsmwf</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="85" /><w:bookmarkStart w:id="87" w:name="ref-Harrison2020" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[6] Harrison H, Griffin SJ, Kuhn I, Usher-Smith JA. Software tools to support title and abstract screening for systematic reviews in healthcare: An evaluation. BMC Med Res Methodol 2020;20:7.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId86"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1186/s12874-020-0897-3</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="87" /><w:bookmarkStart w:id="89" w:name="ref-OMara-Eves2015" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[7] O’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text mining for study identification in systematic reviews: A systematic review of current approaches. Syst Rev 2015;4:5.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId88"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1186/2046-4053-4-5</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="89" /><w:bookmarkStart w:id="91" w:name="ref-Cohen2009" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[8] Cohen AM, Ambert K, McDonagh M. Cross-Topic Learning for Work Prioritization in Systematic Review Creation and Update. J Am Med Inform Assoc 2009;16:690–704.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId90"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/c3shq2</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="91" /><w:bookmarkStart w:id="93" w:name="ref-Shemilt2014" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[9] Shemilt I, Simon A, Hollands GJ, Marteau TM, Ogilvie D, O’Mara‐Eves A, et al. Pinpointing needles in giant haystacks: Use of text mining to reduce impractical screening workload in extremely large scoping reviews. Res Synth Methods 2014;5:31–49.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId92"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1002/jrsm.1093</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="93" /><w:bookmarkStart w:id="95" w:name="ref-Yu2019" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[10] Yu Z, Menzies T. FAST2: An intelligent assistant for finding relevant papers. Expert Syst Appl 2019;120:57–71.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId94"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1016/j.eswa.2018.11.021</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="95" /><w:bookmarkStart w:id="97" w:name="ref-Yu2018" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[11] Yu Z, Kraft NA, Menzies T. Finding better active learners for faster literature reviews. Empir Softw Eng 2018;23:3161–86.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId96"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1007/s10664-017-9587-0</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="97" /><w:bookmarkStart w:id="99" w:name="ref-Miwa2014" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[12] Miwa M, Thomas J, O’Mara-Eves A, Ananiadou S. Reducing systematic review workload through certainty-based screening. J Biomed Inform 2014;51:242–53.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId98"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1016/j.jbi.2014.06.005</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="99" /><w:bookmarkStart w:id="101" w:name="ref-Cormack2014" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[13] Cormack GV, Grossman MR. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In:. Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval, Gold Coast, Queensland, Australia: Association for Computing Machinery; 2014, pp. 153–62.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId100"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1145/2600428.2609601</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="101" /><w:bookmarkStart w:id="102" w:name="ref-Cormack2015" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[14] Cormack GV, Grossman MR. Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review 2015.</w:t></w:r></w:p><w:bookmarkEnd w:id="102" /><w:bookmarkStart w:id="104" w:name="ref-Wallace2010" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[15] Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-automated screening of biomedical citations for systematic reviews. BMC Bioinform 2010;11:55.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId103"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1186/1471-2105-11-55</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="104" /><w:bookmarkStart w:id="106" w:name="ref-Gates2018a" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[16] Gates A, Johnson C, Hartling L. Technology-assisted title and abstract screening for systematic reviews: A retrospective evaluation of the Abstrackr machine learning tool. Syst Rev 2018;7:45.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId105"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/ggpsx4</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="106" /><w:bookmarkStart w:id="108" w:name="ref-Settles2012" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[17] Settles B. Active Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning 2012;6:1–114.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId107"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.2200/S00429ED1V01Y201207AIM018</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="108" /><w:bookmarkStart w:id="109" w:name="ref-Settles2009" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[18] Settles B. Active Learning Literature Survey. University of Wisconsin-Madison Department of Computer Sciences; 2009.</w:t></w:r></w:p><w:bookmarkEnd w:id="109" /><w:bookmarkStart w:id="111" w:name="ref-Kilicoglu2009" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[19] Kilicoglu H, Demner-Fushman D, Rindflesch TC, Wilczynski NL, Haynes RB. Towards Automatic Recognition of Scientifically Rigorous Clinical Research Evidence. J Am Med Inform Assn 2009;16:25–31.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId110"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/bjkhh9</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="111" /><w:bookmarkStart w:id="113" w:name="ref-Aphinyanaphongs2004" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[20] Aphinyanaphongs Y. Text Categorization Models for High-Quality Article Retrieval in Internal Medicine. J Am Med Inform Assoc 2004;12:207–16.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId112"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/cpvp52</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="113" /><w:bookmarkStart w:id="114" w:name="ref-Le2014" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[21] Le QV, Mikolov T. Distributed Representations of Sentences and Documents 2014.</w:t></w:r></w:p><w:bookmarkEnd w:id="114" /><w:bookmarkStart w:id="116" w:name="ref-Zhang2011" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[22] Zhang W, Yoshida T, Tang X. A comparative study of TF*IDF, LSI and multi-words for text classification. Expert Syst Appl 2011;38:2758–65.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId115"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/dp7268</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="116" /><w:bookmarkStart w:id="118" w:name="ref-Marshall2020" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[23] Marshall IJ, Johnson BT, Wang Z, Rajasekaran S, Wallace BC. Semi-Automated evidence synthesis in health psychology: Current methods and future prospects. Health Psychol Rev 2020;14:145–58.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId117"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/ggjv98</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="118" /><w:bookmarkStart w:id="120" w:name="ref-Cohen2006" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[24] Cohen AM, Hersh WR, Peterson K, Yen P-Y. Reducing Workload in Systematic Review Preparation Using Automated Citation Classification. J Am Med Inform Assoc 2006;13:206–19.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId119"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1197/jamia.M1929</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="120" /><w:bookmarkStart w:id="122" w:name="ref-Appenzeller-Herzog2019" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[25] Appenzeller‐Herzog C, Mathes T, Heeres MLS, Weiss KH, Houwen RHJ, Ewald H. Comparative effectiveness of common therapies for Wilson disease: A systematic review and meta-analysis of controlled studies. Liver Int 2019;39:2136–52.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId121"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1111/liv.14179</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="122" /><w:bookmarkStart w:id="124" w:name="ref-vandeSchoot2017" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[26] van de Schoot R, Sijbrandij M, Winter SD, Depaoli S, Vermunt JK. The GRoLTS-Checklist: Guidelines for reporting on latent trajectory studies. Struct Equ Model Multidiscip J 2017;24:451–67.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId123"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/gdpcw9</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="124" /><w:bookmarkStart w:id="126" w:name="ref-Nagtegaal2019" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[27] Nagtegaal R, Tummers L, Noordegraaf M, Bekkers V. Nudging healthcare professionals towards evidence-based medicine: A systematic scoping review. J Behav Public Adm 2019;2.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId125"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/doi.org/10.30636/jbpa.22.71</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="126" /><w:bookmarkStart w:id="128" w:name="ref-Kwok2020" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[28] Kwok KTT, Nieuwenhuijse DF, Phan MVT, Koopmans MPG. Virus Metagenomics in Farm Animals: A Systematic Review. Viruses 2020;12:107.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId127"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.3390/v12010107</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="128" /><w:bookmarkStart w:id="130" w:name="ref-ASReview2020" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[29] van de Schoot R, de Bruin J, Schram R, Zahedi P, Kramer B, Ferdinands G, et al. ASReview: Active learning for systematic reviews 2020.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId129"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/ggssnj</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="130" /><w:bookmarkStart w:id="131" w:name="ref-Tong2001" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[30] Tong S, Koller D. Support vector machine active learning with applications to text classification. J Mach Learn Res 2001;2:45–66.</w:t></w:r></w:p><w:bookmarkEnd w:id="131" /><w:bookmarkStart w:id="133" w:name="ref-Kremer2014" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[31] Kremer J, Steenstrup Pedersen K, Igel C. Active learning with support vector machines. WIREs Data Min Knowl Discov 2014;4:313–26.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId132"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/f6fss7</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="133" /><w:bookmarkStart w:id="135" w:name="ref-Wallace2012b" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[32] Wallace BC, Small K, Brodley CE, Lau J, Trikalinos TA. Deploying an interactive machine learning system in an evidence-based practice center: Abstrackr. In:. Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium, Miami, Florida, USA: Association for Computing Machinery; 2012, pp. 819–24.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId134"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1145/2110363.2110464</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="135" /><w:bookmarkStart w:id="137" w:name="ref-Cheng2018" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[33] Cheng SH, Augustin C, Bethel A, Gill D, Anzaroot S, Brun J, et al. Using machine learning to advance synthesis and use of conservation and environmental evidence. Conserv Biol 2018;32:762–4.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId136"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1111/cobi.13117</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="137" /><w:bookmarkStart w:id="139" w:name="ref-Ouzzani2016" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[34] Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan—a web and mobile app for systematic reviews. Syst Rev 2016;5:210.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId138"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1186/s13643-016-0384-4</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="139" /><w:bookmarkStart w:id="141" w:name="ref-Przybyla2018" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[35] Przybyła P, Brockmeier AJ, Kontonatsios G, Pogam M-AL, McNaught J, Erik von Elm, et al. Prioritising references for systematic reviews with RobotAnalyst: A user study. Res Synth Methods 2018;9:470–88.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId140"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1002/jrsm.1311</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="141" /><w:bookmarkStart w:id="142" w:name="ref-scikit-learn" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[36] Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: Machine learning in Python. J Mach Learn Res 2011;12:2825–30.</w:t></w:r></w:p><w:bookmarkEnd w:id="142" /><w:bookmarkStart w:id="143" w:name="ref-Zhang2004" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[37] Zhang H. The Optimality of Naive Bayes. In:. vol. 2, 2004.</w:t></w:r></w:p><w:bookmarkEnd w:id="143" /><w:bookmarkStart w:id="145" w:name="ref-Breiman2001" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[38] Breiman L. Random Forests. Machine Learning 2001;45:5–32.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId144"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1023/A:1010933404324</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="145" /><w:bookmarkStart w:id="147" w:name="ref-Aggarwal2012" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[39] Aggarwal CC, Zhai C. A Survey of Text Classification Algorithms. In: Aggarwal CC, Zhai C, editors. Mining Text Data, Boston, MA: Springer US; 2012, pp. 163–222.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId146"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1007/978-1-4614-3223-4_6</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="147" /><w:bookmarkStart w:id="148" w:name="ref-Ramos2003" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[40] Ramos J, others. Using tf-idf to determine word relevance in document queries. In:. Proceedings of the first instructional conference on machine learning, vol. 242, Piscataway, NJ; 2003, pp. 133–42.</w:t></w:r></w:p><w:bookmarkEnd w:id="148" /><w:bookmarkStart w:id="150" w:name="ref-Fu2011" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[41] Fu JH, Lee SL. Certainty-Enhanced Active Learning for Improving Imbalanced Data Classification. In:. 2011 IEEE 11th International Conference on Data Mining Workshops, Vancouver, BC, Canada: IEEE; 2011, pp. 405–12.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId149"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1109/ICDMW.2011.43</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="150" /><w:bookmarkStart w:id="151" w:name="ref-RCoreTeam2019" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[42] R Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing; 2019.</w:t></w:r></w:p><w:bookmarkEnd w:id="151" /><w:bookmarkStart w:id="152" w:name="ref-Appenzeller-Herzog2020" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[43] Appenzeller-Herzog C. Data from Comparative effectiveness of common therapies for Wilson disease: A systematic review and meta‐analysis of controlled studies 2020.</w:t></w:r></w:p><w:bookmarkEnd w:id="152" /><w:bookmarkStart w:id="154" w:name="ref-Hall2012" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[44] Hall T, Beecham S, Bowes D, Gray D, Counsell S. A Systematic Literature Review on Fault Prediction Performance in Software Engineering. IEEE Trans Softw Eng 2012;38:1276–304.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId153"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10.1109/TSE.2011.103</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="154" /><w:bookmarkStart w:id="155" w:name="ref-Nagtegaal2019a" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[45] Nagtegaal R, Tummers L, Noordegraaf M, Bekkers V. Nudging healthcare professionals towards evidence-based medicine: A systematic scoping review 2019.</w:t></w:r></w:p><w:bookmarkEnd w:id="155" /><w:bookmarkStart w:id="157" w:name="ref-Rathbone2015" /><w:p><w:pPr><w:pStyle w:val="Bibliography" /></w:pPr><w:r><w:t xml:space="preserve">[46] Rathbone J, Hoffmann T, Glasziou P. Faster title and abstract screening? Evaluating Abstrackr, a semi-automated online screening program for systematic reviewers. Systematic Reviews 2015;4:80.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId156"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://doi.org/10/f7ms4w</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="157" /><w:bookmarkEnd w:id="158" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:bookmarkStart w:id="159" w:name="additional-file-1---remaining-figures" /><w:r><w:t xml:space="preserve">Additional file 1 - Remaining figures</w:t></w:r><w:bookmarkEnd w:id="159" /></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="160" w:name="ptsd" /><w:r><w:t xml:space="preserve">PTSD</w:t></w:r><w:bookmarkEnd w:id="160" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the PTSD dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_all_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId53" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the PTSD dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_tfidf_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId161" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the PTSD dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_d2v_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId162" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the PTSD dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_d2v_vs_tfidf_logistic_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId163" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the PTSD dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_d2v_vs_tfidf_rf_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId164" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the PTSD dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ptsd/inclusion_d2v_vs_tfidf_svm_ptsd.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId165" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="166" w:name="software" /><w:r><w:t xml:space="preserve">Software</w:t></w:r><w:bookmarkEnd w:id="166" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Software dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/inclusion_all_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId167" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Software dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/inclusion_tfidf_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId168" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Software dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/inclusion_d2v_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId169" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Software dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/inclusion_d2v_vs_tfidf_logistic_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId170" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Software dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/inclusion_d2v_vs_tfidf_rf_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId171" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Software dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/software/inclusion_d2v_vs_tfidf_svm_software.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId172" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="173" w:name="ace" /><w:r><w:t xml:space="preserve">Ace</w:t></w:r><w:bookmarkEnd w:id="173" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Ace dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/inclusion_all_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId174" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Ace dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/inclusion_tfidf_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId175" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Ace dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/inclusion_d2v_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId176" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Ace dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/inclusion_d2v_vs_tfidf_logistic_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId177" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Ace dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/inclusion_d2v_vs_tfidf_rf_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId178" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Ace dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/ace/inclusion_d2v_vs_tfidf_svm_ace.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId179" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="180" w:name="virus" /><w:r><w:t xml:space="preserve">Virus</w:t></w:r><w:bookmarkEnd w:id="180" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Virus dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/inclusion_all_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId181" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Virus dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/inclusion_tfidf_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId182" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Virus dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/inclusion_d2v_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId183" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Virus dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/inclusion_d2v_vs_tfidf_logistic_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId184" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Virus dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/inclusion_d2v_vs_tfidf_rf_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId185" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Virus dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/virus/inclusion_d2v_vs_tfidf_svm_virus.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId186" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:bookmarkStart w:id="187" w:name="wilson" /><w:r><w:t xml:space="preserve">Wilson</w:t></w:r><w:bookmarkEnd w:id="187" /></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/inclusion_all_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId188" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/inclusion_tfidf_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId189" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/inclusion_d2v_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId190" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/inclusion_d2v_vs_tfidf_logistic_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId191" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/inclusion_d2v_vs_tfidf_rf_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId192" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Recall curves for the Wilson dataset." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="/Volumes/Gerbrich/asreview/results/one_seed/plots/wilson/inclusion_d2v_vs_tfidf_svm_wilson.pdf" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId193" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:sectPr /></w:body></w:document>