
@misc{2006,
  title = {Mining {{Text}} with {{Pimiento}}},
  year = {2006},
  month = jul,
  doi = {10.1109/MIC.2006.85},
  abstract = {Information systems are using an increasing amount of unstructured information in the form of text. This situation has spawned a need to improve the text-mining technologies needed for information retrieval, filtering, and classification. This article compares some of the options available and how they can provide textual data-mining functionalities to software applications. In particular, the authors focus on Pimiento, a new object-oriented application framework for text mining. This framework allows developers to easily create distributed applications that use machine learning and statistical techniques to automatically process documents.},
  file = {/Users/gerbrich/Zotero/storage/TILLFCG4/13rRUypGGf7.html},
  howpublished = {https://www.computer.org/csdl/magazine/ic/2006/04/w4027/13rRUypGGf7},
  language = {en},
  type = {Text}
}

@misc{2019,
  title = {How Does Machine Learning Work at Sysrev?},
  year = {2019},
  month = sep,
  abstract = {Sysrev automatically builds machine learning models at every stage of ~review.

In each sysrev users can screen articles in their corpus by marking them as an
'include' or 'exclude'. ~While reviewing a sysrev screening model is silently
learning how to replicate reviewer decisions. ~Screening models can help
accelerate the review process and eventually automate reviews. ~Automated
reviews will be a key step in the updateable 'living' reviews talked that will
be the next frontier for document rev},
  file = {/Users/gerbrich/Zotero/storage/IS5JUAHZ/machine-learning.html},
  howpublished = {http://blog.sysrev.com/machine-learning/},
  journal = {Sysrev Blog},
  language = {en}
}

@misc{AndyShapiro2018,
  title = {Shapiromatron/Hawc: 2018-{{Q3}}},
  shorttitle = {Shapiromatron/Hawc},
  author = {Andy Shapiro and Josh Addington and Shane Thacker and Justin Comeaux},
  year = {2018},
  month = sep,
  doi = {10.5281/zenodo.1414622},
  abstract = {2018 Q3 release.},
  file = {/Users/gerbrich/Zotero/storage/ESIYK5HH/1414622.html},
  howpublished = {Zenodo}
}

@techreport{ASReviewCoreDevelopmentTeam2019,
  title = {{{ASReview}}: {{Active}} Learning for Systematic Reviews},
  author = {{ASReview Core Development Team}},
  year = {2019},
  address = {{Utrecht, The Netherlands}},
  doi = {10.5281/zenodo.3345592},
  organization = {{Utrecht University}}
}

@inproceedings{Barn2014,
  title = {Slrtool: A Tool to Support Collaborative Systematic Literature Reviews},
  shorttitle = {Slrtool},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Enterprise Information Systems}}, {{Volume}} 2},
  author = {Barn, Balbir and Raimondi, Franco and Athiappan, Lalith and Clark, Tony},
  year = {2014},
  pages = {440--447},
  publisher = {{SCITEPRESS}},
  address = {{Lisbon, Portugal}},
  abstract = {Systematic Literature Reviews (SLRs) are used in a number of fields to produce unbiased accounts of specific research topics. The SLR process is particularly well documented and regulated in the medical field, where it is accepted as the standard mechanism to assess, for instance, the benefits of drugs and treatments. SLRs and meta-analysis techniques are increasingly being used in other fields as well, from Social Sciences to Software Engineering.},
  file = {/Users/gerbrich/Zotero/storage/95IIU42B/Barn et al. - 2014 - Slrtool a tool to support collaborative systemati.pdf;/Users/gerbrich/Zotero/storage/MXNN29UK/14668.html},
  isbn = {978-989-758-028-4},
  language = {en}
}

@inproceedings{Bigendako2020,
  title = {Modeling a {{Tool}} for {{Conducting Systematic Reviews Iteratively}}},
  booktitle = {6th {{International Conference}} on {{Model}}-{{Driven Engineering}} and {{Software Development}}},
  author = {Bigendako, Brice and Syriani, Eugene},
  year = {2020},
  month = feb,
  pages = {552--559},
  abstract = {Digital Library},
  file = {/Users/gerbrich/Zotero/storage/8IK7WPAE/Link.html},
  isbn = {978-989-758-283-7},
  keywords = {relis}
}

@article{Breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  volume = {45},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  file = {/Users/gerbrich/Zotero/storage/GXJE7SAK/Breiman - 2001 - Random Forests.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@article{Cheng2018,
  title = {Using Machine Learning to Advance Synthesis and Use of Conservation and Environmental Evidence},
  author = {Cheng, S. H. and Augustin, C. and Bethel, A. and Gill, D. and Anzaroot, S. and Brun, J. and DeWilde, B. and Minnich, R. C. and Garside, R. and Masuda, Y. J. and Miller, D. C. and Wilkie, D. and Wongbusarakum, S. and McKinnon, M. C.},
  year = {2018},
  volume = {32},
  pages = {762--764},
  issn = {1523-1739},
  doi = {10.1111/cobi.13117},
  abstract = {Article impact statement: Machine learning optimizes processes of systematic evidence synthesis and improves its utility for evidence-based conservation.},
  copyright = {\textcopyright{} 2018 Society for Conservation Biology},
  file = {/Users/gerbrich/Zotero/storage/KZKWUGRY/Cheng et al. - 2018 - Using machine learning to advance synthesis and us.pdf;/Users/gerbrich/Zotero/storage/G3QCVYE7/cobi.html},
  journal = {Conservation Biology},
  keywords = {colandr},
  number = {4}
}

@article{Cleo2019,
  title = {Usability and Acceptability of Four Systematic Review Automation Software Packages: A Mixed Method Design},
  shorttitle = {Usability and Acceptability of Four Systematic Review Automation Software Packages},
  author = {Cleo, Gina and Scott, Anna Mae and Islam, Farhana and Julien, Blair and Beller, Elaine},
  year = {2019},
  month = jun,
  volume = {8},
  pages = {145},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1069-6},
  abstract = {New software packages help to improve the efficiency of conducting a systematic review through automation of key steps in the systematic review. The aim of this study was to gather qualitative data on the usability and acceptability of four systematic review automation software packages (Covidence, SRA-Helper for EndNote, Rayyan and RobotAnalyst) for the citation screening step of a systematic review.},
  file = {/Users/gerbrich/Zotero/storage/M7D3VD7L/Cleo et al. - 2019 - Usability and acceptability of four systematic rev.pdf;/Users/gerbrich/Zotero/storage/QMF2STW7/s13643-019-1069-6.html},
  journal = {Systematic Reviews},
  number = {1}
}

@inproceedings{Felizardo2011,
  title = {Using {{Visual Text Mining}} to {{Support}} the {{Study Selection Activity}} in {{Systematic Literature Reviews}}},
  booktitle = {International {{Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Felizardo, Katia and Salleh, Norsaremah and Martins, Rafael and Mendes, Emilia and MacDonell, Stephen and Maldonado, Jos{\'e}},
  year = {2011},
  month = sep,
  pages = {77--86},
  doi = {10.1109/ESEM.2011.16},
  abstract = {Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.},
  file = {/Users/gerbrich/Zotero/storage/WRQ3UZ89/Felizardo et al. - 2011 - Using Visual Text Mining to Support the Study Sele.pdf},
  keywords = {revis}
}

@inproceedings{Fernandez-Saez2010,
  title = {{{SLR}}-{{Tool}} - {{A Tool}} for {{Performing Systematic Literature Reviews}}.},
  author = {{Fern{\'a}ndez-S{\'a}ez}, Ana and Genero, Marcela and Romero, Francisco},
  year = {2010},
  month = jan,
  pages = {157--166},
  abstract = {Systematic literature reviews (SLRs) have been gaining a significant amount of attention from Software Engineering researchers since 2004. SLRs are considered to be a new research methodology in Software Engineering, which allow evidence to be gathered with regard to the usefulness or effectiveness of the technology proposed in Software Engineering for the development and maintenance of software products.
This is demonstrated by the growing number of publications related to SLRs that have appeared in recent years. While some tools exist that can support some or all of the activities of the SLR processes defined in (Kitchenham \& Charters, 2007), these are not free. The objective of this paper is to present the SLR-Tool, which is a free tool and is available on the following website: http://alarcosj.esi.uclm.es/SLRTool/, to be used by researchers from any discipline, and not only Software Engineering. SLR-Tool not only supports the process of performing SLRs proposed in (Kitchenham \& Charters, 2007), but also provides additional functionalities such as: refining searches within the documents by applying text mining techniques; defining a classification schema in order to facilitate data synthesis; exporting the results obtained to the format of tables and charts; and exporting the references from the primary studies to the formats used in bibliographic
packages such as EndNote, BibTeX or Ris. This tool has, to date, been used by members of the Alarcos Research Group and PhD students, and their perception of it is that it is both highly necessary and useful. Our purpose now is to circulate the use of SLR-Tool throughout the entire research community in order to obtain feedback from other users.},
  file = {/Users/gerbrich/Zotero/storage/HF4M9SFC/Fernández-Sáez et al. - 2010 - SLR-Tool - A Tool for Performing Systematic Litera.pdf}
}

@misc{Freitas2020,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  year = {2020},
  month = feb,
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  copyright = {MIT},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@misc{Freitas2020a,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  year = {2020},
  month = feb,
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  copyright = {MIT},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@article{Gates2018,
  title = {Technology-Assisted Title and Abstract Screening for Systematic Reviews: A Retrospective Evaluation of the {{Abstrackr}} Machine Learning Tool},
  author = {Gates, Allison and Johnson, Cydney and Hartling, Lisa},
  year = {2018},
  month = mar,
  volume = {7},
  pages = {45},
  issn = {2046-4053},
  doi = {10.1186/s13643-018-0707-8},
  abstract = {Machine learning tools can expedite systematic review (SR) processes by semi-automating citation screening. Abstrackr semi-automates citation screening by predicting relevant records. We evaluated its performance for four screening projects.},
  journal = {Systematic Reviews},
  keywords = {simulation},
  number = {1}
}

@article{Glujovsky2011,
  title = {{{PRM2 EROS}}: {{A New Software For Early Stage Of Systematic REVIEWS}}},
  shorttitle = {{{PRM2 EROS}}},
  author = {Glujovsky, D. and Bardach, A. and Mart{\'i}, S. Garc{\'i}a and Comand{\'e}, D. and Ciapponi, A.},
  year = {2011},
  month = nov,
  volume = {14},
  pages = {A564},
  issn = {1098-3015, 1524-4733},
  doi = {10.1016/j.jval.2011.08.1689},
  abstract = {The workload of the initial phases of the process of developing a systematic review
(SR) is often underestimated. The screening and quality assessment of studies, usually
done by pairs of independent reviewers, is not only time-consuming, but it also is
complicated, tiresome, and prone to mistakes. A computer-software designed to cope
with the initial phases of a SR would be of great help. There is a generalized lack
of development in this regard, and the available options are not very accessible or
affordable.},
  file = {/Users/gerbrich/Zotero/storage/IK4HEMZJ/Glujovsky et al. - 2011 - PRM2 EROS A New Software For Early Stage Of Syste.pdf;/Users/gerbrich/Zotero/storage/NBIWRX58/fulltext.html},
  journal = {Value in Health},
  language = {English},
  number = {7},
  pmid = {21669381}
}

@unpublished{Harkema,
  title = {Replacing {{Scientists}} by {{Machines}}.},
  author = {Harkema, A},
  file = {/Users/gerbrich/Zotero/storage/PY6JF9QJ/Harkema - Replacing Scientists by Machines..pdf},
  language = {en}
}

@article{Harrison2020,
  title = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare: An Evaluation},
  shorttitle = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare},
  author = {Harrison, Hannah and Griffin, Simon J. and Kuhn, Isla and {Usher-Smith}, Juliet A.},
  year = {2020},
  month = jan,
  volume = {20},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-0897-3},
  abstract = {Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T\&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T\&Ab screening for systematic reviews within healthcare research.},
  file = {/Users/gerbrich/Zotero/storage/7UYH9YX8/12874_2020_897_MOESM3_ESM.docx;/Users/gerbrich/Zotero/storage/NMB789XG/Harrison et al. - 2020 - Software tools to support title and abstract scree.pdf;/Users/gerbrich/Zotero/storage/CSUQM7G7/s12874-020-0897-3.html},
  journal = {BMC Medical Research Methodology},
  number = {1}
}

@article{Howard2016,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review},
  shorttitle = {{{SWIFT}}-{{Review}}},
  author = {Howard, Brian E. and Phillips, Jason and Miller, Kyle and Tandon, Arpit and Mav, Deepak and Shah, Mihir R. and Holmgren, Stephanie and Pelch, Katherine E. and Walker, Vickie and Rooney, Andrew A. and Macleod, Malcolm and Shah, Ruchir R. and Thayer, Kristina},
  year = {2016},
  month = may,
  volume = {5},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0263-z},
  abstract = {Background
There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus.

Methods
Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95~\% of known relevant studies and (2) the ``Work Saved over Sampling'' (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics.

Results
Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50~\% of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation.

Conclusions
Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation.

Electronic supplementary material
The online version of this article (doi:10.1186/s13643-016-0263-z) contains supplementary material, which is available to authorized users.},
  file = {/Users/gerbrich/Zotero/storage/QM5IJC8Z/Howard et al. - 2016 - SWIFT-Review a text-mining workbench for systemat.pdf},
  journal = {Systematic Reviews},
  keywords = {swift-review},
  pmcid = {PMC4877757},
  pmid = {27216467}
}

@article{Kohl2018,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  shorttitle = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  year = {2018},
  month = feb,
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K{\"u}hn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  file = {/Users/gerbrich/Zotero/storage/XZX9XDTZ/Kohl et al. - 2018 - Online tools supporting the conduct and reporting .pdf;/Users/gerbrich/Zotero/storage/ECBH5U46/s13750-018-0115-5.html},
  journal = {Environmental Evidence},
  keywords = {cadima},
  number = {1}
}

@article{Lajeunesse2016,
  title = {Facilitating Systematic Reviews, Data Extraction, and Meta-Analysis with the Metagear Package for {{R}}},
  author = {Lajeunesse, Marc J.},
  year = {2016},
  volume = {7},
  pages = {323--330},
  journal = {Methods in Ecology and Evolution}
}

@article{Le2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014},
  month = may,
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ``powerful,'' ``strong'' and ``Paris'' are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  archivePrefix = {arXiv},
  eprint = {1405.4053},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/IH9XHJUV/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf},
  journal = {arXiv:1405.4053 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,doc2vec,feature_extraction},
  language = {en},
  primaryClass = {cs}
}

@incollection{Lewis1994,
  title = {Heterogeneous {{Uncertainty Sampling}} for {{Supervised Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Lewis, David D. and Catlett, Jason},
  editor = {Cohen, William W. and Hirsh, Haym},
  year = {1994},
  month = jan,
  pages = {148--156},
  publisher = {{Morgan Kaufmann}},
  address = {{San Francisco (CA)}},
  doi = {10.1016/B978-1-55860-335-6.50026-X},
  abstract = {Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.},
  file = {/Users/gerbrich/Zotero/storage/V8XP4FY3/Lewis and Catlett - 1994 - Heterogeneous Uncertainty Sampling for Supervised .pdf;/Users/gerbrich/Zotero/storage/GFRUCYX6/B978155860335650026X.html},
  isbn = {978-1-55860-335-6},
  keywords = {query_strategy},
  language = {en}
}

@article{Matwin2010,
  title = {A New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews},
  author = {Matwin, Stan and Kouznetsov, Alexandre and Inkpen, Diana and Frunza, Oana and O'Blenis, Peter},
  year = {2010},
  month = jul,
  volume = {17},
  pages = {446--453},
  issn = {1067-5027},
  doi = {10.1136/jamia.2010.004325},
  abstract = {Abstract.  Objective To determine whether a factorized version of the complement na{\"i}ve Bayes (FCNB) classifier can reduce the time spent by experts reviewing jo},
  file = {/Users/gerbrich/Zotero/storage/D2P8QY98/Matwin et al. - 2010 - A new algorithm for reducing the workload of exper.pdf;/Users/gerbrich/Zotero/storage/7R2UK3MA/867054.html},
  journal = {Journal of the American Medical Informatics Association},
  language = {en},
  number = {4}
}

@article{modAL2018,
  title = {{{modAL}}: {{A}} Modular Active Learning Framework for {{Python}}},
  author = {Danka, Tivadar and Horvath, Peter}
}

@article{OMara-Eves2015,
  title = {Using Text Mining for Study Identification in Systematic Reviews: A Systematic Review of Current Approaches},
  shorttitle = {Using Text Mining for Study Identification in Systematic Reviews},
  author = {{O'Mara-Eves}, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
  year = {2015},
  month = jan,
  volume = {4},
  pages = {5},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-5},
  abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
  file = {/Users/gerbrich/Zotero/storage/WWD9DBD2/O’Mara-Eves et al. - 2015 - Using text mining for study identification in syst.pdf;/Users/gerbrich/Zotero/storage/HMT2W9P7/2046-4053-4-5.html},
  journal = {Systematic Reviews},
  number = {1}
}

@article{Ouzzani2016,
  title = {Rayyan\textemdash{}a Web and Mobile App for Systematic Reviews},
  author = {Ouzzani, Mourad and Hammady, Hossam and Fedorowicz, Zbys and Elmagarmid, Ahmed},
  year = {2016},
  volume = {5},
  pages = {210},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0384-4},
  abstract = {Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making.},
  journal = {Systematic Reviews},
  number = {1}
}

@article{PRISMA-PGroup2015,
  title = {Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols ({{PRISMA}}-{{P}}) 2015 Statement},
  author = {{PRISMA-P Group} and Moher, David and Shamseer, Larissa and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A},
  year = {2015},
  month = dec,
  volume = {4},
  pages = {1},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-1},
  abstract = {Systematic reviews should build on a protocol that describes the rationale, hypothesis, and planned methods of the review; few reviews report whether a protocol exists. Detailed, well-described protocols can facilitate the understanding and appraisal of the review methods, as well as the detection of modifications to methods and selective reporting in completed reviews. We describe the development of a reporting guideline, the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 (PRISMA-P 2015). PRISMA-P consists of a 17-item checklist intended to facilitate the preparation and reporting of a robust protocol for the systematic review. Funders and those commissioning reviews might consider mandating the use of the checklist to facilitate the submission of relevant protocol information in funding applications. Similarly, peer reviewers and editors can use the guidance to gauge the completeness and transparency of a systematic review protocol submitted for publication in a journal or other medium.},
  file = {/Users/gerbrich/Zotero/storage/PT8NAI48/PRISMA-P Group et al. - 2015 - Preferred reporting items for systematic review an.pdf},
  journal = {Systematic Reviews},
  language = {en},
  number = {1}
}

@article{Przybyla2018,
  title = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}: {{A}} User Study},
  shorttitle = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}},
  author = {Przyby{\l}a, Piotr and Brockmeier, Austin J. and Kontonatsios, Georgios and Pogam, Marie-Annick Le and McNaught, John and {\noopsort{elm}}von Elm, Erik and Nolan, Kay and Ananiadou, Sophia},
  year = {2018},
  volume = {9},
  pages = {470--488},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1311},
  abstract = {Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95\% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.},
  copyright = {\textcopyright{} 2018 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  file = {/Users/gerbrich/Zotero/storage/7FPGPSK2/Przybyła et al. - 2018 - Prioritising references for systematic reviews wit.pdf;/Users/gerbrich/Zotero/storage/U7KPBBJ4/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {3}
}

@inproceedings{Ramos2003,
  title = {Using Tf-Idf to Determine Word Relevance in Document Queries},
  booktitle = {Proceedings of the First Instructional Conference on Machine Learning},
  author = {Ramos, Juan and others},
  year = {2003},
  volume = {242},
  pages = {133--142},
  file = {/Users/gerbrich/Zotero/storage/YUB2N9D9/Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf},
  organization = {{Piscataway, NJ}}
}

@inproceedings{Rehurek2010,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r{u}}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  month = may,
  pages = {45--50},
  publisher = {{ELRA}},
  address = {{Valletta, Malta}},
  language = {English}
}

@article{Reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde{}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archivePrefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/QWTQF9NF/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/gerbrich/Zotero/storage/SI4PQX5T/1908.html},
  journal = {arXiv:1908.10084 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research}
}

@article{Settles2012,
  title = {Active {{Learning}}},
  author = {Settles, Burr},
  year = {2012},
  month = jun,
  volume = {6},
  pages = {1--114},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00429ED1V01Y201207AIM018},
  file = {/Users/gerbrich/Zotero/storage/2BSTA3X9/Settles - 2012 - Active Learning.pdf},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  language = {en},
  number = {1}
}

@article{Shemilt2014,
  title = {Pinpointing Needles in Giant Haystacks: Use of Text Mining to Reduce Impractical Screening Workload in Extremely Large Scoping Reviews},
  shorttitle = {Pinpointing Needles in Giant Haystacks},
  author = {Shemilt, Ian and Simon, Antonia and Hollands, Gareth J. and Marteau, Theresa M. and Ogilvie, David and O'Mara-Eves, Alison and Kelly, Michael P. and Thomas, James},
  year = {2014},
  volume = {5},
  pages = {31--49},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1093},
  abstract = {In scoping reviews, boundaries of relevant evidence may be initially fuzzy, with refined conceptual understanding of interventions and their proposed mechanisms of action an intended output of the scoping process rather than its starting point. Electronic searches are therefore sensitive, often retrieving very large record sets that are impractical to screen in their entirety. This paper describes methods for applying and evaluating the use of text mining (TM) technologies to reduce impractical screening workload in reviews, using examples of two extremely large-scale scoping reviews of public health evidence (choice architecture (CA) and economic environment (EE)). Electronic searches retrieved {$>$}800,000 (CA) and {$>$}1 million (EE) records. TM technologies were used to prioritise records for manual screening. TM performance was measured prospectively. TM reduced manual screening workload by 90\% (CA) and 88\% (EE) compared with conventional screening (absolute reductions of {$\approx$}430 000 (CA) and {$\approx$}378 000 (EE) records). This study expands an emerging corpus of empirical evidence for the use of TM to expedite study selection in reviews. By reducing screening workload to manageable levels, TM made it possible to assemble and configure large, complex evidence bases that crossed research discipline boundaries. These methods are transferable to other scoping and systematic reviews incorporating conceptual development or explanatory dimensions. \textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  copyright = {\textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/C42P4W4M/Shemilt et al. - 2014 - Pinpointing needles in giant haystacks use of tex.pdf;/Users/gerbrich/Zotero/storage/5QFKVQ8Z/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {scoping review methods,study selection,systematic review methods,text mining},
  language = {en},
  number = {1}
}

@misc{Shperber2019,
  title = {A Gentle Introduction to {{Doc2Vec}}},
  author = {Shperber, Gidi},
  year = {2019},
  month = nov,
  abstract = {TL;DR},
  file = {/Users/gerbrich/Zotero/storage/MKWGDB73/a-gentle-introduction-to-doc2vec-db3e8c0cce5e.html},
  howpublished = {https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e},
  journal = {Medium},
  language = {en}
}

@article{Stansfield2013,
  title = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research: A Case Study},
  shorttitle = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research},
  author = {Stansfield, Claire and Thomas, James and Kavanagh, Josephine},
  year = {2013},
  volume = {4},
  pages = {230--241},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1082},
  abstract = {Background Scoping reviews of research help determine the feasibility and the resource requirements of conducting a systematic review, and the potential to generate a description of the literature quickly is attractive. Aims To test the utility and applicability of an automated clustering tool to describe and group research studies to improve the efficiency of scoping reviews. Methods A retrospective study of two completed scoping reviews was conducted. This compared the groups and descriptive categories obtained by automatically clustering titles and abstracts with those that had originally been derived using traditional researcher-driven techniques. Results The clustering tool rapidly categorised research into themes, which were useful in some instances, but not in others. This provided a dynamic means to view each dataset. Interpretation was challenging where there were potentially multiple meanings of terms. Where relevant clusters were unambiguous, there was a high precision of relevant studies, although recall varied widely. Conclusions Policy-relevant scoping reviews are often undertaken rapidly, and this could potentially be enhanced by automation depending on the nature of the dataset and information sought. However, it is not a replacement for researcher-developed classification. The possibilities of further applications and potential for use in other types of review are discussed. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/B92KBE6S/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {automatic clustering,automation,information storage and retrieval,lingo,mapping,methods,scoping reviews,text mining},
  language = {en},
  number = {3}
}

@article{Thomas2011,
  title = {Applications of Text Mining within Systematic Reviews},
  author = {Thomas, James and McNaught, John and Ananiadou, Sophia},
  year = {2011},
  volume = {2},
  pages = {1--14},
  doi = {10.1002/jrsm.27},
  abstract = {Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.27},
  journal = {Research Synthesis Methods},
  keywords = {automatic summarization,document classification,document clustering,research synthesis,screening,searching,systematic review,term recognition,text mining},
  number = {1}
}

@inproceedings{Tomassetti2011,
  title = {Linked {{Data}} Approach for Selection Process Automation in {{Systematic Reviews}}},
  booktitle = {15th {{Annual Conference}} on {{Evaluation}} \& {{Assessment}} in {{Software Engineering}} ({{EASE}} 2011)},
  author = {Tomassetti, Federico Cesare Argentino and Rizzo, Giuseppe and Vetro', Antonio and Ardito, Luca and Torchiano, Marco and Morisio, Maurizio},
  year = {2011},
  pages = {31--35},
  publisher = {{IEE}},
  doi = {10.1049/ic.2011.0004},
  abstract = {Background: a systematic review identifies, evaluates and synthesizes the available literature on a given topic using scientific and repeatable methodologies. The significant workload required and the subjectivity bias could affect results. Aim: semi-automate the selection process to reduce the amount of manual work needed and the consequent subjectivity bias. Method: extend and enrich the selection of primary studies using the existing technologies in the field of Linked Data and text mining. We define formally the selection process and we also develop a prototype that implements it. Finally, we conduct a case study that simulates the selection process of a systematic literature published in literature. Results: the process presented in this paper could reduce the work load of 20\% with respect to the work load needed in the fully manually selection, with a recall of 100\%. Conclusions: the extraction of knowledge from scientific studies through Linked Data and text mining techniques could be used in the selection phase of the systematic review process to reduce the work load and subjectivity bias.},
  file = {/Users/gerbrich/Zotero/storage/8PJ2V6EQ/Tomassetti et al. - 2011 - Linked Data approach for selection process automat.pdf;/Users/gerbrich/Zotero/storage/8PT4A537/2381987.html},
  isbn = {978-1-84919-509-6},
  keywords = {dbpedia},
  language = {eng}
}

@inproceedings{Wallace2012,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  month = jan,
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/JHVMNGCY/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {abstrackr,active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@article{Westgate2019,
  title = {Revtools: {{An R}} Package to Support Article Screening for Evidence Synthesis},
  author = {Westgate, Martin J.},
  year = {2019},
  doi = {10.1002/jrsm.1374},
  journal = {Research Synthesis Methods}
}

@article{Yu2018,
  title = {Finding Better Active Learners for Faster Literature Reviews},
  author = {Yu, Zhe and Kraft, Nicholas and Menzies, Tim},
  year = {2018},
  month = mar,
  doi = {10.1007/s10664-017-9587-0},
  abstract = {Literature reviews can be time-consuming and tedious to complete. By cataloging and refactoring three state-of-the-art active learning techniques from evidence-based medicine and legal electronic discovery, this paper finds and implements FASTREAD, a faster technique for studying a large corpus of documents, combining and parametrizing the most efficient active learning algorithms. This paper assesses FASTREAD using datasets generated from existing SE literature reviews (Hall, Wahono, Radjenovi{\'c}, Kitchenham et al.). Compared to manual methods, FASTREAD lets researchers find 95\% relevant studies after reviewing an order of magnitude fewer papers. Compared to other state-of-the-art automatic methods, FASTREAD reviews 20\textendash{}50\% fewer studies while finding same number of relevant primary studies in a systematic literature review.},
  file = {/Users/gerbrich/Zotero/storage/MTLEK8XH/Yu et al. - 2018 - Finding better active learners for faster literatu.pdf},
  journal = {Empirical Software Engineering}
}

@inproceedings{Zhang2004,
  title = {The {{Optimality}} of {{Naive Bayes}}},
  booktitle = {Proceedings of the {{Seventeenth International Florida Artificial Intelligence Research Society Conference}}, {{FLAIRS}} 2004},
  author = {Zhang, Harry},
  year = {2004},
  month = jan,
  volume = {2},
  abstract = {Naive Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classifica- tion is surprising, because the conditional independence assumption on which it is based, is rarely true in real- world applications. An open question is: what is the true reason for the surprisingly good performance of naive Bayes in classification? In this paper, we propose a novel explanation on the superb classification performance of naive Bayes. We show that, essentially, the dependence distribution; i.e., how the local dependence of a node distributes in each class, evenly or unevenly, and how the local dependen- cies of all nodes work together, consistently (support- ing a certain classification) or inconsistently (cancel- ing each other out), plays a crucial role. Therefore, no matter how strong the dependences among attributes are, naive Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences can- cel each other out. We propose and prove a sufficient and necessary conditions for the optimality of naive Bayes. Further, we investigate the optimality of naive Bayes under the Gaussian distribution. We present and prove a sufficient condition for the optimality of naive Bayes, in which the dependence between attributes do exist. This provides evidence that dependence among attributes may cancel out each other. In addition, we explore when naive Bayes works well.},
  file = {/Users/gerbrich/Zotero/storage/UP2XUMJR/Zhang - The Optimality of Naive Bayes.pdf},
  keywords = {model}
}

@misc{zotero-254,
  title = {Machine Learning Algorithms for Systematic Review: Reducing Workload in a Preclinical Review of Animal Studies and Reducing Human Screening Error | {{Systematic Reviews}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/FTXIT6LW/s13643-019-0942-7.html},
  howpublished = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-0942-7}
}

@misc{zotero-263,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}},
  file = {/Users/gerbrich/Zotero/storage/GYHDPIKS/MANUAL.html},
  howpublished = {https://pandoc.org/MANUAL.html\#definition-lists}
}

@misc{zotero-298,
  title = {Is It Time to Trust the Robots? {{The}} Reliability and Usability of Machine Learning Tools for Screening in Systematic Reviews | {{Colloquium Abstracts}}},
  shorttitle = {Is It Time to Trust the Robots?},
  file = {/Users/gerbrich/Zotero/storage/S2WIJV9A/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening.html},
  howpublished = {/2019-santiago/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening},
  language = {en}
}

@misc{zotero-306,
  title = {{{SWIFT}}-{{Active Screener}}},
  abstract = {SWIFT-Active Screener is a web-based, collaborative systematic review software application. Active Screener was designed to be easy-to-use, incorporating a simple, but powerful, graphical user interface with rich project status updates. What makes Active Screener special, however, is its behind-the-scenes application of state-of-the-art statistical models designed to save screeners time and effort by automatically prioritizing articles \ldots{}},
  file = {/Users/gerbrich/Zotero/storage/LCYQDK29/swift-activescreener.html},
  journal = {Sciome},
  language = {en-US}
}

@misc{zotero-311,
  title = {{{SWIFT}}-{{Active Screener}} | {{Sciome}}},
  file = {/Users/gerbrich/Zotero/storage/CJVCDF2E/swift-activescreener.html},
  howpublished = {https://www.sciome.com/swift-activescreener/},
  keywords = {swift-activescreener}
}

@misc{zotero-316,
  title = {{{EPPI}}-{{Reviewer}}: Introduction},
  file = {/Users/gerbrich/Zotero/storage/3LBEQ8A8/Default.html},
  howpublished = {https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=1913}
}

@misc{zotero-318,
  title = {{{EPPI}}-{{Reviewer}} 4: Systematic Review Software},
  file = {/Users/gerbrich/Zotero/storage/6EL4UVGU/Default.html},
  howpublished = {https://eppi.ioe.ac.uk/CMS/Default.aspx?alias=eppi.ioe.ac.uk/cms/er4\&},
  keywords = {eppi}
}

@misc{zotero-323,
  title = {Perform {{Systematic Literature Reviews}}},
  abstract = {Parsifal is a tool to support researchers to perform systematic literature reviews. Performing a systematic literature review is a labor-intensive task that requires a huge amount of work from the researcher. Parsifal will help you planning, conducting and reporting the review.},
  file = {/Users/gerbrich/Zotero/storage/6HEHCPW8/about.html},
  howpublished = {https://parsif.al/about/},
  journal = {Parsifal}
}

@misc{zotero-332,
  title = {{{GAPscreener}}: {{An}} Automatic Tool for Screening Human Genetic Association Literature in {{PubMed}} Using the Support Vector Machine Technique | {{BMC Bioinformatics}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/M7XPKFAT/1471-2105-9-205.html},
  howpublished = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-205}
}

@misc{zotero-349,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review | {{Systematic Reviews}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/HC8TLNZ2/s13643-016-0263-z.html},
  howpublished = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-016-0263-z}
}

@misc{zotero-351,
  title = {Completing a Full Systematic Review in under Two Weeks: Processes, Barriers and Facilitators | {{The}} 26th {{Cochrane Colloquium}}},
  file = {/Users/gerbrich/Zotero/storage/6ZPXMXSL/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators.html},
  howpublished = {https://colloquium2019.cochrane.org/abstracts/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators},
  keywords = {2weeks}
}

@misc{zotero-358,
  title = {{{MDX SLR Tool}}},
  file = {/Users/gerbrich/Zotero/storage/P6GXB2WY/about.html},
  howpublished = {http://ta.mdx.ac.uk/slr/about/}
}

@misc{zotero-383,
  title = {{{SyRF}}: {{Systematic Review Facility}} | {{SyRF}}: {{Systematic Review Facility}}},
  file = {/Users/gerbrich/Zotero/storage/8ALRBI87/syrf.org.uk.html},
  howpublished = {http://syrf.org.uk/}
}

@misc{zotero-389,
  title = {6.2. {{Feature}} Extraction \textemdash{} Scikit-Learn 0.22.1 Documentation},
  file = {/Users/gerbrich/Zotero/storage/6Y956NI9/feature_extraction.html},
  howpublished = {https://scikit-learn.org/stable/modules/feature\_extraction.html}
}

@preamble{ "\newcommand{\noopsort}[1]{} " }

