
@inproceedings{10.1145/2695664.2695902,
  title = {A Method to Support Search String Building in Systematic Literature Reviews through Visual Text Mining},
  booktitle = {Proceedings of the 30th Annual {{ACM}} Symposium on Applied Computing},
  author = {Mergel, Germano Duarte and Silveira, Milene Selbach and {\noopsort{silva}}{da Silva}, Tiago Silva},
  year = {2015},
  pages = {1594--1601},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2695664.2695902},
  isbn = {978-1-4503-3196-8},
  keywords = {information visualization,systematic literature review,visual text mining},
  numpages = {8},
  place = {Salamanca, Spain},
  series = {{{SAC}} '15}
}

@inproceedings{10.1145/2915970.2916013,
  title = {Improvements in the {{StArt}} Tool to Better Support the Systematic Review Process},
  booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  author = {Fabbri, Sandra and Silva, Cleiton and Hernandes, Elis and Octaviano, F{\'a}bio and Di Thommazo, Andr{\'e} and Belgamo, Anderson},
  year = {2016},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2915970.2916013},
  articleno = {Article 21},
  isbn = {978-1-4503-3691-8},
  keywords = {evidence-based software engineering,StArt tool,systematic literature review,systematic review,tool support},
  numpages = {5},
  place = {Limerick, Ireland},
  series = {{{EASE}} '16}
}

@misc{2014,
  title = {Review {{Manager}} ({{RevMan}})},
  year = {2014},
  address = {{Copenhagen: The Nordic Cochrane Centre}},
  howpublished = {The Cochrane Collaboration}
}

@misc{2019,
  title = {How Does Machine Learning Work at Sysrev?},
  year = {2019},
  month = sep,
  abstract = {Sysrev automatically builds machine learning models at every stage of ~review.

In each sysrev users can screen articles in their corpus by marking them as an
'include' or 'exclude'. ~While reviewing a sysrev screening model is silently
learning how to replicate reviewer decisions. ~Screening models can help
accelerate the review process and eventually automate reviews. ~Automated
reviews will be a key step in the updateable 'living' reviews talked that will
be the next frontier for document rev},
  file = {/Users/gerbrich/Zotero/storage/IS5JUAHZ/machine-learning.html},
  howpublished = {http://blog.sysrev.com/machine-learning/},
  journal = {Sysrev Blog},
  language = {en}
}

@article{Appenzeller-Herzog2019,
  title = {Comparative Effectiveness of Common Therapies for {{Wilson}} Disease: {{A}} Systematic Review and Meta-Analysis of Controlled Studies},
  shorttitle = {Comparative Effectiveness of Common Therapies for {{Wilson}} Disease},
  author = {Appenzeller-Herzog, Christian and Mathes, Tim and Heeres, Marlies L. S. and Weiss, Karl Heinz and Houwen, Roderick H. J. and Ewald, Hannah},
  year = {2019},
  volume = {39},
  pages = {2136--2152},
  issn = {1478-3231},
  doi = {10.1111/liv.14179},
  abstract = {Background \& aims Wilson disease (WD) is a rare disorder of copper metabolism. The objective of this systematic review was to determine the comparative effectiveness and safety of common treatments of WD. Methods We included WD patients of any age or stage and the study drugs D-penicillamine, zinc salts, trientine and tetrathiomolybdate. The control could be placebo, no treatment or any other treatment. We included prospective, retrospective, randomized and non-randomized studies. We searched Medline and Embase via Ovid, the Cochrane Central Register of Controlled Trials, and screened reference lists of included articles. Where possible, we applied random-effects meta-analyses. Results The 23 included studies reported on 2055 patients and mostly compared D-penicillamine to no treatment, zinc, trientine or succimer. One study compared tetrathiomolybdate and trientine. Post-decoppering maintenance therapy was addressed in one study only. Eleven of 23 studies were of low quality. When compared to no treatment, D-penicillamine was associated with a lower mortality (odds ratio 0.013; 95\% CI 0.0010 to 0.17). When compared to zinc, there was no association with mortality (odds ratio 0.73; 95\% CI 0.16 to 3.40) and prevention or amelioration of clinical symptoms (odds ratio 0.84; 95\% CI 0.48 to 1.48). Conversely, D-penicillamine may have a greater impact on side effects and treatment discontinuations than zinc. Conclusions There are some indications that zinc is safer than D-penicillamine therapy while being similarly effective in preventing or reducing hepatic or neurological WD symptoms. Study quality was low warranting cautious interpretation of our findings.},
  copyright = {\textcopyright{} 2019 John Wiley \& Sons A/S. Published by John Wiley \& Sons Ltd},
  file = {/Users/gerbrich/Zotero/storage/XSIFPGCQ/Appenzeller‐Herzog et al. - 2019 - Comparative effectiveness of common therapies for .pdf;/Users/gerbrich/Zotero/storage/3BAL863U/liv.html},
  journal = {Liver International},
  keywords = {hepatolenticular degeneration,meta-analysis,systematic review,Wilson disease},
  language = {en},
  number = {11}
}

@dataset{Appenzeller-Herzog2020,
  title = {Data from {{Comparative}} Effectiveness of Common Therapies for {{Wilson}} Disease: {{A}} Systematic Review and Meta-analysis of Controlled Studies},
  author = {{Appenzeller-Herzog}, Christian},
  year = {2020},
  month = jan,
  publisher = {{Zenodo}},
  keywords = {dataset,wilson}
}

@techreport{Aromataris2017,
  title = {Joanna {{Briggs Institute Reviewer}}'s {{Manual}}.},
  author = {Aromataris, E and Munn, Z},
  year = {2017},
  institution = {{The Joanna Briggs Institute}}
}

@techreport{ASReviewCoreDevelopmentTeam2019,
  title = {{{ASReview}}: {{Active}} Learning for Systematic Reviews},
  author = {{ASReview Core Development Team}},
  year = {2019},
  address = {{Utrecht, The Netherlands}},
  doi = {10.5281/zenodo.3345592},
  organization = {{Utrecht University}}
}

@inproceedings{Barn2014,
  title = {Slrtool: A Tool to Support Collaborative Systematic Literature Reviews},
  shorttitle = {Slrtool},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Enterprise Information Systems}}, {{Volume}} 2},
  author = {Barn, Balbir and Raimondi, Franco and Athiappan, Lalith and Clark, Tony},
  year = {2014},
  pages = {440--447},
  publisher = {{SCITEPRESS}},
  address = {{Lisbon, Portugal}},
  abstract = {Systematic Literature Reviews (SLRs) are used in a number of fields to produce unbiased accounts of specific research topics. The SLR process is particularly well documented and regulated in the medical field, where it is accepted as the standard mechanism to assess, for instance, the benefits of drugs and treatments. SLRs and meta-analysis techniques are increasingly being used in other fields as well, from Social Sciences to Software Engineering.},
  file = {/Users/gerbrich/Zotero/storage/95IIU42B/Barn et al. - 2014 - Slrtool a tool to support collaborative systemati.pdf;/Users/gerbrich/Zotero/storage/MXNN29UK/14668.html},
  isbn = {978-989-758-028-4},
  language = {en}
}

@inproceedings{Bigendako2020,
  title = {Modeling a {{Tool}} for {{Conducting Systematic Reviews Iteratively}}},
  booktitle = {6th {{International Conference}} on {{Model}}-{{Driven Engineering}} and {{Software Development}}},
  author = {Bigendako, Brice and Syriani, Eugene},
  year = {2020},
  month = feb,
  pages = {552--559},
  abstract = {Digital Library},
  file = {/Users/gerbrich/Zotero/storage/8IK7WPAE/Link.html},
  isbn = {978-989-758-283-7},
  keywords = {relis}
}

@article{Breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  volume = {45},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  file = {/Users/gerbrich/Zotero/storage/GXJE7SAK/Breiman - 2001 - Random Forests.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@article{Cheng2018,
  title = {Using Machine Learning to Advance Synthesis and Use of Conservation and Environmental Evidence},
  author = {Cheng, S. H. and Augustin, C. and Bethel, A. and Gill, D. and Anzaroot, S. and Brun, J. and DeWilde, B. and Minnich, R. C. and Garside, R. and Masuda, Y. J. and Miller, D. C. and Wilkie, D. and Wongbusarakum, S. and McKinnon, M. C.},
  year = {2018},
  volume = {32},
  pages = {762--764},
  issn = {1523-1739},
  doi = {10.1111/cobi.13117},
  abstract = {Article impact statement: Machine learning optimizes processes of systematic evidence synthesis and improves its utility for evidence-based conservation.},
  copyright = {\textcopyright{} 2018 Society for Conservation Biology},
  file = {/Users/gerbrich/Zotero/storage/KZKWUGRY/Cheng et al. - 2018 - Using machine learning to advance synthesis and us.pdf;/Users/gerbrich/Zotero/storage/G3QCVYE7/cobi.html},
  journal = {Conservation Biology},
  keywords = {colandr},
  number = {4}
}

@article{Cleo2019,
  title = {Usability and Acceptability of Four Systematic Review Automation Software Packages: A Mixed Method Design},
  shorttitle = {Usability and Acceptability of Four Systematic Review Automation Software Packages},
  author = {Cleo, Gina and Scott, Anna Mae and Islam, Farhana and Julien, Blair and Beller, Elaine},
  year = {2019},
  month = jun,
  volume = {8},
  pages = {145},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1069-6},
  abstract = {New software packages help to improve the efficiency of conducting a systematic review through automation of key steps in the systematic review. The aim of this study was to gather qualitative data on the usability and acceptability of four systematic review automation software packages (Covidence, SRA-Helper for EndNote, Rayyan and RobotAnalyst) for the citation screening step of a systematic review.},
  file = {/Users/gerbrich/Zotero/storage/M7D3VD7L/Cleo et al. - 2019 - Usability and acceptability of four systematic rev.pdf;/Users/gerbrich/Zotero/storage/QMF2STW7/s13643-019-1069-6.html},
  journal = {Systematic Reviews},
  number = {1}
}

@article{Cohen2006,
  title = {Reducing {{Workload}} in {{Systematic Review Preparation Using Automated Citation Classification}}},
  author = {Cohen, A.M. and Hersh, W.R. and Peterson, K. and Yen, Po-Yin},
  year = {2006},
  volume = {13},
  pages = {206--219},
  issn = {1067-5027},
  doi = {10.1197/jamia.M1929},
  abstract = {Objective: To determine whether automated classification of document citations can be useful in reducing the time spent by experts reviewing journal articles for inclusion in updating systematic reviews of drug class efficacy for treatment of disease., Design: A test collection was built using the annotated reference files from 15 systematic drug class reviews. A voting perceptron-based automated citation classification system was constructed to classify each article as containing high-quality, drug class\textendash{}specific evidence or not. Cross-validation experiments were performed to evaluate performance., Measurements: Precision, recall, and F-measure were evaluated at a range of sample weightings. Work saved over sampling at 95\% recall was used as the measure of value to the review process., Results: A reduction in the number of articles needing manual review was found for 11 of the 15 drug review topics studied. For three of the topics, the reduction was 50\% or greater., Conclusion: Automated document citation classification could be a useful tool in maintaining systematic reviews of the efficacy of drug therapy. Further work is needed to refine the classification system and determine the best manner to integrate the system into the production of systematic reviews.},
  file = {/Users/gerbrich/Zotero/storage/ELCDFZ4C/Cohen et al. - 2006 - Reducing Workload in Systematic Review Preparation.pdf},
  journal = {Journal of the American Medical Informatics Association : JAMIA},
  keywords = {simulation},
  number = {2},
  pmcid = {PMC1447545},
  pmid = {16357352}
}

@article{deVries2020,
  title = {Title, Abstract, and Keyword Searching Resulted in Poor Recovery of Articles in Systematic Reviews of Epidemiologic Practice},
  author = {{\noopsort{vries}}{de Vries}, Bas B.L. Penning and {\noopsort{smeden}}{van Smeden}, Maarten and Rosendaal, Frits R. and Groenwold, Rolf H.H.},
  year = {2020},
  volume = {121},
  pages = {55--61},
  issn = {0895-4356},
  doi = {https://doi.org/10.1016/j.jclinepi.2020.01.009},
  abstract = {Objective Article full texts are often inaccessible via the standard search engines of biomedical literature, such as PubMed and Embase, which are commonly used for systematic reviews. Excluding the full-text bodies from a literature search may result in a small or selective subset of articles being included in the review because of the limited information that is available in only title, abstract, and keywords. This article describes a comparison of search strategies based on a systematic literature review of all articles published in 5 top-ranked epidemiology journals between 2000 and 2017. Study Design and Setting Based on a text-mining approach, we studied how nine different methodological topics were mentioned across text fields (title, abstract, keywords, and text body). The following methodological topics were studied: propensity score methods, inverse probability weighting, marginal structural modeling, multiple imputation, Kaplan-Meier estimation, number needed to treat, measurement error, randomized controlled trial, and latent class analysis. Results In total, 31,641 Hypertext Markup Language (HTML) files were downloaded from the journals' websites. For all methodological topics and journals, at most 50\% of articles with a mention of a topic in the text body also mentioned the topic in the title, abstract, or keywords. For several topics, a gradual decrease over calendar time was observed of reporting in the title, abstract, or keywords. Conclusion Literature searches based on title, abstract, and keywords alone may not be sufficiently sensitive for studies of epidemiological research practice. This study also illustrates the potential value of full-text literature searches, provided there is accessibility of full-text bodies for literature searches.},
  journal = {Journal of Clinical Epidemiology},
  keywords = {Bibliometrics,Epidemiological methods,Statistical methods,Systematic literature review,Text mining}
}

@article{Doeleman2019,
  title = {Immunogenicity of Biologic Agents in Juvenile Idiopathic Arthritis: A Systematic Review and Meta-Analysis},
  shorttitle = {Immunogenicity of Biologic Agents in Juvenile Idiopathic Arthritis},
  author = {Doeleman, Martijn J. H. and {\noopsort{maarseveen}}{van Maarseveen}, Erik M. and Swart, Joost F.},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {1839--1849},
  issn = {1462-0324},
  doi = {10.1093/rheumatology/kez030},
  abstract = {AbstractObjective.  The clinical impact of anti-drug antibodies (ADAbs) in paediatric patients with JIA remains unknown. This systematic review and meta-analysi},
  file = {/Users/gerbrich/Zotero/storage/55ZSQXSV/Doeleman et al. - 2019 - Immunogenicity of biologic agents in juvenile idio.pdf;/Users/gerbrich/Zotero/storage/HWXLZCEM/5365497.html},
  journal = {Rheumatology},
  keywords = {review},
  language = {en},
  number = {10}
}

@inproceedings{Felizardo2011,
  title = {Using {{Visual Text Mining}} to {{Support}} the {{Study Selection Activity}} in {{Systematic Literature Reviews}}},
  booktitle = {International {{Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Felizardo, Katia and Salleh, Norsaremah and Martins, Rafael and Mendes, Emilia and MacDonell, Stephen and Maldonado, Jos{\'e}},
  year = {2011},
  month = sep,
  pages = {77--86},
  doi = {10.1109/ESEM.2011.16},
  abstract = {Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.},
  file = {/Users/gerbrich/Zotero/storage/WRQ3UZ89/Felizardo et al. - 2011 - Using Visual Text Mining to Support the Study Sele.pdf},
  keywords = {revis}
}

@inproceedings{Fernandez-Saez2010,
  title = {{{SLR}}-{{Tool}} - {{A Tool}} for {{Performing Systematic Literature Reviews}}.},
  author = {{Fern{\'a}ndez-S{\'a}ez}, Ana and Genero, Marcela and Romero, Francisco},
  year = {2010},
  month = jan,
  pages = {157--166},
  abstract = {Systematic literature reviews (SLRs) have been gaining a significant amount of attention from Software Engineering researchers since 2004. SLRs are considered to be a new research methodology in Software Engineering, which allow evidence to be gathered with regard to the usefulness or effectiveness of the technology proposed in Software Engineering for the development and maintenance of software products.
This is demonstrated by the growing number of publications related to SLRs that have appeared in recent years. While some tools exist that can support some or all of the activities of the SLR processes defined in (Kitchenham \& Charters, 2007), these are not free. The objective of this paper is to present the SLR-Tool, which is a free tool and is available on the following website: http://alarcosj.esi.uclm.es/SLRTool/, to be used by researchers from any discipline, and not only Software Engineering. SLR-Tool not only supports the process of performing SLRs proposed in (Kitchenham \& Charters, 2007), but also provides additional functionalities such as: refining searches within the documents by applying text mining techniques; defining a classification schema in order to facilitate data synthesis; exporting the results obtained to the format of tables and charts; and exporting the references from the primary studies to the formats used in bibliographic
packages such as EndNote, BibTeX or Ris. This tool has, to date, been used by members of the Alarcos Research Group and PhD students, and their perception of it is that it is both highly necessary and useful. Our purpose now is to circulate the use of SLR-Tool throughout the entire research community in order to obtain feedback from other users.},
  file = {/Users/gerbrich/Zotero/storage/HF4M9SFC/Fernández-Sáez et al. - 2010 - SLR-Tool - A Tool for Performing Systematic Litera.pdf}
}

@misc{Freitas2020,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  year = {2020},
  month = feb,
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  copyright = {MIT},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@misc{Freitas2020a,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  year = {2020},
  month = feb,
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  copyright = {MIT},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@article{GarciaAdeva2006,
  title = {Mining {{Text}} with {{Pimiento}}},
  author = {Garcia Adeva, J.J. and Calvo, R.},
  year = {2006},
  month = jul,
  volume = {10},
  pages = {27--35},
  issn = {1089-7801},
  doi = {10.1109/MIC.2006.85},
  file = {/Users/gerbrich/Zotero/storage/C4J8FE54/Garcia Adeva and Calvo - 2006 - Mining Text with Pimiento.pdf},
  journal = {IEEE Internet Computing},
  language = {en},
  number = {4}
}

@article{Gates2018,
  title = {Technology-Assisted Title and Abstract Screening for Systematic Reviews: A Retrospective Evaluation of the {{Abstrackr}} Machine Learning Tool},
  author = {Gates, Allison and Johnson, Cydney and Hartling, Lisa},
  year = {2018},
  month = mar,
  volume = {7},
  pages = {45},
  issn = {2046-4053},
  doi = {10.1186/s13643-018-0707-8},
  abstract = {Machine learning tools can expedite systematic review (SR) processes by semi-automating citation screening. Abstrackr semi-automates citation screening by predicting relevant records. We evaluated its performance for four screening projects.},
  journal = {Systematic Reviews},
  keywords = {simulation},
  number = {1}
}

@article{Glujovsky2011,
  title = {{{PRM2 EROS}}: {{A New Software For Early Stage Of Systematic REVIEWS}}},
  shorttitle = {{{PRM2 EROS}}},
  author = {Glujovsky, D. and Bardach, A. and Mart{\'i}, S. Garc{\'i}a and Comand{\'e}, D. and Ciapponi, A.},
  year = {2011},
  month = nov,
  volume = {14},
  pages = {A564},
  issn = {1098-3015, 1524-4733},
  doi = {10.1016/j.jval.2011.08.1689},
  abstract = {The workload of the initial phases of the process of developing a systematic review
(SR) is often underestimated. The screening and quality assessment of studies, usually
done by pairs of independent reviewers, is not only time-consuming, but it also is
complicated, tiresome, and prone to mistakes. A computer-software designed to cope
with the initial phases of a SR would be of great help. There is a generalized lack
of development in this regard, and the available options are not very accessible or
affordable.},
  file = {/Users/gerbrich/Zotero/storage/IK4HEMZJ/Glujovsky et al. - 2011 - PRM2 EROS A New Software For Early Stage Of Syste.pdf;/Users/gerbrich/Zotero/storage/NBIWRX58/fulltext.html},
  journal = {Value in Health},
  language = {English},
  number = {7},
  pmid = {21669381}
}

@article{Hall2012,
  title = {A {{Systematic Literature Review}} on {{Fault Prediction Performance}} in {{Software Engineering}}},
  author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
  year = {2012},
  month = nov,
  volume = {38},
  pages = {1276--1304},
  issn = {2326-3881},
  doi = {10.1109/TSE.2011.103},
  abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
  file = {/Users/gerbrich/Zotero/storage/BZZDJVA3/Hall et al. - 2012 - A Systematic Literature Review on Fault Prediction.pdf;/Users/gerbrich/Zotero/storage/J6R99SUF/6035727.html},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {Analytical models,Bayes methods,Context modeling,contextual information,cost reduction,Data models,Fault diagnosis,fault prediction models,fault prediction performance,fault prediction study,feature selection,independent variables,logistic regression,methodological information,naive Bayes,Predictive models,predictive performance,regression analysis,reliable methodology,simple modeling techniques,software engineering,software fault prediction,software fault tolerance,software quality,Software testing,systematic literature review,Systematic literature review,Systematics},
  number = {6}
}

@unpublished{Harkema,
  title = {Replacing {{Scientists}} by {{Machines}}.},
  author = {Harkema, A},
  file = {/Users/gerbrich/Zotero/storage/PY6JF9QJ/Harkema - Replacing Scientists by Machines..pdf},
  language = {en}
}

@article{Harrison2020,
  title = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare: An Evaluation},
  shorttitle = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare},
  author = {Harrison, Hannah and Griffin, Simon J. and Kuhn, Isla and {Usher-Smith}, Juliet A.},
  year = {2020},
  month = jan,
  volume = {20},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-0897-3},
  abstract = {Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T\&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T\&Ab screening for systematic reviews within healthcare research.},
  file = {/Users/gerbrich/Zotero/storage/7UYH9YX8/12874_2020_897_MOESM3_ESM.docx;/Users/gerbrich/Zotero/storage/NMB789XG/Harrison et al. - 2020 - Software tools to support title and abstract scree.pdf;/Users/gerbrich/Zotero/storage/CSUQM7G7/s12874-020-0897-3.html},
  journal = {BMC Medical Research Methodology},
  number = {1}
}

@article{Howard2016,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review},
  shorttitle = {{{SWIFT}}-{{Review}}},
  author = {Howard, Brian E. and Phillips, Jason and Miller, Kyle and Tandon, Arpit and Mav, Deepak and Shah, Mihir R. and Holmgren, Stephanie and Pelch, Katherine E. and Walker, Vickie and Rooney, Andrew A. and Macleod, Malcolm and Shah, Ruchir R. and Thayer, Kristina},
  year = {2016},
  month = may,
  volume = {5},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0263-z},
  abstract = {Background
There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus.

Methods
Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95~\% of known relevant studies and (2) the ``Work Saved over Sampling'' (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics.

Results
Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50~\% of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation.

Conclusions
Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation.

Electronic supplementary material
The online version of this article (doi:10.1186/s13643-016-0263-z) contains supplementary material, which is available to authorized users.},
  file = {/Users/gerbrich/Zotero/storage/QM5IJC8Z/Howard et al. - 2016 - SWIFT-Review a text-mining workbench for systemat.pdf},
  journal = {Systematic Reviews},
  keywords = {swift-review},
  pmcid = {PMC4877757},
  pmid = {27216467}
}

@misc{J.Thomas2010,
  title = {{{EPPI}}-{{Reviewer}} 4: Software for Research Synthesis.},
  author = {{J. Thomas} and {J. Brunton} and {S. Graziosi}},
  year = {2010},
  address = {{London: Social Science Research Unit}},
  howpublished = {UCL Institute of Education}
}

@article{Jelihovschi2014,
  title = {{{ScottKnott}}: {{A Package}} for {{Performing}} the {{Scott}}-{{Knott Clustering Algorithm}} in {{R}}},
  shorttitle = {{{ScottKnott}}},
  author = {Jelihovschi, Enio and Faria, Jos{\'e}},
  year = {2014},
  month = mar,
  volume = {15},
  doi = {10.5540/tema.2014.015.01.0003},
  abstract = {Scott-Knott is an hierarchical clustering algorithm used in the application of ANOVA, when the researcher is comparing treatment means, with a very important characteristic: it does not present any overlapping in its grouping results. We wrote a code, in R, that performs this algorithm starting from vectors, matrix, data.frame, aov or aov.list objects. The results are presented with letters representing groups, as well as through graphics using different colors to differentiate distinct groups. This R package, named ScottKnott is the main topic of this article.},
  file = {/Users/gerbrich/Zotero/storage/2GAVUFKT/Jelihovschi and Faria - 2014 - ScottKnott A Package for Performing the Scott-Kno.pdf},
  journal = {TEMA (S{\~a}o Carlos)}
}

@article{Jelihovschi2014a,
  title = {{{ScottKnott}}: A Package for Performing the {{Scott}}-{{Knott}} Clustering Algorithm in {{R}}},
  shorttitle = {{{ScottKnott}}},
  author = {Jelihovschi, E. G. and Faria, J. C. and Allaman, I. B.},
  year = {2014},
  month = apr,
  volume = {15},
  pages = {3--17},
  publisher = {{Sociedade Brasileira de Matem{\'a}tica Aplicada e Computacional}},
  issn = {2179-8451},
  doi = {10.5540/tema.2014.015.01.0003},
  file = {/Users/gerbrich/Zotero/storage/BH7YUCIH/Jelihovschi et al. - 2014 - ScottKnott a package for performing the Scott-Kno.pdf;/Users/gerbrich/Zotero/storage/DX32ASY6/scielo.html},
  journal = {TEMA (S{\~a}o Carlos)},
  language = {en},
  number = {1}
}

@article{joia2011local,
  title = {Local Affine Multidimensional Projection},
  author = {Joia, Paulo and Coimbra, Danilo and Cuminato, Jose A and Paulovich, Fernando V and Nonato, Luis G},
  year = {2011},
  volume = {17},
  pages = {2563--2571},
  publisher = {{IEEE}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  number = {12}
}

@article{Kohl2018,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  shorttitle = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  year = {2018},
  month = feb,
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K{\"u}hn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  file = {/Users/gerbrich/Zotero/storage/XZX9XDTZ/Kohl et al. - 2018 - Online tools supporting the conduct and reporting .pdf;/Users/gerbrich/Zotero/storage/ECBH5U46/s13750-018-0115-5.html},
  journal = {Environmental Evidence},
  keywords = {cadima},
  number = {1}
}

@article{Kohl2018a,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  year = {2018},
  month = feb,
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K{\"u}hn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  journal = {Environmental Evidence},
  number = {1}
}

@article{Lajeunesse2016,
  title = {Facilitating Systematic Reviews, Data Extraction, and Meta-Analysis with the Metagear Package for {{R}}},
  author = {Lajeunesse, Marc J.},
  year = {2016},
  volume = {7},
  pages = {323--330},
  journal = {Methods in Ecology and Evolution}
}

@article{Le2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014},
  month = may,
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ``powerful,'' ``strong'' and ``Paris'' are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  archivePrefix = {arXiv},
  eprint = {1405.4053},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/IH9XHJUV/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf},
  journal = {arXiv:1405.4053 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,doc2vec,feature_extraction},
  language = {en},
  primaryClass = {cs}
}

@incollection{Lewis1994,
  title = {Heterogeneous {{Uncertainty Sampling}} for {{Supervised Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Lewis, David D. and Catlett, Jason},
  editor = {Cohen, William W. and Hirsh, Haym},
  year = {1994},
  month = jan,
  pages = {148--156},
  publisher = {{Morgan Kaufmann}},
  address = {{San Francisco (CA)}},
  doi = {10.1016/B978-1-55860-335-6.50026-X},
  abstract = {Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.},
  file = {/Users/gerbrich/Zotero/storage/V8XP4FY3/Lewis and Catlett - 1994 - Heterogeneous Uncertainty Sampling for Supervised .pdf;/Users/gerbrich/Zotero/storage/GFRUCYX6/B978155860335650026X.html},
  isbn = {978-1-55860-335-6},
  keywords = {query_strategy},
  language = {en}
}

@article{Matwin2010,
  title = {A New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews},
  author = {Matwin, Stan and Kouznetsov, Alexandre and Inkpen, Diana and Frunza, Oana and O'Blenis, Peter},
  year = {2010},
  month = jul,
  volume = {17},
  pages = {446--453},
  issn = {1067-5027},
  doi = {10.1136/jamia.2010.004325},
  abstract = {Abstract.  Objective To determine whether a factorized version of the complement na{\"i}ve Bayes (FCNB) classifier can reduce the time spent by experts reviewing jo},
  file = {/Users/gerbrich/Zotero/storage/D2P8QY98/Matwin et al. - 2010 - A new algorithm for reducing the workload of exper.pdf;/Users/gerbrich/Zotero/storage/7R2UK3MA/867054.html},
  journal = {Journal of the American Medical Informatics Association},
  language = {en},
  number = {4}
}

@inproceedings{Miller2016,
  title = {{{SWIFT}}-{{Active Screener}}: Reducing Literature Screening Effort through Machine Learning for Systematic Reviews},
  booktitle = {Abstracts of the 24th {{Cochrane Colloquium}}},
  author = {Miller, K. and J, Phillips and {M. Shah} and {B. Howard} and {D. Mav} and {K. Thayer} and {R. Shah}},
  year = {2016},
  publisher = {{John Wiley \& Sons}},
  address = {{Seoul, Korea}}
}

@article{modAL2018,
  title = {{{modAL}}: {{A}} Modular Active Learning Framework for {{Python}}},
  author = {Danka, Tivadar and Horvath, Peter}
}

@article{Molleri,
  title = {{AUTOMATIZA{\c C}{\~A}O DO PROCESSO DE CONDU{\c C}{\~A}O DE REVIS{\~O}ES SISTEM{\'A}TICAS DA LITERATURA EM ENGENHARIA DE SOFTWARE}},
  author = {Moll{\'e}ri, Jefferson Seide},
  pages = {192},
  file = {/Users/gerbrich/Zotero/storage/4VDSH5NP/Molléri - AUTOMATIZAÇÃO DO PROCESSO DE CONDUÇÃO DE REVISÕES .pdf},
  language = {pt}
}

@article{Nagtegaal2019,
  title = {Nudging Healthcare Professionals towards Evidence-Based Medicine: {{A}} Systematic Scoping Review},
  author = {Nagtegaal, Rosanna and Tummers, Lars and Noordegraaf, Mirko and Bekkers, Victor},
  year = {2019},
  volume = {2},
  doi = {doi.org/10.30636/jbpa.22.71},
  file = {/Users/gerbrich/Zotero/storage/PJ8QLKFJ/Nudging healthcare professionals towards evidence-.pdf;/Users/gerbrich/Zotero/storage/RFLHKMUP/71.html},
  journal = {Journal of Behavioral Public Administration},
  keywords = {nudging,review},
  number = {2}
}

@data{Nagtegaal2019a,
  title = {Nudging Healthcare Professionals towards Evidence-Based Medicine: {{A}} Systematic Scoping Review},
  author = {Nagtegaal, Rosanna and Tummers, Lars and Noordegraaf, Mirko and Bekkers, Victor},
  year = {2019},
  publisher = {{Harvard Dataverse}},
  note = {tyep: dataset},
  unf = {UNF:6:xUGLGKnjKj5IOIlZQXTwDg==},
  version = {V1}
}

@article{OMara-Eves2015,
  title = {Using Text Mining for Study Identification in Systematic Reviews: A Systematic Review of Current Approaches},
  shorttitle = {Using Text Mining for Study Identification in Systematic Reviews},
  author = {{O'Mara-Eves}, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
  year = {2015},
  month = jan,
  volume = {4},
  pages = {5},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-5},
  abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
  file = {/Users/gerbrich/Zotero/storage/WWD9DBD2/O’Mara-Eves et al. - 2015 - Using text mining for study identification in syst.pdf;/Users/gerbrich/Zotero/storage/HMT2W9P7/2046-4053-4-5.html},
  journal = {Systematic Reviews},
  number = {1}
}

@article{Ouzzani2016,
  title = {Rayyan\textemdash{}a Web and Mobile App for Systematic Reviews},
  author = {Ouzzani, Mourad and Hammady, Hossam and Fedorowicz, Zbys and Elmagarmid, Ahmed},
  year = {2016},
  volume = {5},
  pages = {210},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0384-4},
  abstract = {Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making.},
  journal = {Systematic Reviews},
  number = {1}
}

@article{PRISMA-PGroup2015,
  title = {Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols ({{PRISMA}}-{{P}}) 2015 Statement},
  author = {{PRISMA-P Group} and Moher, David and Shamseer, Larissa and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A},
  year = {2015},
  month = dec,
  volume = {4},
  pages = {1},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-1},
  abstract = {Systematic reviews should build on a protocol that describes the rationale, hypothesis, and planned methods of the review; few reviews report whether a protocol exists. Detailed, well-described protocols can facilitate the understanding and appraisal of the review methods, as well as the detection of modifications to methods and selective reporting in completed reviews. We describe the development of a reporting guideline, the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 (PRISMA-P 2015). PRISMA-P consists of a 17-item checklist intended to facilitate the preparation and reporting of a robust protocol for the systematic review. Funders and those commissioning reviews might consider mandating the use of the checklist to facilitate the submission of relevant protocol information in funding applications. Similarly, peer reviewers and editors can use the guidance to gauge the completeness and transparency of a systematic review protocol submitted for publication in a journal or other medium.},
  file = {/Users/gerbrich/Zotero/storage/PT8NAI48/PRISMA-P Group et al. - 2015 - Preferred reporting items for systematic review an.pdf},
  journal = {Systematic Reviews},
  language = {en},
  number = {1}
}

@article{Przybyla2018,
  title = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}: {{A}} User Study},
  shorttitle = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}},
  author = {Przyby{\l}a, Piotr and Brockmeier, Austin J. and Kontonatsios, Georgios and Pogam, Marie-Annick Le and McNaught, John and {\noopsort{elm}}von Elm, Erik and Nolan, Kay and Ananiadou, Sophia},
  year = {2018},
  volume = {9},
  pages = {470--488},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1311},
  abstract = {Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95\% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.},
  copyright = {\textcopyright{} 2018 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  file = {/Users/gerbrich/Zotero/storage/7FPGPSK2/Przybyła et al. - 2018 - Prioritising references for systematic reviews wit.pdf;/Users/gerbrich/Zotero/storage/U7KPBBJ4/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {3}
}

@inproceedings{Ramos2003,
  title = {Using Tf-Idf to Determine Word Relevance in Document Queries},
  booktitle = {Proceedings of the First Instructional Conference on Machine Learning},
  author = {Ramos, Juan and others},
  year = {2003},
  volume = {242},
  pages = {133--142},
  file = {/Users/gerbrich/Zotero/storage/YUB2N9D9/Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf},
  organization = {{Piscataway, NJ}}
}

@inproceedings{Rehurek2010,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r{u}}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  month = may,
  pages = {45--50},
  publisher = {{ELRA}},
  address = {{Valletta, Malta}},
  language = {English}
}

@article{Reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde{}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archivePrefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/QWTQF9NF/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/gerbrich/Zotero/storage/SI4PQX5T/1908.html},
  journal = {arXiv:1908.10084 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research}
}

@article{Settles2012,
  title = {Active {{Learning}}},
  author = {Settles, Burr},
  year = {2012},
  month = jun,
  volume = {6},
  pages = {1--114},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00429ED1V01Y201207AIM018},
  file = {/Users/gerbrich/Zotero/storage/2BSTA3X9/Settles - 2012 - Active Learning.pdf},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  language = {en},
  number = {1}
}

@misc{Shapiro2018,
  title = {Shapiromatron/Hawc: 2018-{{Q3}}},
  shorttitle = {Shapiromatron/Hawc},
  author = {Shapiro, Andy and Addington, Josh and Thacker, Shane and Comeaux, Justin},
  year = {2018},
  month = sep,
  doi = {10.5281/zenodo.1414622},
  abstract = {2018 Q3 release.},
  file = {/Users/gerbrich/Zotero/storage/ESIYK5HH/1414622.html},
  howpublished = {Zenodo}
}

@article{Shemilt2014,
  title = {Pinpointing Needles in Giant Haystacks: Use of Text Mining to Reduce Impractical Screening Workload in Extremely Large Scoping Reviews},
  shorttitle = {Pinpointing Needles in Giant Haystacks},
  author = {Shemilt, Ian and Simon, Antonia and Hollands, Gareth J. and Marteau, Theresa M. and Ogilvie, David and O'Mara-Eves, Alison and Kelly, Michael P. and Thomas, James},
  year = {2014},
  volume = {5},
  pages = {31--49},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1093},
  abstract = {In scoping reviews, boundaries of relevant evidence may be initially fuzzy, with refined conceptual understanding of interventions and their proposed mechanisms of action an intended output of the scoping process rather than its starting point. Electronic searches are therefore sensitive, often retrieving very large record sets that are impractical to screen in their entirety. This paper describes methods for applying and evaluating the use of text mining (TM) technologies to reduce impractical screening workload in reviews, using examples of two extremely large-scale scoping reviews of public health evidence (choice architecture (CA) and economic environment (EE)). Electronic searches retrieved {$>$}800,000 (CA) and {$>$}1 million (EE) records. TM technologies were used to prioritise records for manual screening. TM performance was measured prospectively. TM reduced manual screening workload by 90\% (CA) and 88\% (EE) compared with conventional screening (absolute reductions of {$\approx$}430 000 (CA) and {$\approx$}378 000 (EE) records). This study expands an emerging corpus of empirical evidence for the use of TM to expedite study selection in reviews. By reducing screening workload to manageable levels, TM made it possible to assemble and configure large, complex evidence bases that crossed research discipline boundaries. These methods are transferable to other scoping and systematic reviews incorporating conceptual development or explanatory dimensions. \textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  copyright = {\textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/C42P4W4M/Shemilt et al. - 2014 - Pinpointing needles in giant haystacks use of tex.pdf;/Users/gerbrich/Zotero/storage/5QFKVQ8Z/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {scoping review methods,study selection,systematic review methods,text mining},
  language = {en},
  number = {1}
}

@article{Shemilt2016,
  title = {Use of Cost-Effectiveness Analysis to Compare the Efficiency of Study Identification Methods in Systematic Reviews},
  author = {Shemilt, Ian and Khan, Nada and Park, Sophie and Thomas, James},
  year = {2016},
  month = aug,
  volume = {5},
  pages = {140},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0315-4},
  abstract = {Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews.},
  file = {/Users/gerbrich/Zotero/storage/CZQI8A6B/Shemilt et al. - 2016 - Use of cost-effectiveness analysis to compare the .pdf},
  journal = {Systematic Reviews},
  language = {en},
  number = {1}
}

@misc{Shperber2019,
  title = {A Gentle Introduction to {{Doc2Vec}}},
  author = {Shperber, Gidi},
  year = {2019},
  month = nov,
  abstract = {TL;DR},
  file = {/Users/gerbrich/Zotero/storage/MKWGDB73/a-gentle-introduction-to-doc2vec-db3e8c0cce5e.html},
  howpublished = {https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e},
  journal = {Medium},
  language = {en}
}

@article{Stansfield2013,
  title = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research: A Case Study},
  shorttitle = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research},
  author = {Stansfield, Claire and Thomas, James and Kavanagh, Josephine},
  year = {2013},
  volume = {4},
  pages = {230--241},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1082},
  abstract = {Background Scoping reviews of research help determine the feasibility and the resource requirements of conducting a systematic review, and the potential to generate a description of the literature quickly is attractive. Aims To test the utility and applicability of an automated clustering tool to describe and group research studies to improve the efficiency of scoping reviews. Methods A retrospective study of two completed scoping reviews was conducted. This compared the groups and descriptive categories obtained by automatically clustering titles and abstracts with those that had originally been derived using traditional researcher-driven techniques. Results The clustering tool rapidly categorised research into themes, which were useful in some instances, but not in others. This provided a dynamic means to view each dataset. Interpretation was challenging where there were potentially multiple meanings of terms. Where relevant clusters were unambiguous, there was a high precision of relevant studies, although recall varied widely. Conclusions Policy-relevant scoping reviews are often undertaken rapidly, and this could potentially be enhanced by automation depending on the nature of the dataset and information sought. However, it is not a replacement for researcher-developed classification. The possibilities of further applications and potential for use in other types of review are discussed. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/B92KBE6S/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {automatic clustering,automation,information storage and retrieval,lingo,mapping,methods,scoping reviews,text mining},
  language = {en},
  number = {3}
}

@article{Thomas2011,
  title = {Applications of Text Mining within Systematic Reviews},
  author = {Thomas, James and McNaught, John and Ananiadou, Sophia},
  year = {2011},
  volume = {2},
  pages = {1--14},
  doi = {10.1002/jrsm.27},
  abstract = {Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.27},
  journal = {Research Synthesis Methods},
  keywords = {automatic summarization,document classification,document clustering,research synthesis,screening,searching,systematic review,term recognition,text mining},
  number = {1}
}

@inproceedings{Tomassetti2011,
  title = {Linked {{Data}} Approach for Selection Process Automation in {{Systematic Reviews}}},
  booktitle = {15th {{Annual Conference}} on {{Evaluation}} \& {{Assessment}} in {{Software Engineering}} ({{EASE}} 2011)},
  author = {Tomassetti, Federico Cesare Argentino and Rizzo, Giuseppe and Vetro', Antonio and Ardito, Luca and Torchiano, Marco and Morisio, Maurizio},
  year = {2011},
  pages = {31--35},
  publisher = {{IEE}},
  doi = {10.1049/ic.2011.0004},
  abstract = {Background: a systematic review identifies, evaluates and synthesizes the available literature on a given topic using scientific and repeatable methodologies. The significant workload required and the subjectivity bias could affect results. Aim: semi-automate the selection process to reduce the amount of manual work needed and the consequent subjectivity bias. Method: extend and enrich the selection of primary studies using the existing technologies in the field of Linked Data and text mining. We define formally the selection process and we also develop a prototype that implements it. Finally, we conduct a case study that simulates the selection process of a systematic literature published in literature. Results: the process presented in this paper could reduce the work load of 20\% with respect to the work load needed in the fully manually selection, with a recall of 100\%. Conclusions: the extraction of knowledge from scientific studies through Linked Data and text mining techniques could be used in the selection phase of the systematic review process to reduce the work load and subjectivity bias.},
  file = {/Users/gerbrich/Zotero/storage/8PJ2V6EQ/Tomassetti et al. - 2011 - Linked Data approach for selection process automat.pdf;/Users/gerbrich/Zotero/storage/8PT4A537/2381987.html},
  isbn = {978-1-84919-509-6},
  keywords = {dbpedia},
  language = {eng}
}

@article{vandeSchoot2018,
  title = {Bayesian {{PTSD}}-{{Trajectory}} Analysis with Informed Priors Based on a Systematic Literature Search and Expert Elicitation},
  author = {{\noopsort{schoot}}{van de Schoot}, Rens and Sijbrandij, Marit and Depaoli, Sarah and Winter, Sonja D. and Olff, Miranda and {\noopsort{loey}}{van Loey}, Nancy E.},
  year = {2018},
  volume = {53},
  pages = {267--291},
  publisher = {{Routledge}},
  doi = {10.1080/00273171.2017.1412293},
  eprint = {https://doi.org/10.1080/00273171.2017.1412293},
  journal = {Multivariate Behavioral Research},
  number = {2}
}

@misc{VeritasHealthInnovation,
  title = {Covidence Systematic Review Software},
  address = {{Melbourne, Australia}},
  collaborator = {{Veritas Health Innovation}}
}

@inproceedings{Wallace2012,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  month = jan,
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/JHVMNGCY/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {abstrackr,active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@inproceedings{Wallace2012a,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: {{Abstrackr}}},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT}} International Health Informatics Symposium},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2110363.2110464},
  isbn = {978-1-4503-0781-9},
  keywords = {active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  numpages = {6},
  place = {Miami, Florida, USA},
  series = {{{IHI}} '12}
}

@inproceedings{Wallace2012b,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  month = jan,
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/NPDGJ92Z/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@article{Westgate2019,
  title = {Revtools: {{An R}} Package to Support Article Screening for Evidence Synthesis},
  author = {Westgate, Martin J.},
  year = {2019},
  doi = {10.1002/jrsm.1374},
  journal = {Research Synthesis Methods}
}

@misc{Winter2020,
  title = {Additional {{Information}}: {{Bayesian PTSD}}-{{Trajectory Analysis}} with {{Informed Priors}}},
  author = {Winter, Sonja D. and {\noopsort{schoot}}{van de Schoot}, Rens},
  year = {2020},
  month = feb,
  publisher = {{OSF}}
}

@article{Yu2008a,
  title = {{{GAPscreener}}: {{An}} Automatic Tool for Screening Human Genetic Association Literature in {{PubMed}} Using the Support Vector Machine Technique},
  author = {Yu, Wei and Clyne, Melinda and Dolan, Siobhan M. and Yesupriya, Ajay and Wulf, Anja and Liu, Tiebin and Khoury, Muin J. and Gwinn, Marta},
  year = {2008},
  month = apr,
  volume = {9},
  pages = {205},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-9-205},
  abstract = {Synthesis of data from published human genetic association studies is a critical step in the translation of human genome discoveries into health applications. Although genetic association studies account for a substantial proportion of the abstracts in PubMed, identifying them with standard queries is not always accurate or efficient. Further automating the literature-screening process can reduce the burden of a labor-intensive and time-consuming traditional literature search. The Support Vector Machine (SVM), a well-established machine learning technique, has been successful in classifying text, including biomedical literature. The GAPscreener, a free SVM-based software tool, can be used to assist in screening PubMed abstracts for human genetic association studies.},
  journal = {BMC Bioinformatics},
  number = {1}
}

@dataset{Yu2017a,
  title = {Data Sets for {{FASTREAD}}},
  author = {Yu, Zhe and Kraft, Nicholas and Menzies, Tim},
  year = {2017},
  month = aug,
  publisher = {{Zenodo}},
  version = {v1.3}
}

@article{Yu2018,
  title = {Finding Better Active Learners for Faster Literature Reviews},
  author = {Yu, Zhe and Kraft, Nicholas and Menzies, Tim},
  year = {2018},
  month = mar,
  doi = {10.1007/s10664-017-9587-0},
  abstract = {Literature reviews can be time-consuming and tedious to complete. By cataloging and refactoring three state-of-the-art active learning techniques from evidence-based medicine and legal electronic discovery, this paper finds and implements FASTREAD, a faster technique for studying a large corpus of documents, combining and parametrizing the most efficient active learning algorithms. This paper assesses FASTREAD using datasets generated from existing SE literature reviews (Hall, Wahono, Radjenovi{\'c}, Kitchenham et al.). Compared to manual methods, FASTREAD lets researchers find 95\% relevant studies after reviewing an order of magnitude fewer papers. Compared to other state-of-the-art automatic methods, FASTREAD reviews 20\textendash{}50\% fewer studies while finding same number of relevant primary studies in a systematic literature review.},
  file = {/Users/gerbrich/Zotero/storage/MTLEK8XH/Yu et al. - 2018 - Finding better active learners for faster literatu.pdf},
  journal = {Empirical Software Engineering}
}

@article{Yu2018a,
  title = {Finding Better Active Learners for Faster Literature Reviews},
  author = {Yu, Zhe and Kraft, Nicholas A. and Menzies, Tim},
  year = {2018},
  month = mar,
  volume = {23},
  pages = {3161--3186},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9587-0},
  file = {/Users/gerbrich/Zotero/storage/ACKMBK39/Yu et al. - 2018 - Finding better active learners for faster literatu.pdf},
  journal = {Empirical Software Engineering},
  number = {6}
}

@article{Yu2019,
  title = {{{FAST2}}: {{An}} Intelligent Assistant for Finding Relevant Papers},
  shorttitle = {{{FAST2}}},
  author = {Yu, Zhe and Menzies, Tim},
  year = {2019},
  month = apr,
  volume = {120},
  pages = {57--71},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.11.021},
  abstract = {Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. Finding relevant papers can be hard due to the huge amount of candidates provided by search. FAST2 is a novel tool for assisting the researchers to find the next promising paper to read. This paper describes FAST2 and tests it on four large systematic literature review datasets. We show that FAST2 robustly optimizes the human effort to find most (95\%) of the relevant software engineering papers while also compensating for the errors made by humans during the review process. The effectiveness of FAST2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers\textemdash{}which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found\textemdash{}which helps the reviewer decide when to stop the review; (3) a novel human error correction algorithm\textemdash{}which corrects a majority of human misclassifications (labeling relevant papers as non-relevant or vice versa) without imposing too much extra human effort.},
  file = {/Users/gerbrich/Zotero/storage/R65D4TB5/Yu and Menzies - 2019 - FAST2 An intelligent assistant for finding releva.pdf;/Users/gerbrich/Zotero/storage/ITB4FS2M/S0957417418307413.html},
  journal = {Expert Systems with Applications},
  keywords = {Active learning,Literature reviews,Relevance feedback,Selection process,Semi-supervised learning,Text mining},
  language = {en}
}

@inproceedings{Zhang2004,
  title = {The {{Optimality}} of {{Naive Bayes}}},
  booktitle = {Proceedings of the {{Seventeenth International Florida Artificial Intelligence Research Society Conference}}, {{FLAIRS}} 2004},
  author = {Zhang, Harry},
  year = {2004},
  month = jan,
  volume = {2},
  abstract = {Naive Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classifica- tion is surprising, because the conditional independence assumption on which it is based, is rarely true in real- world applications. An open question is: what is the true reason for the surprisingly good performance of naive Bayes in classification? In this paper, we propose a novel explanation on the superb classification performance of naive Bayes. We show that, essentially, the dependence distribution; i.e., how the local dependence of a node distributes in each class, evenly or unevenly, and how the local dependen- cies of all nodes work together, consistently (support- ing a certain classification) or inconsistently (cancel- ing each other out), plays a crucial role. Therefore, no matter how strong the dependences among attributes are, naive Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences can- cel each other out. We propose and prove a sufficient and necessary conditions for the optimality of naive Bayes. Further, we investigate the optimality of naive Bayes under the Gaussian distribution. We present and prove a sufficient condition for the optimality of naive Bayes, in which the dependence between attributes do exist. This provides evidence that dependence among attributes may cancel out each other. In addition, we explore when naive Bayes works well.},
  file = {/Users/gerbrich/Zotero/storage/UP2XUMJR/Zhang - The Optimality of Naive Bayes.pdf},
  keywords = {model}
}

@misc{zotero-254,
  title = {Machine Learning Algorithms for Systematic Review: Reducing Workload in a Preclinical Review of Animal Studies and Reducing Human Screening Error | {{Systematic Reviews}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/FTXIT6LW/s13643-019-0942-7.html},
  howpublished = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-0942-7}
}

@misc{zotero-263,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}},
  file = {/Users/gerbrich/Zotero/storage/GYHDPIKS/MANUAL.html},
  howpublished = {https://pandoc.org/MANUAL.html\#definition-lists}
}

@misc{zotero-298,
  title = {Is It Time to Trust the Robots? {{The}} Reliability and Usability of Machine Learning Tools for Screening in Systematic Reviews | {{Colloquium Abstracts}}},
  shorttitle = {Is It Time to Trust the Robots?},
  file = {/Users/gerbrich/Zotero/storage/S2WIJV9A/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening.html},
  howpublished = {/2019-santiago/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening},
  language = {en}
}

@misc{zotero-349,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review | {{Systematic Reviews}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/HC8TLNZ2/s13643-016-0263-z.html},
  howpublished = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-016-0263-z}
}

@misc{zotero-351,
  title = {Completing a Full Systematic Review in under Two Weeks: Processes, Barriers and Facilitators | {{The}} 26th {{Cochrane Colloquium}}},
  file = {/Users/gerbrich/Zotero/storage/6ZPXMXSL/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators.html},
  howpublished = {https://colloquium2019.cochrane.org/abstracts/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators},
  keywords = {2weeks}
}

@misc{zotero-358,
  title = {{{MDX SLR Tool}}},
  file = {/Users/gerbrich/Zotero/storage/P6GXB2WY/about.html},
  howpublished = {http://ta.mdx.ac.uk/slr/about/}
}

@misc{zotero-383,
  title = {{{SyRF}}: {{Systematic Review Facility}} | {{SyRF}}: {{Systematic Review Facility}}},
  file = {/Users/gerbrich/Zotero/storage/8ALRBI87/syrf.org.uk.html},
  howpublished = {http://syrf.org.uk/}
}

@misc{zotero-389,
  title = {6.2. {{Feature}} Extraction \textemdash{} Scikit-Learn 0.22.1 Documentation},
  file = {/Users/gerbrich/Zotero/storage/6Y956NI9/feature_extraction.html},
  howpublished = {https://scikit-learn.org/stable/modules/feature\_extraction.html}
}

@misc{zotero-436,
  title = {{{DistillerSR}}},
  address = {{Ottowa, Canada}},
  howpublished = {Evidence Partners}
}

@misc{zotero-5773,
  title = {({{PDF}}) {{How}} to {{Read Less}}: {{Better Machine Assisted Reading Methods}} for {{Systematic Literature Reviews}}},
  file = {/Users/gerbrich/Zotero/storage/5MHXQG65/311586326_How_to_Read_Less_Better_Machine_Assisted_Reading_Methods_for_Systematic_Literature_Re.html},
  howpublished = {https://www.researchgate.net/publication/311586326\_How\_to\_Read\_Less\_Better\_Machine\_Assisted\_Reading\_Methods\_for\_Systematic\_Literature\_Reviews}
}

@preamble{ "\newcommand{\noopsort}[1]{} " }

