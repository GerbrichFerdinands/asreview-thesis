
@inproceedings{10.1145/2695664.2695902,
  title = {A Method to Support Search String Building in Systematic Literature Reviews through Visual Text Mining},
  booktitle = {Proceedings of the 30th Annual {{ACM}} Symposium on Applied Computing},
  author = {Mergel, Germano Duarte and Silveira, Milene Selbach and {\noopsort{silva}}{da Silva}, Tiago Silva},
  year = {2015},
  pages = {1594--1601},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2695664.2695902},
  isbn = {978-1-4503-3196-8},
  keywords = {information visualization,systematic literature review,visual text mining},
  numpages = {8},
  place = {Salamanca, Spain},
  series = {{{SAC}} '15}
}

@inproceedings{10.1145/2915970.2916013,
  title = {Improvements in the {{StArt}} Tool to Better Support the Systematic Review Process},
  booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  author = {Fabbri, Sandra and Silva, Cleiton and Hernandes, Elis and Octaviano, F{\'a}bio and Di Thommazo, Andr{\'e} and Belgamo, Anderson},
  year = {2016},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2915970.2916013},
  articleno = {Article 21},
  isbn = {978-1-4503-3691-8},
  keywords = {evidence-based software engineering,StArt tool,systematic literature review,systematic review,tool support},
  numpages = {5},
  place = {Limerick, Ireland},
  series = {{{EASE}} '16}
}

@misc{2014,
  title = {Review {{Manager}} ({{RevMan}})},
  year = {2014},
  address = {{Copenhagen: The Nordic Cochrane Centre}},
  howpublished = {The Cochrane Collaboration}
}

@misc{2019,
  title = {How Does Machine Learning Work at Sysrev?},
  year = {2019},
  month = sep,
  abstract = {Sysrev automatically builds machine learning models at every stage of ~review.

In each sysrev users can screen articles in their corpus by marking them as an
'include' or 'exclude'. ~While reviewing a sysrev screening model is silently
learning how to replicate reviewer decisions. ~Screening models can help
accelerate the review process and eventually automate reviews. ~Automated
reviews will be a key step in the updateable 'living' reviews talked that will
be the next frontier for document rev},
  file = {/Users/gerbrich/Zotero/storage/IS5JUAHZ/machine-learning.html},
  howpublished = {http://blog.sysrev.com/machine-learning/},
  journal = {Sysrev Blog},
  language = {en}
}

@misc{2019a,
  title = {Make a {{Bash Script Executable}}},
  year = {2019},
  month = jun,
  abstract = {Explains how to make a bash script, enable execution, and run it at the command line.},
  file = {/Users/gerbrich/Zotero/storage/B7IPVWMC/make-bash-script-executable.html},
  howpublished = {https://www.andrewcbancroft.com/blog/musings/make-bash-script-executable/},
  language = {en}
}

@misc{262588213843476,
  title = {Align Multiple Ggplot2 Graphs with a Common x Axis and Different y Axes, Each with Different y-Axis Labels.},
  author = {262588213843476},
  abstract = {Align multiple ggplot2 graphs with a common x axis and different y axes, each with different y-axis labels. - plot\_aligned\_series.R},
  file = {/Users/gerbrich/Zotero/storage/T49FIBUT/faa24797bb44addeba79.html},
  howpublished = {https://gist.github.com/tomhopper/faa24797bb44addeba79},
  journal = {Gist},
  language = {en}
}

@article{Allen2007,
  title = {Photoplethysmography and Its Application in Clinical Physiological Measurement},
  author = {Allen, John},
  year = {2007},
  month = mar,
  volume = {28},
  pages = {R1-R39},
  issn = {0967-3334, 1361-6579},
  doi = {10.1088/0967-3334/28/3/R01},
  journal = {Physiological Measurement},
  number = {3}
}

@article{allen2007a,
  title = {Photoplethysmography and Its Application in Clinical Physiological Measurement},
  author = {Allen, John},
  year = {2007},
  month = feb,
  volume = {28},
  pages = {R1--R39},
  issn = {0967-3334},
  doi = {10.1088/0967-3334/28/3/R01},
  abstract = {Photoplethysmography (PPG) is a simple and low-cost optical technique that can be used to detect blood volume changes in the microvascular bed of tissue. It is often used non-invasively to make measurements at the skin surface. The PPG waveform comprises a pulsatile (`AC') physiological waveform attributed to cardiac synchronous changes in the blood volume with each heart beat, and is superimposed on a slowly varying (`DC') baseline with various lower frequency components attributed to respiration, sympathetic nervous system activity and thermoregulation. Although the origins of the components of the PPG signal are not fully understood, it is generally accepted that they can provide valuable information about the cardiovascular system. There has been a resurgence of interest in the technique in recent years, driven by the demand for low cost, simple and portable technology for the primary care and community based clinical settings, the wide availability of low cost and small semiconductor components, and the advancement of computer-based pulse wave analysis techniques. The PPG technology has been used in a wide range of commercially available medical devices for measuring oxygen saturation, blood pressure and cardiac output, assessing autonomic function and also detecting peripheral vascular disease. The introductory sections of the topical review describe the basic principle of operation and interaction of light with tissue, early and recent history of PPG, instrumentation, measurement protocol, and pulse wave analysis. The review then focuses on the applications of PPG in clinical physiological measurements, including clinical physiological monitoring, vascular assessment and autonomic function.},
  journal = {Physiological Measurement},
  keywords = {photoplethysmography},
  language = {en},
  number = {3}
}

@article{Appenzeller-Herzog2019,
  title = {Comparative Effectiveness of Common Therapies for {{Wilson}} Disease: {{A}} Systematic Review and Meta-Analysis of Controlled Studies},
  shorttitle = {Comparative Effectiveness of Common Therapies for {{Wilson}} Disease},
  author = {Appenzeller-Herzog, Christian and Mathes, Tim and Heeres, Marlies L. S. and Weiss, Karl Heinz and Houwen, Roderick H. J. and Ewald, Hannah},
  year = {2019},
  volume = {39},
  pages = {2136--2152},
  issn = {1478-3231},
  doi = {10.1111/liv.14179},
  abstract = {Background \& aims Wilson disease (WD) is a rare disorder of copper metabolism. The objective of this systematic review was to determine the comparative effectiveness and safety of common treatments of WD. Methods We included WD patients of any age or stage and the study drugs D-penicillamine, zinc salts, trientine and tetrathiomolybdate. The control could be placebo, no treatment or any other treatment. We included prospective, retrospective, randomized and non-randomized studies. We searched Medline and Embase via Ovid, the Cochrane Central Register of Controlled Trials, and screened reference lists of included articles. Where possible, we applied random-effects meta-analyses. Results The 23 included studies reported on 2055 patients and mostly compared D-penicillamine to no treatment, zinc, trientine or succimer. One study compared tetrathiomolybdate and trientine. Post-decoppering maintenance therapy was addressed in one study only. Eleven of 23 studies were of low quality. When compared to no treatment, D-penicillamine was associated with a lower mortality (odds ratio 0.013; 95\% CI 0.0010 to 0.17). When compared to zinc, there was no association with mortality (odds ratio 0.73; 95\% CI 0.16 to 3.40) and prevention or amelioration of clinical symptoms (odds ratio 0.84; 95\% CI 0.48 to 1.48). Conversely, D-penicillamine may have a greater impact on side effects and treatment discontinuations than zinc. Conclusions There are some indications that zinc is safer than D-penicillamine therapy while being similarly effective in preventing or reducing hepatic or neurological WD symptoms. Study quality was low warranting cautious interpretation of our findings.},
  copyright = {\textcopyright{} 2019 John Wiley \& Sons A/S. Published by John Wiley \& Sons Ltd},
  file = {/Users/gerbrich/Zotero/storage/XSIFPGCQ/Appenzeller‚ÄêHerzog et al. - 2019 - Comparative effectiveness of common therapies for .pdf;/Users/gerbrich/Zotero/storage/3BAL863U/liv.html},
  journal = {Liver International},
  keywords = {hepatolenticular degeneration,meta-analysis,systematic review,Wilson disease},
  language = {en},
  number = {11}
}

@dataset{Appenzeller-Herzog2020,
  title = {Data from {{Comparative}} Effectiveness of Common Therapies for {{Wilson}} Disease: {{A}} Systematic Review and Meta-analysis of Controlled Studies},
  author = {{Appenzeller-Herzog}, Christian},
  year = {2020},
  month = jan,
  publisher = {{Zenodo}},
  keywords = {dataset,wilson}
}

@techreport{Aromataris2017,
  title = {Joanna {{Briggs Institute Reviewer}}'s {{Manual}}.},
  author = {Aromataris, E and Munn, Z},
  year = {2017},
  institution = {{The Joanna Briggs Institute}}
}

@techreport{ASReviewCoreDevelopmentTeam2019,
  title = {{{ASReview}}: {{Active}} Learning for Systematic Reviews},
  author = {{ASReview Core Development Team}},
  year = {2019},
  address = {{Utrecht, The Netherlands}},
  doi = {10.5281/zenodo.3345592},
  organization = {{Utrecht University}}
}

@inproceedings{Barn2014,
  title = {Slrtool: A Tool to Support Collaborative Systematic Literature Reviews},
  shorttitle = {Slrtool},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Enterprise Information Systems}}, {{Volume}} 2},
  author = {Barn, Balbir and Raimondi, Franco and Athiappan, Lalith and Clark, Tony},
  year = {2014},
  pages = {440--447},
  publisher = {{SCITEPRESS}},
  address = {{Lisbon, Portugal}},
  abstract = {Systematic Literature Reviews (SLRs) are used in a number of fields to produce unbiased accounts of specific research topics. The SLR process is particularly well documented and regulated in the medical field, where it is accepted as the standard mechanism to assess, for instance, the benefits of drugs and treatments. SLRs and meta-analysis techniques are increasingly being used in other fields as well, from Social Sciences to Software Engineering.},
  file = {/Users/gerbrich/Zotero/storage/95IIU42B/Barn et al. - 2014 - Slrtool a tool to support collaborative systemati.pdf;/Users/gerbrich/Zotero/storage/MXNN29UK/14668.html},
  isbn = {978-989-758-028-4},
  language = {en}
}

@inproceedings{Bigendako2020,
  title = {Modeling a {{Tool}} for {{Conducting Systematic Reviews Iteratively}}},
  booktitle = {6th {{International Conference}} on {{Model}}-{{Driven Engineering}} and {{Software Development}}},
  author = {Bigendako, Brice and Syriani, Eugene},
  year = {2020},
  month = feb,
  pages = {552--559},
  abstract = {Digital Library},
  file = {/Users/gerbrich/Zotero/storage/8IK7WPAE/Link.html},
  isbn = {978-989-758-283-7},
  keywords = {relis}
}

@article{Breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  volume = {45},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  file = {/Users/gerbrich/Zotero/storage/GXJE7SAK/Breiman - 2001 - Random Forests.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@article{Butterworth1930,
  title = {On the Theory of Filter Amplifiers},
  author = {Butterworth, S},
  year = {1930},
  volume = {7},
  pages = {536,541},
  file = {/Users/gerbrich/Zotero/storage/5ZNN5CZI/On_the_Theory_of_Filter_Amplifiers.pdf},
  journal = {Experimental Wireless and the Wireless Engineer}
}

@article{ChanPak-Hei,
  title = {Diagnostic {{Performance}} of a {{Smartphone}}-{{Based Photoplethysmographic Application}} for {{Atrial Fibrillation Screening}} in a {{Primary Care Setting}}},
  author = {{Chan Pak-Hei} and {Wong Chun-Ka} and {Poh Yukkee C.} and {Pun Louise} and {Leung Wangie Wan-Chiu} and {Wong Yu-Fai} and {Wong Michelle Man-Ying} and {Poh Ming-Zher} and {Chu Daniel Wai-Sing} and {Siu Chung-Wah}},
  volume = {5},
  pages = {e003428},
  doi = {10.1161/JAHA.116.003428},
  abstract = {BackgroundDiagnosing atrial fibrillation (AF) before ischemic stroke occurs is a priority for stroke prevention in AF. Smartphone camera\textendash{}based photoplethysmographic (PPG) pulse waveform measurement discriminates between different heart rhythms, but its ability to diagnose AF in real-world situations has not been adequately investigated. We sought to assess the diagnostic performance of a standalone smartphone PPG application, Cardiio Rhythm, for AF screening in primary care setting.Methods and ResultsPatients with hypertension, with diabetes mellitus, and/or aged {$\geq$}65~years were recruited. A single-lead ECG was recorded by using the AliveCor heart monitor with tracings reviewed subsequently by 2 cardiologists to provide the reference standard. PPG measurements were performed by using the Cardiio Rhythm smartphone application. AF was diagnosed in 28 (2.76\%) of 1013 participants. The diagnostic sensitivity of the Cardiio Rhythm for AF detection was 92.9\% (95\% CI] 77\textendash{}99\%) and was higher than that of the AliveCor automated algorithm (71.4\% [95\% CI 51\textendash{}87\%]). The specificities of Cardiio Rhythm and the AliveCor automated algorithm were comparable (97.7\% [95\% CI: 97\textendash{}99\%] versus 99.4\% [95\% CI 99\textendash{}100\%]). The positive predictive value of the Cardiio Rhythm was lower than that of the AliveCor automated algorithm (53.1\% [95\% CI 38\textendash{}67\%] versus 76.9\% [95\% CI 56\textendash{}91\%]); both had a very high negative predictive value (99.8\% [95\% CI 99\textendash{}100\%] versus 99.2\% [95\% CI 98\textendash{}100\%]).ConclusionsThe Cardiio Rhythm smartphone PPG application provides an accurate and reliable means to detect AF in patients at risk of developing AF and has the potential to enable population-based screening for AF.},
  file = {/Users/gerbrich/Zotero/storage/C4YHIGMZ/Chan Pak‚ÄêHei et al. - Diagnostic Performance of a Smartphone‚ÄêBased Photo.pdf;/Users/gerbrich/Zotero/storage/7K663XU3/JAHA.116.html},
  journal = {Journal of the American Heart Association},
  number = {7}
}

@article{Cheng2018,
  title = {Using Machine Learning to Advance Synthesis and Use of Conservation and Environmental Evidence},
  author = {Cheng, S. H. and Augustin, C. and Bethel, A. and Gill, D. and Anzaroot, S. and Brun, J. and DeWilde, B. and Minnich, R. C. and Garside, R. and Masuda, Y. J. and Miller, D. C. and Wilkie, D. and Wongbusarakum, S. and McKinnon, M. C.},
  year = {2018},
  volume = {32},
  pages = {762--764},
  issn = {1523-1739},
  doi = {10.1111/cobi.13117},
  abstract = {Article impact statement: Machine learning optimizes processes of systematic evidence synthesis and improves its utility for evidence-based conservation.},
  copyright = {\textcopyright{} 2018 Society for Conservation Biology},
  file = {/Users/gerbrich/Zotero/storage/KZKWUGRY/Cheng et al. - 2018 - Using machine learning to advance synthesis and us.pdf;/Users/gerbrich/Zotero/storage/G3QCVYE7/cobi.html},
  journal = {Conservation Biology},
  keywords = {colandr},
  number = {4}
}

@misc{CholeskyDecompositionExample,
  title = {Cholesky {{Decomposition}} with {{R Example}}},
  file = {/Users/gerbrich/Zotero/storage/QC5WLS7T/cholesky-decomposition-r-example.html},
  howpublished = {https://aaronschlegel.me/cholesky-decomposition-r-example.html}
}

@article{Chugh2014,
  title = {Worldwide {{Epidemiology}} of {{Atrial Fibrillation}}: {{A Global Burden}} of {{Disease}} 2010 {{Study}}},
  shorttitle = {Worldwide {{Epidemiology}} of {{Atrial Fibrillation}}},
  author = {Chugh, Sumeet S. and Havmoeller, Rasmus and Narayanan, Kumar and Singh, David and Rienstra, Michiel and Benjamin, Emelia J. and Gillum, Richard F. and Kim, Young-Hoon and McAnulty, John H. and Zheng, Zhi-Jie and Forouzanfar, Mohammad H. and Naghavi, Mohsen and Mensah, George A. and Ezzati, Majid and Murray, Christopher J. L.},
  year = {2014},
  month = feb,
  volume = {129},
  pages = {837--847},
  issn = {0009-7322},
  doi = {10.1161/CIRCULATIONAHA.113.005119},
  abstract = {Background
The global burden of atrial fibrillation (AF) is unknown.

Methods and Results
We systematically reviewed population-based studies of AF published 1980\textendash{}2010, from the 21 Global Burden of Disease (GBD) regions to estimate global/regional prevalence, incidence, as well as morbidity and mortality related to AF (DisModMR software). Of 377 potential studies identified, 184 met pre-specified eligibility criteria. The estimated number of individuals with AF globally in 2010 was 33{$\cdot$}5 million [(20{$\cdot$}9 million males (UI, 19{$\cdot$}5\textendash{}22{$\cdot$}2 million) and 12{$\cdot$}6 million females (UI, 12{$\cdot$}0\textendash{}13{$\cdot$}7 million)]. Burden associated with AF, measured as disability adjusted life-years (DALYs), increased by 18{$\cdot$}8\% (UI, 15{$\cdot$}8\textendash{}19{$\cdot$}3) in males and 18{$\cdot$}9\% (UI, 15{$\cdot$}8\textendash{}23{$\cdot$}5) in females, from 1990 to 2010. In 1990, the estimated age-adjusted prevalence rates of AF (per 100,000 population) were 569{$\cdot$}5 in males [95\% uncertainty interval (UI), 532{$\cdot$}8\textendash{}612{$\cdot$}7] and 359{$\cdot$}9 in females (UI, 334{$\cdot$}7\textendash{}392{$\cdot$}6); the estimated age-adjusted incidence rates were 60{$\cdot$}7/100,000 person-years in males (UI, 49{$\cdot$}2\textendash{}78{$\cdot$}5) and 43{$\cdot$}8 in females (UI, 35{$\cdot$}9\textendash{}55{$\cdot$}0). In 2010 the prevalence rate increased to 596{$\cdot$}2 (UI, 558{$\cdot$}4\textendash{}636{$\cdot$}7) in males and 373{$\cdot$}1 (UI, 347{$\cdot$}9\textendash{}402{$\cdot$}2) in females; incidence rate increased to 77{$\cdot$}5 (UI, 65{$\cdot$}2\textendash{}95{$\cdot$}4) in males and 59{$\cdot$}5 (UI, 49{$\cdot$}9\textendash{}74{$\cdot$}9) in females. Mortality associated with AF was higher in females, and increased by 2-fold (UI, 2{$\cdot$}0\textendash{}2{$\cdot$}2) and 1{$\cdot$}9-fold (UI, 1{$\cdot$}8\textendash{}2{$\cdot$}0) in males and females, respectively, from 1990 to 2010.

Conclusions
These findings provide evidence of progressive increases in overall burden, incidence, prevalence and AF-associated mortality between 1990\textendash{}2010. Systematic, global surveillance of AF is required to better direct prevention and treatment strategies.},
  file = {/Users/gerbrich/Zotero/storage/C99Q9N8C/Chugh et al. - 2014 - Worldwide Epidemiology of Atrial Fibrillation A G.pdf},
  journal = {Circulation},
  number = {8},
  pmcid = {PMC4151302},
  pmid = {24345399}
}

@article{Cleo2019,
  title = {Usability and Acceptability of Four Systematic Review Automation Software Packages: A Mixed Method Design},
  shorttitle = {Usability and Acceptability of Four Systematic Review Automation Software Packages},
  author = {Cleo, Gina and Scott, Anna Mae and Islam, Farhana and Julien, Blair and Beller, Elaine},
  year = {2019},
  month = jun,
  volume = {8},
  pages = {145},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1069-6},
  abstract = {New software packages help to improve the efficiency of conducting a systematic review through automation of key steps in the systematic review. The aim of this study was to gather qualitative data on the usability and acceptability of four systematic review automation software packages (Covidence, SRA-Helper for EndNote, Rayyan and RobotAnalyst) for the citation screening step of a systematic review.},
  file = {/Users/gerbrich/Zotero/storage/M7D3VD7L/Cleo et al. - 2019 - Usability and acceptability of four systematic rev.pdf;/Users/gerbrich/Zotero/storage/QMF2STW7/s13643-019-1069-6.html},
  journal = {Systematic Reviews},
  number = {1}
}

@incollection{cochrane:14,
  title = {Evidence-Informed Public Health: {{Opportunities}} and Challenges},
  booktitle = {Abstracts of the 22nd Cochrane Colloquium},
  year = {2014},
  month = sep,
  publisher = {{John Wiley \& Sons}},
  venue = {Hyderabad, India}
}

@article{Cohen2005,
  title = {A Survey of Current Work in Biomedical Text Mining},
  author = {Cohen, A. M.},
  year = {2005},
  month = jan,
  volume = {6},
  pages = {57--71},
  issn = {1467-5463, 1477-4054},
  doi = {10/bnfhhs},
  abstract = {The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5\textendash{}10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.},
  file = {/Users/gerbrich/Zotero/storage/7PX3RNPM/Cohen - 2005 - A survey of current work in biomedical text mining.pdf},
  journal = {Briefings in Bioinformatics},
  language = {en},
  number = {1}
}

@article{Cohen2006,
  title = {Reducing {{Workload}} in {{Systematic Review Preparation Using Automated Citation Classification}}},
  author = {Cohen, A.M. and Hersh, W.R. and Peterson, K. and Yen, Po-Yin},
  year = {2006},
  volume = {13},
  pages = {206--219},
  issn = {1067-5027},
  doi = {10.1197/jamia.M1929},
  abstract = {Objective: To determine whether automated classification of document citations can be useful in reducing the time spent by experts reviewing journal articles for inclusion in updating systematic reviews of drug class efficacy for treatment of disease., Design: A test collection was built using the annotated reference files from 15 systematic drug class reviews. A voting perceptron-based automated citation classification system was constructed to classify each article as containing high-quality, drug class\textendash{}specific evidence or not. Cross-validation experiments were performed to evaluate performance., Measurements: Precision, recall, and F-measure were evaluated at a range of sample weightings. Work saved over sampling at 95\% recall was used as the measure of value to the review process., Results: A reduction in the number of articles needing manual review was found for 11 of the 15 drug review topics studied. For three of the topics, the reduction was 50\% or greater., Conclusion: Automated document citation classification could be a useful tool in maintaining systematic reviews of the efficacy of drug therapy. Further work is needed to refine the classification system and determine the best manner to integrate the system into the production of systematic reviews.},
  file = {/Users/gerbrich/Zotero/storage/ELCDFZ4C/Cohen et al. - 2006 - Reducing Workload in Systematic Review Preparation.pdf},
  journal = {Journal of the American Medical Informatics Association : JAMIA},
  keywords = {simulation},
  number = {2},
  pmcid = {PMC1447545},
  pmid = {16357352}
}

@article{Cohen2011,
  title = {Performance of Support-Vector-Machine-Based Classification on 15 Systematic Review Topics Evaluated with the {{WSS}}@95 Measure},
  author = {Cohen, Aaron M},
  year = {2011},
  volume = {18},
  pages = {104},
  issn = {1067-5027},
  doi = {10/cskz4h},
  file = {/Users/gerbrich/Zotero/storage/JHQS5MV2/Cohen - 2011 - Performance of support-vector-machine-based classi.pdf},
  journal = {Journal of the American Medical Informatics Association : JAMIA},
  number = {1},
  pmcid = {PMC3005879},
  pmid = {21169622}
}

@inproceedings{Cormack2014,
  title = {Evaluation of Machine-Learning Protocols for Technology-Assisted Review in Electronic Discovery},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval},
  author = {Cormack, Gordon V. and Grossman, Maura R.},
  year = {2014},
  month = jul,
  pages = {153--162},
  publisher = {{Association for Computing Machinery}},
  address = {{Gold Coast, Queensland, Australia}},
  doi = {10.1145/2600428.2609601},
  abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P{$<$}0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P{$<$}0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
  file = {/Users/gerbrich/Zotero/storage/WSWY6S52/Cormack and Grossman - 2014 - Evaluation of machine-learning protocols for techn.pdf},
  isbn = {978-1-4503-2257-7},
  keywords = {e-discovery,electronic discovery,predictive coding,technology-assisted review},
  series = {{{SIGIR}} '14}
}

@article{deVries2020,
  title = {Title, Abstract, and Keyword Searching Resulted in Poor Recovery of Articles in Systematic Reviews of Epidemiologic Practice},
  author = {{\noopsort{vries}}{de Vries}, Bas B.L. Penning and {\noopsort{smeden}}{van Smeden}, Maarten and Rosendaal, Frits R. and Groenwold, Rolf H.H.},
  year = {2020},
  volume = {121},
  pages = {55--61},
  issn = {0895-4356},
  doi = {https://doi.org/10.1016/j.jclinepi.2020.01.009},
  abstract = {Objective Article full texts are often inaccessible via the standard search engines of biomedical literature, such as PubMed and Embase, which are commonly used for systematic reviews. Excluding the full-text bodies from a literature search may result in a small or selective subset of articles being included in the review because of the limited information that is available in only title, abstract, and keywords. This article describes a comparison of search strategies based on a systematic literature review of all articles published in 5 top-ranked epidemiology journals between 2000 and 2017. Study Design and Setting Based on a text-mining approach, we studied how nine different methodological topics were mentioned across text fields (title, abstract, keywords, and text body). The following methodological topics were studied: propensity score methods, inverse probability weighting, marginal structural modeling, multiple imputation, Kaplan-Meier estimation, number needed to treat, measurement error, randomized controlled trial, and latent class analysis. Results In total, 31,641 Hypertext Markup Language (HTML) files were downloaded from the journals' websites. For all methodological topics and journals, at most 50\% of articles with a mention of a topic in the text body also mentioned the topic in the title, abstract, or keywords. For several topics, a gradual decrease over calendar time was observed of reporting in the title, abstract, or keywords. Conclusion Literature searches based on title, abstract, and keywords alone may not be sufficiently sensitive for studies of epidemiological research practice. This study also illustrates the potential value of full-text literature searches, provided there is accessibility of full-text bodies for literature searches.},
  journal = {Journal of Clinical Epidemiology},
  keywords = {Bibliometrics,Epidemiological methods,Statistical methods,Systematic literature review,Text mining}
}

@article{Doeleman2019,
  title = {Immunogenicity of Biologic Agents in Juvenile Idiopathic Arthritis: A Systematic Review and Meta-Analysis},
  shorttitle = {Immunogenicity of Biologic Agents in Juvenile Idiopathic Arthritis},
  author = {Doeleman, Martijn J. H. and {\noopsort{maarseveen}}{van Maarseveen}, Erik M. and Swart, Joost F.},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {1839--1849},
  issn = {1462-0324},
  doi = {10.1093/rheumatology/kez030},
  abstract = {AbstractObjective.  The clinical impact of anti-drug antibodies (ADAbs) in paediatric patients with JIA remains unknown. This systematic review and meta-analysi},
  file = {/Users/gerbrich/Zotero/storage/55ZSQXSV/Doeleman et al. - 2019 - Immunogenicity of biologic agents in juvenile idio.pdf;/Users/gerbrich/Zotero/storage/HWXLZCEM/5365497.html},
  journal = {Rheumatology},
  keywords = {review},
  language = {en},
  number = {10}
}

@article{Eilers2003,
  title = {A {{Perfect Smoother}}},
  author = {Eilers, Paul H. C.},
  year = {2003},
  month = jul,
  volume = {75},
  pages = {3631--3636},
  issn = {0003-2700},
  doi = {10.1021/ac034173t},
  abstract = {The well-known and popular Savitzky-Golay filter has several disadvantages. A very attractive alternative is a smoother based on penalized least squares, extending ideas presented by Whittaker 80 years ago. This smoother is extremely fast, gives continuous control over smoothness, interpolates automatically, and allows fast leave-one-out cross-validation. It can be programmed in a few lines of Matlab code. Theory, implementation, and applications are presented.},
  file = {/Users/gerbrich/Zotero/storage/6YJRJCS3/Eilers - 2003 - A Perfect Smoother.pdf;/Users/gerbrich/Zotero/storage/SS2WVUK4/ac034173t.html},
  journal = {Analytical Chemistry},
  number = {14}
}

@article{Elgendi2016,
  title = {Optimal {{Signal Quality Index}} for {{Photoplethysmogram Signals}}},
  author = {Elgendi, Mohamed},
  year = {2016},
  month = dec,
  volume = {3},
  pages = {21},
  doi = {10.3390/bioengineering3040021},
  abstract = {A photoplethysmogram (PPG) is a noninvasive circulatory signal related to the pulsatile volume of blood in tissue and is typically collected by pulse oximeters. PPG signals collected via mobile devices are prone to artifacts that negatively impact measurement accuracy, which can lead to a significant number of misleading diagnoses. Given the rapidly increased use of mobile devices to collect PPG signals, developing an optimal signal quality index (SQI) is essential to classify the signal quality from these devices. Eight SQIs were developed and tested based on: perfusion, kurtosis, skewness, relative power, non-stationarity, zero crossing, entropy, and the matching of systolic wave detectors. Two independent annotators annotated all PPG data (106 recordings, 60 s each) and a third expert conducted the adjudication of differences. The independent annotators labeled each PPG signal with one of the following labels: excellent, acceptable or unfit for diagnosis. All indices were compared using Mahalanobis distance, linear discriminant analysis, quadratic discriminant analysis, and support vector machine with leave-one-out cross-validation. The skewness index outperformed the other seven indices in differentiating between excellent PPG and acceptable, acceptable combined with unfit, and unfit recordings, with overall     F 1     scores of 86.0\%, 87.2\%, and 79.1\%, respectively.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/gerbrich/Zotero/storage/BH9H36XR/Elgendi - 2016 - Optimal Signal Quality Index for Photoplethysmogra.pdf;/Users/gerbrich/Zotero/storage/S9FLYQJG/21.html},
  journal = {Bioengineering},
  keywords = {affordable healthcare,global health,mobile health,noise detection,point-of-care device,pulsatile signal,pulse oximeter,signal segmentation,telemonitoring,wearable sensors},
  language = {en},
  number = {4}
}

@inproceedings{Fallet2018,
  title = {Signal Processing Techniques for Cardiovascular Monitoring Applications Using Conventional and Video-Based Photoplethysmography},
  author = {Fallet, Sibylle},
  year = {2018},
  doi = {10.5075/epfl-thesis-8493},
  abstract = {Photoplethysmography (PPG)-based monitoring devices will probably play a decisive role in healthcare environment of the future, which will be preventive, predictive, personalized and participatory. Indeed, this optical technology presents several practical advantages over gold standard methods based on electrocardiography, because PPG wearable devices can be comfortably used for long-term continuous monitoring during daily life activities. Contactless video-based PPG technique, also known as imaging photoplethysmography (iPPG), has also attracted much attention recently. In that case, the cardiac pulse is remotely measured from the subtle skin color changes resulting from the blood circulation, using a simple video camera. PPG/iPPG have a lot of potential for a wide range of cardiovascular applications. Hence, there is a substantial need for signal processing techniques to explore these applications and to improve the reliability of the PPG/iPPG-based parameters. A part of the thesis is dedicated to the development of robust processing schemes to estimate heart rate from the PPG/iPPG signals. The proposed approaches were built on adaptive frequency tracking algorithms that were previously developed in our group. These tools, based on adaptive band-pass filters, provide instantaneous frequency estimates of the input signal(s) with a very low time delay, making them suitable for real-time applications. In case of conventional PPG, a prior adaptive noise cancellation step involving the use of accelerometer signals was also necessary to reconstruct clean PPG signals during the regions corrupted by motion artifacts. Regarding iPPG, after comparing different regions of interest on the subject face, we hypothesized that the simultaneous use of different iPPG signal derivation methods (i.e. methods to derive the iPPG time series from the pixel values of the consecutive frames) could be advantageous. Methods to assess signal quality online and to incorporate it into instantaneous frequency estimation were also examined and successfully applied to improve system reliability. This thesis also explored different innovative applications involving PPG/iPPG signals. The detection of atrial fibrillation was studied. Novel features derived directly from the PPG waveforms, designed to reflect the morphological changes observed during arrhythmic episodes, were proposed and proven to be successful for atrial fibrillation detection. Arrhythmia detection and robust heart rate estimation approaches were combined in another study aimed at reducing the number of false arrhythmia alarms in the intensive care unit by exploiting signals from independent sources, including PPG. Evaluation on a hidden dataset demonstrated that the number of false alarms was drastically reduced while almost no true alarm was suppressed. Finally, other aspects of the iPPG technology were examined, such as the measurement of pulse rate variability indexes from the iPPG signals and the estimation of respiratory rate from the iPPG interbeat intervals.},
  file = {/Users/gerbrich/Zotero/storage/ECXFXM37/Fallet - 2018 - Signal processing techniques for cardiovascular mo.pdf},
  keywords = {accelerometers,algorithm,Atrial Fibrillation,Atrial Septal Defects,Broadcast delay,Color,Contactless smart card,Electrocardiography,Estimated,Heart Atrium,Heart rate variability,Instantaneous phase,Morphologic artifacts,Personalization,Photoplethysmography,Pixel,Real-time clock,Region of interest,Respiratory rate,Sensor,Signal processing,Spectral density estimation,SRGN gene,Time series,Wearable technology}
}

@inproceedings{Felizardo2011,
  title = {Using {{Visual Text Mining}} to {{Support}} the {{Study Selection Activity}} in {{Systematic Literature Reviews}}},
  booktitle = {International {{Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Felizardo, Katia and Salleh, Norsaremah and Martins, Rafael and Mendes, Emilia and MacDonell, Stephen and Maldonado, Jos{\'e}},
  year = {2011},
  month = sep,
  pages = {77--86},
  doi = {10.1109/ESEM.2011.16},
  abstract = {Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.},
  file = {/Users/gerbrich/Zotero/storage/WRQ3UZ89/Felizardo et al. - 2011 - Using Visual Text Mining to Support the Study Sele.pdf},
  keywords = {revis}
}

@inproceedings{Fernandez-Saez2010,
  title = {{{SLR}}-{{Tool}} - {{A Tool}} for {{Performing Systematic Literature Reviews}}.},
  author = {{Fern{\'a}ndez-S{\'a}ez}, Ana and Genero, Marcela and Romero, Francisco},
  year = {2010},
  month = jan,
  pages = {157--166},
  abstract = {Systematic literature reviews (SLRs) have been gaining a significant amount of attention from Software Engineering researchers since 2004. SLRs are considered to be a new research methodology in Software Engineering, which allow evidence to be gathered with regard to the usefulness or effectiveness of the technology proposed in Software Engineering for the development and maintenance of software products.
This is demonstrated by the growing number of publications related to SLRs that have appeared in recent years. While some tools exist that can support some or all of the activities of the SLR processes defined in (Kitchenham \& Charters, 2007), these are not free. The objective of this paper is to present the SLR-Tool, which is a free tool and is available on the following website: http://alarcosj.esi.uclm.es/SLRTool/, to be used by researchers from any discipline, and not only Software Engineering. SLR-Tool not only supports the process of performing SLRs proposed in (Kitchenham \& Charters, 2007), but also provides additional functionalities such as: refining searches within the documents by applying text mining techniques; defining a classification schema in order to facilitate data synthesis; exporting the results obtained to the format of tables and charts; and exporting the references from the primary studies to the formats used in bibliographic
packages such as EndNote, BibTeX or Ris. This tool has, to date, been used by members of the Alarcos Research Group and PhD students, and their perception of it is that it is both highly necessary and useful. Our purpose now is to circulate the use of SLR-Tool throughout the entire research community in order to obtain feedback from other users.},
  file = {/Users/gerbrich/Zotero/storage/HF4M9SFC/Fern√°ndez-S√°ez et al. - 2010 - SLR-Tool - A Tool for Performing Systematic Litera.pdf}
}

@misc{Freitas2020,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  year = {2020},
  month = feb,
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  copyright = {MIT},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@misc{Freitas2020a,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  year = {2020},
  month = feb,
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  copyright = {MIT},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@article{GarciaAdeva2006,
  title = {Mining {{Text}} with {{Pimiento}}},
  author = {Garcia Adeva, J.J. and Calvo, R.},
  year = {2006},
  month = jul,
  volume = {10},
  pages = {27--35},
  issn = {1089-7801},
  doi = {10.1109/MIC.2006.85},
  file = {/Users/gerbrich/Zotero/storage/C4J8FE54/Garcia Adeva and Calvo - 2006 - Mining Text with Pimiento.pdf},
  journal = {IEEE Internet Computing},
  language = {en},
  number = {4}
}

@article{Gates2018,
  title = {Technology-Assisted Title and Abstract Screening for Systematic Reviews: A Retrospective Evaluation of the {{Abstrackr}} Machine Learning Tool},
  author = {Gates, Allison and Johnson, Cydney and Hartling, Lisa},
  year = {2018},
  month = mar,
  volume = {7},
  pages = {45},
  issn = {2046-4053},
  doi = {10.1186/s13643-018-0707-8},
  abstract = {Machine learning tools can expedite systematic review (SR) processes by semi-automating citation screening. Abstrackr semi-automates citation screening by predicting relevant records. We evaluated its performance for four screening projects.},
  journal = {Systematic Reviews},
  keywords = {simulation},
  number = {1}
}

@article{Glujovsky2011,
  title = {{{PRM2 EROS}}: {{A New Software For Early Stage Of Systematic REVIEWS}}},
  shorttitle = {{{PRM2 EROS}}},
  author = {Glujovsky, D. and Bardach, A. and Mart{\'i}, S. Garc{\'i}a and Comand{\'e}, D. and Ciapponi, A.},
  year = {2011},
  month = nov,
  volume = {14},
  pages = {A564},
  issn = {1098-3015, 1524-4733},
  doi = {10.1016/j.jval.2011.08.1689},
  abstract = {The workload of the initial phases of the process of developing a systematic review
(SR) is often underestimated. The screening and quality assessment of studies, usually
done by pairs of independent reviewers, is not only time-consuming, but it also is
complicated, tiresome, and prone to mistakes. A computer-software designed to cope
with the initial phases of a SR would be of great help. There is a generalized lack
of development in this regard, and the available options are not very accessible or
affordable.},
  file = {/Users/gerbrich/Zotero/storage/IK4HEMZJ/Glujovsky et al. - 2011 - PRM2 EROS A New Software For Early Stage Of Syste.pdf;/Users/gerbrich/Zotero/storage/NBIWRX58/fulltext.html},
  journal = {Value in Health},
  language = {English},
  number = {7},
  pmid = {21669381}
}

@article{Hall2012,
  title = {A {{Systematic Literature Review}} on {{Fault Prediction Performance}} in {{Software Engineering}}},
  author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
  year = {2012},
  month = nov,
  volume = {38},
  pages = {1276--1304},
  issn = {2326-3881},
  doi = {10.1109/TSE.2011.103},
  abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
  file = {/Users/gerbrich/Zotero/storage/BZZDJVA3/Hall et al. - 2012 - A Systematic Literature Review on Fault Prediction.pdf;/Users/gerbrich/Zotero/storage/J6R99SUF/6035727.html},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {Analytical models,Bayes methods,Context modeling,contextual information,cost reduction,Data models,Fault diagnosis,fault prediction models,fault prediction performance,fault prediction study,feature selection,independent variables,logistic regression,methodological information,naive Bayes,Predictive models,predictive performance,regression analysis,reliable methodology,simple modeling techniques,software engineering,software fault prediction,software fault tolerance,software quality,Software testing,systematic literature review,Systematic literature review,Systematics},
  number = {6}
}

@unpublished{Harkema,
  title = {Replacing {{Scientists}} by {{Machines}}.},
  author = {Harkema, A},
  file = {/Users/gerbrich/Zotero/storage/PY6JF9QJ/Harkema - Replacing Scientists by Machines..pdf},
  language = {en}
}

@article{Harrison2020,
  title = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare: An Evaluation},
  shorttitle = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare},
  author = {Harrison, Hannah and Griffin, Simon J. and Kuhn, Isla and {Usher-Smith}, Juliet A.},
  year = {2020},
  month = jan,
  volume = {20},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-0897-3},
  abstract = {Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T\&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T\&Ab screening for systematic reviews within healthcare research.},
  file = {/Users/gerbrich/Zotero/storage/7UYH9YX8/12874_2020_897_MOESM3_ESM.docx;/Users/gerbrich/Zotero/storage/NMB789XG/Harrison et al. - 2020 - Software tools to support title and abstract scree.pdf;/Users/gerbrich/Zotero/storage/CSUQM7G7/s12874-020-0897-3.html},
  journal = {BMC Medical Research Methodology},
  number = {1}
}

@article{Harrison2020a,
  title = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare: An Evaluation},
  shorttitle = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare},
  author = {Harrison, Hannah and Griffin, Simon J. and Kuhn, Isla and {Usher-Smith}, Juliet A.},
  year = {2020},
  month = jan,
  volume = {20},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-0897-3},
  abstract = {Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T\&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T\&Ab screening for systematic reviews within healthcare research.},
  file = {/Users/gerbrich/Zotero/storage/U7IG2Q68/Harrison et al. - 2020 - Software tools to support title and abstract scree.pdf;/Users/gerbrich/Zotero/storage/GYV2RATM/s12874-020-0897-3.html},
  journal = {BMC Medical Research Methodology},
  number = {1}
}

@article{Howard2016,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review},
  shorttitle = {{{SWIFT}}-{{Review}}},
  author = {Howard, Brian E. and Phillips, Jason and Miller, Kyle and Tandon, Arpit and Mav, Deepak and Shah, Mihir R. and Holmgren, Stephanie and Pelch, Katherine E. and Walker, Vickie and Rooney, Andrew A. and Macleod, Malcolm and Shah, Ruchir R. and Thayer, Kristina},
  year = {2016},
  month = may,
  volume = {5},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0263-z},
  abstract = {Background
There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus.

Methods
Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95~\% of known relevant studies and (2) the ``Work Saved over Sampling'' (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics.

Results
Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50~\% of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation.

Conclusions
Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation.

Electronic supplementary material
The online version of this article (doi:10.1186/s13643-016-0263-z) contains supplementary material, which is available to authorized users.},
  file = {/Users/gerbrich/Zotero/storage/QM5IJC8Z/Howard et al. - 2016 - SWIFT-Review a text-mining workbench for systemat.pdf},
  journal = {Systematic Reviews},
  keywords = {swift-review},
  pmcid = {PMC4877757},
  pmid = {27216467}
}

@article{Islam2016,
  title = {Rhythm-Based Heartbeat Duration Normalization for Atrial Fibrillation Detection},
  author = {Islam, Md Saiful and Ammour, Nassim and Alajlan, Naif and Aboalsamh, Hatim},
  year = {2016},
  month = may,
  volume = {72},
  pages = {160--169},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2016.03.015},
  abstract = {Background
Screening of atrial fibrillation (AF) for high-risk patients including all patients aged 65 years and older is important for prevention of risk of stroke. Different technologies such as modified blood pressure monitor, single lead ECG-based finger-probe, and smart phone using plethysmogram signal have been emerging for this purpose. All these technologies use irregularity of heartbeat duration as a feature for AF detection. We have investigated a normalization method of heartbeat duration for improved AF detection.
Method
AF is an arrhythmia in which heartbeat duration generally becomes irregularly irregular. From a window of heartbeat duration, we estimate the possible rhythm of the majority of heartbeats and normalize duration of all heartbeats in the window based on the rhythm so that we can measure the irregularity of heartbeats for both AF and non-AF rhythms in the same scale. Irregularity is measured by the entropy of distribution of the normalized duration. Then we classify a window of heartbeats as AF or non-AF by thresholding the measured irregularity. The effect of this normalization is evaluated by comparing AF detection performances using duration with the normalization, without normalization, and with other existing normalizations.
Results
Sensitivity and specificity of AF detection using normalized heartbeat duration were tested on two landmark databases available online and compared with results of other methods (with/without normalization) by receiver operating characteristic (ROC) curves. ROC analysis showed that the normalization was able to improve the performance of AF detection and it was consistent for a wide range of sensitivity and specificity for use of different thresholds. Detection accuracy was also computed for equal rates of sensitivity and specificity for different methods. Using normalized heartbeat duration, we obtained 96.38\% accuracy which is more than 4\% improvement compared to AF detection without normalization.
Conclusions
The proposed normalization method was found useful for improving performance and robustness of AF detection. Incorporation of this method in a screening device could be crucial to reduce the risk of AF-related stroke. In general, the incorporation of the rhythm-based normalization in an AF detection method seems important for developing a robust AF screening device.},
  file = {/Users/gerbrich/Zotero/storage/8MIML8YM/Islam et al. - 2016 - Rhythm-based heartbeat duration normalization for .pdf;/Users/gerbrich/Zotero/storage/YL8US53T/S0010482516300683.html},
  journal = {Computers in Biology and Medicine},
  keywords = {Atrial fibrillation,Electrocardiographic monitoring,Entropy,Heartbeat normalization,Heartbeat rhythm,Screening},
  language = {en}
}

@misc{J.Thomas2010,
  title = {{{EPPI}}-{{Reviewer}} 4: Software for Research Synthesis.},
  author = {{J. Thomas} and {J. Brunton} and {S. Graziosi}},
  year = {2010},
  address = {{London: Social Science Research Unit}},
  howpublished = {UCL Institute of Education}
}

@article{Jelihovschi2014,
  title = {{{ScottKnott}}: {{A Package}} for {{Performing}} the {{Scott}}-{{Knott Clustering Algorithm}} in {{R}}},
  shorttitle = {{{ScottKnott}}},
  author = {Jelihovschi, Enio and Faria, Jos{\'e}},
  year = {2014},
  month = mar,
  volume = {15},
  doi = {10.5540/tema.2014.015.01.0003},
  abstract = {Scott-Knott is an hierarchical clustering algorithm used in the application of ANOVA, when the researcher is comparing treatment means, with a very important characteristic: it does not present any overlapping in its grouping results. We wrote a code, in R, that performs this algorithm starting from vectors, matrix, data.frame, aov or aov.list objects. The results are presented with letters representing groups, as well as through graphics using different colors to differentiate distinct groups. This R package, named ScottKnott is the main topic of this article.},
  file = {/Users/gerbrich/Zotero/storage/2GAVUFKT/Jelihovschi and Faria - 2014 - ScottKnott A Package for Performing the Scott-Kno.pdf},
  journal = {TEMA (S{\~a}o Carlos)}
}

@article{Jelihovschi2014a,
  title = {{{ScottKnott}}: A Package for Performing the {{Scott}}-{{Knott}} Clustering Algorithm in {{R}}},
  shorttitle = {{{ScottKnott}}},
  author = {Jelihovschi, E. G. and Faria, J. C. and Allaman, I. B.},
  year = {2014},
  month = apr,
  volume = {15},
  pages = {3--17},
  publisher = {{Sociedade Brasileira de Matem{\'a}tica Aplicada e Computacional}},
  issn = {2179-8451},
  doi = {10.5540/tema.2014.015.01.0003},
  file = {/Users/gerbrich/Zotero/storage/BH7YUCIH/Jelihovschi et al. - 2014 - ScottKnott a package for performing the Scott-Kno.pdf;/Users/gerbrich/Zotero/storage/DX32ASY6/scielo.html},
  journal = {TEMA (S{\~a}o Carlos)},
  language = {en},
  number = {1}
}

@article{joia2011local,
  title = {Local Affine Multidimensional Projection},
  author = {Joia, Paulo and Coimbra, Danilo and Cuminato, Jose A and Paulovich, Fernando V and Nonato, Luis G},
  year = {2011},
  volume = {17},
  pages = {2563--2571},
  publisher = {{IEEE}},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  number = {12}
}

@article{Karlen2012,
  title = {Photoplethysmogram Signal Quality Estimation Using Repeated {{Gaussian}} Filters and Cross-Correlation},
  author = {Karlen, Walter and Kobayashi, K and Ansermino, Mark and Dumont, Guy},
  year = {2012},
  month = sep,
  volume = {33},
  pages = {1617--29},
  doi = {10.1088/0967-3334/33/10/1617},
  abstract = {Pulse oximeters are monitors that noninvasively measure heart rate and blood oxygen saturation (SpO(2)). Unfortunately, pulse oximetry is prone to artifacts which negatively impact the accuracy of the measurement and can cause a significant number of false alarms. We have developed an algorithm to segment pulse oximetry signals into pulses and estimate the signal quality in real time. The algorithm iteratively calculates a signal quality index (SQI) ranging from 0 to 100. In the presence of artifacts and irregular signal morphology, the algorithm outputs a low SQI number. The pulse segmentation algorithm uses the derivative of the signal to find pulse slopes and an adaptive set of repeated Gaussian filters to select the correct slopes. Cross-correlation of consecutive pulse segments is used to estimate signal quality. Experimental results using two different benchmark data sets showed a good pulse detection rate with a sensitivity of 96.21\% and a positive predictive value of 99.22\%, which was equivalent to the available reference algorithm. The novel SQI algorithm was effective and produced significantly lower SQI values in the presence of artifacts compared to SQI values during clean signals. The SQI algorithm may help to guide untrained pulse oximeter users and also help in the design of advanced algorithms for generating smart alarms.},
  file = {/Users/gerbrich/Zotero/storage/QRJV6CPD/Karlen et al. - 2012 - Photoplethysmogram signal quality estimation using.pdf},
  journal = {Physiological measurement}
}

@article{Kilicoglu2009,
  title = {Towards {{Automatic Recognition}} of {{Scientifically Rigorous Clinical Research Evidence}}},
  author = {Kilicoglu, H. and {Demner-Fushman}, D. and Rindflesch, T. C. and Wilczynski, N. L. and Haynes, R. B.},
  year = {2009},
  month = jan,
  volume = {16},
  pages = {25--31},
  issn = {1067-5027, 1527-974X},
  doi = {10/bjkhh9},
  file = {/Users/gerbrich/Zotero/storage/NY5RBQ5D/Kilicoglu et al. - 2009 - Towards Automatic Recognition of Scientifically Ri.pdf},
  journal = {Journal of the American Medical Informatics Association},
  language = {en},
  number = {1}
}

@article{Kohl2018,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  shorttitle = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  year = {2018},
  month = feb,
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K{\"u}hn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  file = {/Users/gerbrich/Zotero/storage/XZX9XDTZ/Kohl et al. - 2018 - Online tools supporting the conduct and reporting .pdf;/Users/gerbrich/Zotero/storage/ECBH5U46/s13750-018-0115-5.html},
  journal = {Environmental Evidence},
  keywords = {cadima},
  number = {1}
}

@article{Kohl2018a,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  year = {2018},
  month = feb,
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K{\"u}hn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  journal = {Environmental Evidence},
  number = {1}
}

@incollection{Kurylyak2012,
  title = {Smartphone-{{Based Photoplethysmogram Measurement}}},
  author = {Kurylyak, Yuriy and Lamonaca, Francesco and Grimaldi, Domenico},
  year = {2012},
  month = jan,
  pages = {135--164},
  abstract = {Smartphones have become one of the widest and often used devices that people bring almost every time and everywhere. Their computational capacities allow their application to many every-day tasks. One of them is health state monitoring. This chapter presents a smartphone-based photoplethysmogram (PPG) acquisition and pulse rate evaluation system. The proposal was designed for different smartphone models, equipped with a LED or not. Different cameras represent the same acquired information in different ways: changes may occur in color saturation, resolution, frame rate, etc. Therefore, several smartphones were used to define the common characteristics of the captured video, and establish proper criteria for PPG extraction. Moreover, the appropriate algorithms were proposed and validated to verify the correct device usage, the system calibration, the PPG and pulse rate evaluation. The experimental results have confirmed the correctness and suitability of the proposed method with respect to the medical pulse measurement instruments.},
  file = {/Users/gerbrich/Zotero/storage/8GIA8PCX/Kurylyak et al. - 2012 - Smartphone-Based Photoplethysmogram Measurement.pdf}
}

@incollection{kurylyakSmartphoneBasedPhotoplethysmogramMeasurement2012,
  title = {Smartphone-{{Based Photoplethysmogram Measurement}}},
  author = {Kurylyak, Yuriy and Lamonaca, Francesco and Grimaldi, Domenico},
  year = {2012},
  month = jan,
  pages = {135--164},
  abstract = {Smartphones have become one of the widest and often used devices that people bring almost every time and everywhere. Their computational capacities allow their application to many every-day tasks. One of them is health state monitoring. This chapter presents a smartphone-based photoplethysmogram (PPG) acquisition and pulse rate evaluation system. The proposal was designed for different smartphone models, equipped with a LED or not. Different cameras represent the same acquired information in different ways: changes may occur in color saturation, resolution, frame rate, etc. Therefore, several smartphones were used to define the common characteristics of the captured video, and establish proper criteria for PPG extraction. Moreover, the appropriate algorithms were proposed and validated to verify the correct device usage, the system calibration, the PPG and pulse rate evaluation. The experimental results have confirmed the correctness and suitability of the proposed method with respect to the medical pulse measurement instruments.},
  file = {/Users/gerbrich/Zotero/storage/LGJ73ZX6/Kurylyak et al. - 2012 - Smartphone-Based Photoplethysmogram Measurement.pdf}
}

@article{Lajeunesse2016,
  title = {Facilitating Systematic Reviews, Data Extraction, and Meta-Analysis with the Metagear Package for {{R}}},
  author = {Lajeunesse, Marc J.},
  year = {2016},
  volume = {7},
  pages = {323--330},
  journal = {Methods in Ecology and Evolution}
}

@article{Lang2017,
  title = {Beyond {{Fitbit}}: {{A Critical Appraisal}} of {{Optical Heart Rate Monitoring Wearables}} and {{Apps}}, {{Their Current Limitations}} and {{Legal Implications}}},
  shorttitle = {Beyond {{Fitbit}}},
  author = {Lang, Michael},
  year = {2017},
  month = dec,
  volume = {28},
  pages = {39},
  abstract = {Fitness and health-care-oriented wearables and apps have been around for a couple of years and are still gaining momentum. Over time, they have begun to harness considerable computational power and to incorporate increasingly sophisticated sensors, eventually resulting in a blurring of the lines between consumer electronics and medical devices. While their benefits and potentials are undisputed, the overly optimistic appraisal commonly encountered in both mass media and academic literature does not adequately reflect unsolved problems and inherent limitations of these devices. This Article will argue that while these issues have long been known to the engineering community, their relevance and legal implications appear to have been grossly underestimated. January 2016 marked a turning point, as news of two class-action lawsuits filed against major manufacturer Fitbit brought widespread attention to accuracy, reliability, and safety concerns regarding these devices. This Article will provide a concise overview of optical heart rate monitoring technology, the current state of the art, and research trends. It will be argued that under real-world scenarios these apps and devices are currently inherently inaccurate and unreliable, with even greater problems on the horizon as the industry shifts towards areas such as heart rate variability monitoring or the detection of cardiac arrhythmias.

Available at http://heinonline.org/HOL/P?h=hein.journals/albnyst28\&i=45},
  file = {/Users/gerbrich/Zotero/storage/ZE6KTHZ9/Lang - 2017 - Beyond Fitbit A Critical Appraisal of Optical Hea.pdf},
  journal = {Albany law journal of science \& technology}
}

@article{Le2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014},
  month = may,
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ``powerful,'' ``strong'' and ``Paris'' are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  archivePrefix = {arXiv},
  eprint = {1405.4053},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/IH9XHJUV/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf},
  journal = {arXiv:1405.4053 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,doc2vec,feature_extraction},
  language = {en},
  primaryClass = {cs}
}

@incollection{Lewis1994,
  title = {Heterogeneous {{Uncertainty Sampling}} for {{Supervised Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Lewis, David D. and Catlett, Jason},
  editor = {Cohen, William W. and Hirsh, Haym},
  year = {1994},
  month = jan,
  pages = {148--156},
  publisher = {{Morgan Kaufmann}},
  address = {{San Francisco (CA)}},
  doi = {10.1016/B978-1-55860-335-6.50026-X},
  abstract = {Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.},
  file = {/Users/gerbrich/Zotero/storage/V8XP4FY3/Lewis and Catlett - 1994 - Heterogeneous Uncertainty Sampling for Supervised .pdf;/Users/gerbrich/Zotero/storage/GFRUCYX6/B978155860335650026X.html},
  isbn = {978-1-55860-335-6},
  keywords = {query_strategy},
  language = {en}
}

@article{Li2019,
  title = {The {{Current State}} of {{Mobile Phone Apps}} for {{Monitoring Heart Rate}}, {{Heart Rate Variability}}, and {{Atrial Fibrillation}}: {{Narrative Review}}},
  shorttitle = {The {{Current State}} of {{Mobile Phone Apps}} for {{Monitoring Heart Rate}}, {{Heart Rate Variability}}, and {{Atrial Fibrillation}}},
  author = {Li, Ka Hou Christien and White, Francesca Anne and Tipoe, Timothy and Liu, Tong and Wong, Martin CS and Jesuthasan, Aaron and Baranchuk, Adrian and Tse, Gary and Yan, Bryan P},
  year = {2019},
  month = feb,
  volume = {7},
  pages = {e11606},
  issn = {2291-5222},
  doi = {10.2196/11606},
  abstract = {Background: Mobile phone apps capable of monitoring arrhythmias and heart rate (HR) are increasingly used for screening, diagnosis, and monitoring of HR and rhythm disorders such as atrial fibrillation (AF). These apps involve either the use of (1) photoplethysmographic recording or (2) a handheld external electrocardiographic recording device attached to the mobile phone or wristband.},
  file = {/Users/gerbrich/Zotero/storage/9K9PELFY/Li et al. - 2019 - The Current State of Mobile Phone Apps for Monitor.pdf},
  journal = {JMIR mHealth and uHealth},
  language = {en},
  number = {2}
}

@article{Li2019a,
  title = {The {{Current State}} of {{Mobile Phone Apps}} for {{Monitoring Heart Rate}}, {{Heart Rate Variability}}, and {{Atrial Fibrillation}}: {{Narrative Review}}},
  shorttitle = {The {{Current State}} of {{Mobile Phone Apps}} for {{Monitoring Heart Rate}}, {{Heart Rate Variability}}, and {{Atrial Fibrillation}}},
  author = {Li, Ka Hou Christien and White, Francesca Anne and Tipoe, Timothy and Liu, Tong and Wong, Martin CS and Jesuthasan, Aaron and Baranchuk, Adrian and Tse, Gary and Yan, Bryan P.},
  year = {2019},
  volume = {7},
  pages = {e11606},
  doi = {10.2196/11606},
  abstract = {Background: Mobile phone apps capable of monitoring arrhythmias and heart rate (HR) are increasingly used for screening, diagnosis, and monitoring of HR and rhythm disorders such as atrial fibrillation (AF). These apps involve either the use of (1) photoplethysmographic recording or (2) a handheld external electrocardiographic recording device attached to the mobile phone or wristband. Objective: This review seeks to explore the current state of mobile phone apps in cardiac rhythmology while highlighting shortcomings for further research. Methods: We conducted a narrative review of the use of mobile phone devices by searching PubMed and EMBASE from their inception to October 2018. Potentially relevant papers were then compared against a checklist for relevance and reviewed independently for inclusion, with focus on 4 allocated topics of (1) mobile phone monitoring, (2) AF, (3) HR, and (4) HR variability (HRV). Results: The findings of this narrative review suggest that there is a role for mobile phone apps in the diagnosis, monitoring, and screening for arrhythmias and HR. Photoplethysmography and handheld electrocardiograph recorders are the 2 main techniques adopted in monitoring HR, HRV, and AF. Conclusions: A number of studies have demonstrated high accuracy of a number of different mobile devices for the detection of AF. However, further studies are warranted to validate their use for large scale AF screening.  [JMIR Mhealth Uhealth 2019;7(2):e11606]},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (},
  file = {/Users/gerbrich/Zotero/storage/U36RFALU/Li et al. - 2019 - The Current State of Mobile Phone Apps for Monitor.pdf;/Users/gerbrich/Zotero/storage/5DWBQ2PI/e11606.html},
  journal = {JMIR mHealth and uHealth},
  language = {en},
  number = {2}
}

@article{Liang2018,
  title = {An Optimal Filter for Short Photoplethysmogram Signals},
  author = {Liang, Yongbo and Elgendi, Mohamed and Chen, Zhencheng and Ward, Rabab},
  year = {2018},
  month = jan,
  volume = {5},
  pages = {180076},
  issn = {2052-4463},
  doi = {10.1038/sdata.2018.76},
  abstract = {A photoplethysmogram (PPG) contains a wealth of cardiovascular system information, and with the development of wearable technology, it has become the basic technique for evaluating cardiovascular health and detecting diseases. However, due to the varying environments in which wearable devices are used and, consequently, their varying susceptibility to noise interference, effective processing of PPG signals is challenging. Thus, the aim of this study was to determine the optimal filter and filter order to be used for PPG signal processing to make the systolic and diastolic waves more salient in the filtered PPG signal using the skewness quality index. Nine types of filters with 10 different orders were used to filter 219 (2.1s) short PPG signals. The signals were divided into three categories by PPG experts according to their noise levels: excellent, acceptable, or unfit. Results show that the Chebyshev II filter can improve the PPG signal quality more effectively than other types of filters and that the optimal order for the Chebyshev II filter is the 4th order.},
  file = {/Users/gerbrich/Zotero/storage/JR7JHS5S/Liang et al. - 2018 - An optimal filter for short photoplethysmogram sig.pdf},
  journal = {Scientific Data},
  keywords = {Algorithms,Cardiovascular Diseases,Humans,maybe,Photoplethysmography,Signal Processing; Computer-Assisted},
  language = {eng},
  pmcid = {PMC5928853},
  pmid = {29714722}
}

@article{Liang2018a,
  title = {An Optimal Filter for Short Photoplethysmogram Signals},
  author = {Liang, Yongbo and Elgendi, Mohamed and Chen, Zhencheng and Ward, Rabab},
  year = {2018},
  month = may,
  volume = {5},
  pages = {1--12},
  issn = {2052-4463},
  doi = {10.1038/sdata.2018.76},
  abstract = {A photoplethysmogram (PPG) contains a wealth of cardiovascular system information, and with the development of wearable technology, it has become the basic technique for evaluating cardiovascular health and detecting diseases. However, due to the varying environments in which wearable devices are used and, consequently, their varying susceptibility to noise interference, effective processing of PPG signals is challenging. Thus, the aim of this study was to determine the optimal filter and filter order to be used for PPG signal processing to make the systolic and diastolic waves more salient in the filtered PPG signal using the skewness quality index. Nine types of filters with 10 different orders were used to filter 219 (2.1s) short PPG signals. The signals were divided into three categories by PPG experts according to their noise levels: excellent, acceptable, or unfit. Results show that the Chebyshev II filter can improve the PPG signal quality more effectively than other types of filters and that the optimal order for the Chebyshev II filter is the 4th order.},
  copyright = {2018 The Author(s)},
  file = {/Users/gerbrich/Zotero/storage/VQZY9RQ3/Liang et al. - 2018 - An optimal filter for short photoplethysmogram sig.pdf;/Users/gerbrich/Zotero/storage/CTJZIMDQ/sdata201876.html},
  journal = {Scientific Data},
  language = {en},
  number = {1}
}

@article{Marshall2019,
  title = {Toward Systematic Review Automation: A Practical Guide to Using Machine Learning Tools in Research Synthesis},
  shorttitle = {Toward Systematic Review Automation},
  author = {Marshall, Iain J. and Wallace, Byron C.},
  year = {2019},
  month = dec,
  volume = {8},
  pages = {163, s13643-019-1074-9},
  issn = {2046-4053},
  doi = {10/ggnnp5},
  abstract = {Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.},
  file = {/Users/gerbrich/Zotero/storage/9T5MUH8Q/Marshall and Wallace - 2019 - Toward systematic review automation a practical g.pdf},
  journal = {Systematic Reviews},
  language = {en},
  number = {1}
}

@article{Marshall2020,
  title = {Semi-{{Automated}} Evidence Synthesis in Health Psychology: Current Methods and Future Prospects},
  author = {Marshall, Iain J. and Johnson, Blair T. and Wang, Zigeng and Rajasekaran, Sanguthevar and Wallace, Byron C.},
  year = {2020},
  volume = {14},
  pages = {145--158},
  publisher = {{Routledge}},
  doi = {10/ggjv98},
  eprint = {https://doi.org/10.1080/17437199.2020.1716198},
  file = {/Users/gerbrich/Zotero/storage/ZYRX9353/Marshall et al. - 2020 - Semi-Automated evidence synthesis in health psycho.pdf},
  journal = {Health Psychology Review},
  number = {1}
}

@article{Matsumura2013,
  title = {{{iPhysioMeter}}: {{A}} New Approach for Measuring Heart Rate and Normalized Pulse Volume Using Only a Smartphone},
  shorttitle = {{{iPhysioMeter}}},
  author = {Matsumura, Kenta and Yamakoshi, Takehiro},
  year = {2013},
  month = dec,
  volume = {45},
  pages = {1272--1278},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0312-z},
  abstract = {Heart rate (HR) and normalized pulse volume (NPV) are physiological indices that have been used in a diversity of psychological studies. However, measuring these indices often requires laborious processes. We therefore developed a new smartphone program, named iPhysioMeter, that makes it possible to measure beat-by-beat HR and ln NPV using only a smartphone. We examined its accuracy against conventional laboratory measures. Mental stress tasks were used to alter HR and ln NPV in 12 participants. Bland\textendash{}Altman analyses revealed negligible proportional bias for HR and ln NPV or for their change values, expressed as {$\Delta$}HR and {$\Delta$}ln NPV. However, a relatively large fixed bias did emerge for ln NPV, as well as a small one for {$\Delta$}ln NPV, although both were within the limits of agreement. These findings suggest that iPhysioMeter can yield valid measures of the absolute level of HR and of relative changes in ln NPV.},
  file = {/Users/gerbrich/Zotero/storage/W3Q8UQJJ/Matsumura and Yamakoshi - 2013 - iPhysioMeter A new approach for measuring heart r.pdf},
  journal = {Behavior Research Methods},
  keywords = {Electrocardiograph,Finger photo-plethysmograph,iPhone,Mobile health},
  language = {en},
  number = {4}
}

@article{Matwin2010,
  title = {A New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews},
  author = {Matwin, Stan and Kouznetsov, Alexandre and Inkpen, Diana and Frunza, Oana and O'Blenis, Peter},
  year = {2010},
  month = jul,
  volume = {17},
  pages = {446--453},
  issn = {1067-5027},
  doi = {10.1136/jamia.2010.004325},
  abstract = {Abstract.  Objective To determine whether a factorized version of the complement na{\"i}ve Bayes (FCNB) classifier can reduce the time spent by experts reviewing jo},
  file = {/Users/gerbrich/Zotero/storage/D2P8QY98/Matwin et al. - 2010 - A new algorithm for reducing the workload of exper.pdf;/Users/gerbrich/Zotero/storage/7R2UK3MA/867054.html},
  journal = {Journal of the American Medical Informatics Association},
  language = {en},
  number = {4}
}

@inproceedings{Miller2016,
  title = {{{SWIFT}}-{{Active Screener}}: Reducing Literature Screening Effort through Machine Learning for Systematic Reviews},
  booktitle = {Abstracts of the 24th {{Cochrane Colloquium}}},
  author = {Miller, K. and J, Phillips and {M. Shah} and {B. Howard} and {D. Mav} and {K. Thayer} and {R. Shah}},
  year = {2016},
  publisher = {{John Wiley \& Sons}},
  address = {{Seoul, Korea}}
}

@article{Miwa2014,
  title = {Reducing Systematic Review Workload through Certainty-Based Screening},
  author = {Miwa, Makoto and Thomas, James and {O'Mara-Eves}, Alison and Ananiadou, Sophia},
  year = {2014},
  month = oct,
  volume = {51},
  pages = {242--253},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2014.06.005},
  abstract = {In systematic reviews, the growing number of published studies imposes a significant screening workload on reviewers. Active learning is a promising approach to reduce the workload by automating some of the screening decisions, but it has been evaluated for a limited number of disciplines. The suitability of applying active learning to complex topics in disciplines such as social science has not been studied, and the selection of useful criteria and enhancements to address the data imbalance problem in systematic reviews remains an open problem. We applied active learning with two criteria (certainty and uncertainty) and several enhancements in both clinical medicine and social science (specifically, public health) areas, and compared the results in both. The results show that the certainty criterion is useful for finding relevant documents, and weighting positive instances is promising to overcome the data imbalance problem in both data sets. Latent dirichlet allocation (LDA) is also shown to be promising when little manually-assigned information is available. Active learning is effective in complex topics, although its efficiency is limited due to the difficulties in text classification. The most promising criterion and weighting method are the same regardless of the review topic, and unsupervised techniques like LDA have a possibility to boost the performance of active learning without manual annotation.},
  file = {/Users/gerbrich/Zotero/storage/D36HMHQV/Miwa et al. - 2014 - Reducing systematic review workload through certai.pdf;/Users/gerbrich/Zotero/storage/JNGPDR3A/S1532046414001439.html},
  journal = {Journal of Biomedical Informatics},
  keywords = {Active learning,Certainty,Systematic reviews,Text mining},
  language = {en}
}

@article{modAL2018,
  title = {{{modAL}}: {{A}} Modular Active Learning Framework for {{Python}}},
  author = {Danka, Tivadar and Horvath, Peter}
}

@article{Molleri,
  title = {{AUTOMATIZA{\c C}{\~A}O DO PROCESSO DE CONDU{\c C}{\~A}O DE REVIS{\~O}ES SISTEM{\'A}TICAS DA LITERATURA EM ENGENHARIA DE SOFTWARE}},
  author = {Moll{\'e}ri, Jefferson Seide},
  pages = {192},
  file = {/Users/gerbrich/Zotero/storage/4VDSH5NP/Moll√©ri - AUTOMATIZA√á√ÉO DO PROCESSO DE CONDU√á√ÉO DE REVIS√ïES .pdf},
  language = {pt}
}

@article{Molleria,
  title = {{AUTOMATIZA{\c C}{\~A}O DO PROCESSO DE CONDU{\c C}{\~A}O DE REVIS{\~O}ES SISTEM{\'A}TICAS DA LITERATURA EM ENGENHARIA DE SOFTWARE}},
  author = {Moll{\'e}ri, Jefferson Seide},
  pages = {192},
  file = {/Users/gerbrich/Zotero/storage/KECLSXPY/Moll√©ri - AUTOMATIZA√á√ÉO DO PROCESSO DE CONDU√á√ÉO DE REVIS√ïES .pdf},
  language = {pt}
}

@article{Moraes2018,
  title = {Advances in {{Photopletysmography Signal Analysis}} for {{Biomedical Applications}}},
  author = {Moraes, Jermana L. and Rocha, Matheus X. and Vasconcelos, Glauber G. and Vasconcelos Filho, Jos{\'e} E. and {\noopsort{albuquerque}}{de Albuquerque}, Victor Hugo C. and Alexandria, Auzuir R.},
  year = {2018},
  month = jun,
  volume = {18},
  issn = {1424-8220},
  doi = {10.3390/s18061894},
  abstract = {Heart Rate Variability (HRV) is an important tool for the analysis of a patient\&rsquo;s physiological conditions, as well a method aiding the diagnosis of cardiopathies. Photoplethysmography (PPG) is an optical technique applied in the monitoring of the HRV and its adoption has been growing significantly, compared to the most commonly used method in medicine, Electrocardiography (ECG). In this survey, definitions of these technique are presented, the different types of sensors used are explained, and the methods for the study and analysis of the PPG signal (linear and nonlinear methods) are described. Moreover, the progress, and the clinical and practical applicability of the PPG technique in the diagnosis of cardiovascular diseases are evaluated. In addition, the latest technologies utilized in the development of new tools for medical diagnosis are presented, such as Internet of Things, Internet of Health Things, genetic algorithms, artificial intelligence and biosensors which result in personalized advances in e-health and health care. After the study of these technologies, it can be noted that PPG associated with them is an important tool for the diagnosis of some diseases, due to its simplicity, its cost{$^-$}benefit ratio, the easiness of signals acquisition, and especially because it is a non-invasive technique.},
  file = {/Users/gerbrich/Zotero/storage/QKJU2BGP/Moraes et al. - 2018 - Advances in Photopletysmography Signal Analysis fo.pdf},
  journal = {Sensors (Basel, Switzerland)},
  keywords = {Blood Pressure,cardiovascular diseases,health care,Heart Rate,heart rate variability,Humans,Internet of Health Things,photoplethysmography,Photoplethysmography,Signal Processing; Computer-Assisted,Telemedicine},
  language = {eng},
  number = {6},
  pmcid = {PMC6022166},
  pmid = {29890749}
}

@article{Moraes2018a,
  title = {Advances in {{Photopletysmography Signal Analysis}} for {{Biomedical Applications}}},
  author = {Moraes, Jermana L. and Rocha, Matheus X. and Vasconcelos, Glauber G. and Vasconcelos Filho, Jos{\'e} E. and De Albuquerque, Victor Hugo C. and Alexandria, Auzuir R.},
  year = {2018},
  month = jun,
  volume = {18},
  pages = {1894},
  doi = {10.3390/s18061894},
  abstract = {Heart Rate Variability (HRV) is an important tool for the analysis of a patient\&rsquo;s physiological conditions, as well a method aiding the diagnosis of cardiopathies. Photoplethysmography (PPG) is an optical technique applied in the monitoring of the HRV and its adoption has been growing significantly, compared to the most commonly used method in medicine, Electrocardiography (ECG). In this survey, definitions of these technique are presented, the different types of sensors used are explained, and the methods for the study and analysis of the PPG signal (linear and nonlinear methods) are described. Moreover, the progress, and the clinical and practical applicability of the PPG technique in the diagnosis of cardiovascular diseases are evaluated. In addition, the latest technologies utilized in the development of new tools for medical diagnosis are presented, such as Internet of Things, Internet of Health Things, genetic algorithms, artificial intelligence and biosensors which result in personalized advances in e-health and health care. After the study of these technologies, it can be noted that PPG associated with them is an important tool for the diagnosis of some diseases, due to its simplicity, its cost\&ndash;benefit ratio, the easiness of signals acquisition, and especially because it is a non-invasive technique.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/gerbrich/Zotero/storage/52RGALMN/Moraes et al. - 2018 - Advances in Photopletysmography Signal Analysis fo.pdf;/Users/gerbrich/Zotero/storage/LVU923LC/1894.html},
  journal = {Sensors},
  keywords = {cardiovascular diseases,health care,heart rate variability,Internet of Health Things,photoplethysmography},
  language = {en},
  number = {6}
}

@article{Nagtegaal2019,
  title = {Nudging Healthcare Professionals towards Evidence-Based Medicine: {{A}} Systematic Scoping Review},
  author = {Nagtegaal, Rosanna and Tummers, Lars and Noordegraaf, Mirko and Bekkers, Victor},
  year = {2019},
  volume = {2},
  doi = {doi.org/10.30636/jbpa.22.71},
  file = {/Users/gerbrich/Zotero/storage/PJ8QLKFJ/Nudging healthcare professionals towards evidence-.pdf;/Users/gerbrich/Zotero/storage/RFLHKMUP/71.html},
  journal = {Journal of Behavioral Public Administration},
  keywords = {nudging,review},
  number = {2}
}

@dataset{Nagtegaal2019a,
  title = {Nudging Healthcare Professionals towards Evidence-Based Medicine: {{A}} Systematic Scoping Review},
  author = {Nagtegaal, Rosanna and Tummers, Lars and Noordegraaf, Mirko and Bekkers, Victor},
  year = {2019},
  publisher = {{Harvard Dataverse}},
  note = {tyep: dataset},
  unf = {UNF:6:xUGLGKnjKj5IOIlZQXTwDg==},
  version = {V1}
}

@misc{NewtonDividedDifference,
  title = {Newton's {{Divided Difference Interpolation Formula}} - {{GeeksforGeeks}}},
  howpublished = {https://www.geeksforgeeks.org/newtons-divided-difference-interpolation-formula/}
}

@article{OMara-Eves2015,
  title = {Using Text Mining for Study Identification in Systematic Reviews: A Systematic Review of Current Approaches},
  shorttitle = {Using Text Mining for Study Identification in Systematic Reviews},
  author = {{O'Mara-Eves}, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
  year = {2015},
  month = jan,
  volume = {4},
  pages = {5},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-5},
  abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
  file = {/Users/gerbrich/Zotero/storage/WWD9DBD2/O‚ÄôMara-Eves et al. - 2015 - Using text mining for study identification in syst.pdf;/Users/gerbrich/Zotero/storage/HMT2W9P7/2046-4053-4-5.html},
  journal = {Systematic Reviews},
  number = {1}
}

@article{Ouzzani2016,
  title = {Rayyan\textemdash{}a Web and Mobile App for Systematic Reviews},
  author = {Ouzzani, Mourad and Hammady, Hossam and Fedorowicz, Zbys and Elmagarmid, Ahmed},
  year = {2016},
  volume = {5},
  pages = {210},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0384-4},
  abstract = {Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making.},
  journal = {Systematic Reviews},
  number = {1}
}

@misc{PackageDevelopmentPictorial,
  title = {R {{Package Development Pictorial}}},
  file = {/Users/gerbrich/Zotero/storage/TLGKLY2R/R_Package_Pictorial.html},
  howpublished = {http://www.mjdenny.com/R\_Package\_Pictorial.html}
}

@article{Poh2018,
  title = {Diagnostic Assessment of a Deep Learning System for Detecting Atrial Fibrillation in Pulse Waveforms},
  author = {Poh, Ming-Zher and Poh, Yukkee Cheung and Chan, Pak-Hei and Wong, Chun-Ka and Pun, Louise and Leung, Wangie Wan-Chiu and Wong, Yu-Fai and Wong, Michelle Man-Ying and Chu, Daniel Wai-Sing and Siu, Chung-Wah},
  year = {2018},
  month = dec,
  volume = {104},
  pages = {1921--1928},
  issn = {1355-6037, 1468-201X},
  doi = {10.1136/heartjnl-2018-313147},
  abstract = {Objective To evaluate the diagnostic performance of a deep learning system for automated detection of atrial fibrillation (AF) in photoplethysmographic (PPG) pulse waveforms.
Methods We trained a deep convolutional neural network (DCNN) to detect AF in 17 s PPG waveforms using a training data set of 149 048 PPG waveforms constructed from several publicly available PPG databases. The DCNN was validated using an independent test data set of 3039 smartphone-acquired PPG waveforms from adults at high risk of AF at a general outpatient clinic against ECG tracings reviewed by two cardiologists. Six established AF detectors based on handcrafted features were evaluated on the same test data set for performance comparison.
Results In the validation data set (3039 PPG waveforms) consisting of three sequential PPG waveforms from 1013 participants (mean (SD) age, 68.4 (12.2) years; 46.8\% men), the prevalence of AF was 2.8\%. The area under the receiver operating characteristic curve (AUC) of the DCNN for AF detection was 0.997 (95\% CI 0.996 to 0.999) and was significantly higher than all the other AF detectors (AUC range: 0.924\textendash{}0.985). The sensitivity of the DCNN was 95.2\% (95\% CI 88.3\% to 98.7\%), specificity was 99.0\% (95\% CI 98.6\% to 99.3\%), positive predictive value (PPV) was 72.7\% (95\% CI 65.1\% to 79.3\%) and negative predictive value (NPV) was 99.9\% (95\% CI 99.7\% to 100\%) using a single 17 s PPG waveform. Using the three sequential PPG waveforms in combination ({$<$}1 min in total), the sensitivity was 100.0\% (95\% CI 87.7\% to 100\%), specificity was 99.6\% (95\% CI 99.0\% to 99.9\%), PPV was 87.5\% (95\% CI 72.5\% to 94.9\%) and NPV was 100\% (95\% CI 99.4\% to 100\%).
Conclusions In this evaluation of PPG waveforms from adults screened for AF in a real-world primary care setting, the DCNN had high sensitivity, specificity, PPV and NPV for detecting AF, outperforming other state-of-the-art methods based on handcrafted features.},
  copyright = {\textcopyright{} Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2018. All rights reserved. No commercial use is permitted unless otherwise expressly granted.},
  file = {/Users/gerbrich/Zotero/storage/MHKKD79F/1921.html},
  journal = {Heart},
  keywords = {atrial fibrillation,ehealth/telemedicine/mobile health,premature ventricular beats},
  language = {en},
  number = {23},
  pmid = {29853485}
}

@article{PRISMA-PGroup2015,
  title = {Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols ({{PRISMA}}-{{P}}) 2015 Statement},
  author = {{PRISMA-P Group} and Moher, David and Shamseer, Larissa and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A},
  year = {2015},
  month = dec,
  volume = {4},
  pages = {1},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-1},
  abstract = {Systematic reviews should build on a protocol that describes the rationale, hypothesis, and planned methods of the review; few reviews report whether a protocol exists. Detailed, well-described protocols can facilitate the understanding and appraisal of the review methods, as well as the detection of modifications to methods and selective reporting in completed reviews. We describe the development of a reporting guideline, the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 (PRISMA-P 2015). PRISMA-P consists of a 17-item checklist intended to facilitate the preparation and reporting of a robust protocol for the systematic review. Funders and those commissioning reviews might consider mandating the use of the checklist to facilitate the submission of relevant protocol information in funding applications. Similarly, peer reviewers and editors can use the guidance to gauge the completeness and transparency of a systematic review protocol submitted for publication in a journal or other medium.},
  file = {/Users/gerbrich/Zotero/storage/PT8NAI48/PRISMA-P Group et al. - 2015 - Preferred reporting items for systematic review an.pdf},
  journal = {Systematic Reviews},
  language = {en},
  number = {1}
}

@article{Proesmans2019,
  title = {Mobile {{Phone}}-{{Based Use}} of the {{Photoplethysmography Technique}} to {{Detect Atrial Fibrillation}} in {{Primary Care}}: {{Diagnostic Accuracy Study}} of the {{FibriCheck App}}},
  shorttitle = {Mobile {{Phone}}-{{Based Use}} of the {{Photoplethysmography Technique}} to {{Detect Atrial Fibrillation}} in {{Primary Care}}},
  author = {Proesmans, Tine and Mortelmans, Christophe and Van Haelst, Ruth and Verbrugge, Frederik and Vandervoort, Pieter and Vaes, Bert},
  year = {2019},
  month = mar,
  volume = {7},
  pages = {e12284},
  issn = {2291-5222},
  doi = {10.2196/12284},
  abstract = {BACKGROUND: Mobile phone apps using photoplethysmography (PPG) technology through their built-in camera are becoming an attractive alternative for atrial fibrillation (AF) screening because of their low cost, convenience, and broad accessibility. However, some important questions concerning their diagnostic accuracy remain to be answered.
OBJECTIVE: This study tested the diagnostic accuracy of the FibriCheck AF algorithm for the detection of AF on the basis of mobile phone PPG and single-lead electrocardiography (ECG) signals.
METHODS: A convenience sample of patients aged 65 years and above, with or without a known history of AF, was recruited from 17 primary care facilities. Patients with an active pacemaker rhythm were excluded. A PPG signal was obtained with the rear camera of an iPhone 5S. Simultaneously, a single-lead ECG was registered using a dermal patch with a wireless connection to the same mobile phone. PPG and single-lead ECG signals were analyzed using the FibriCheck AF algorithm. At the same time, a 12-lead ECG was obtained and interpreted offline by independent cardiologists to determine the presence of AF.
RESULTS: A total of 45.7\% (102/223) subjects were having AF. PPG signal quality was sufficient for analysis in 93\% and single-lead ECG quality was sufficient in 94\% of the participants. After removing insufficient quality measurements, the sensitivity and specificity were 96\% (95\% CI 89\%-99\%) and 97\% (95\% CI 91\%-99\%) for the PPG signal versus 95\% (95\% CI 88\%-98\%) and 97\% (95\% CI 91\%-99\%) for the single-lead ECG, respectively. False-positive results were mainly because of premature ectopic beats. PPG and single-lead ECG techniques yielded adequate signal quality in 196 subjects and a similar diagnosis in 98.0\% (192/196) subjects.
CONCLUSIONS: The FibriCheck AF algorithm can accurately detect AF on the basis of mobile phone PPG and single-lead ECG signals in a primary care convenience sample.},
  journal = {JMIR mHealth and uHealth},
  keywords = {algorithm,atrial fibrillation,electrocardiography,mobile phone,photoplethysmography},
  language = {eng},
  number = {3},
  pmcid = {PMC6456825},
  pmid = {30916656}
}

@article{proesmans2019a,
  title = {Mobile {{Phone}}\textendash{{Based Use}} of the {{Photoplethysmography Technique}} to {{Detect Atrial Fibrillation}} in {{Primary Care}}: {{Diagnostic Accuracy Study}} of the {{FibriCheck App}}},
  shorttitle = {Mobile {{Phone}}\textendash{{Based Use}} of the {{Photoplethysmography Technique}} to {{Detect Atrial Fibrillation}} in {{Primary Care}}},
  author = {Proesmans, Tine and Mortelmans, Christophe and Van Haelst, Ruth and Verbrugge, Frederik and Vandervoort, Pieter and Vaes, Bert},
  year = {2019},
  month = mar,
  volume = {7},
  issn = {2291-5222},
  doi = {10.2196/12284},
  abstract = {Background
Mobile phone apps using photoplethysmography (PPG) technology through their built-in camera are becoming an attractive alternative for atrial fibrillation (AF) screening because of their low cost, convenience, and broad accessibility. However, some important questions concerning their diagnostic accuracy remain to be answered.

Objective
This study tested the diagnostic accuracy of the FibriCheck AF algorithm for the detection of AF on the basis of mobile phone PPG and single-lead electrocardiography (ECG) signals.

Methods
A convenience sample of patients aged 65 years and above, with or without a known history of AF, was recruited from 17 primary care facilities. Patients with an active pacemaker rhythm were excluded. A PPG signal was obtained with the rear camera of an iPhone 5S. Simultaneously, a single-lead ECG was registered using a dermal patch with a wireless connection to the same mobile phone. PPG and single-lead ECG signals were analyzed using the FibriCheck AF algorithm. At the same time, a 12-lead ECG was obtained and interpreted offline by independent cardiologists to determine the presence of AF.

Results
A total of 45.7\% (102/223) subjects were having AF. PPG signal quality was sufficient for analysis in 93\% and single-lead ECG quality was sufficient in 94\% of the participants. After removing insufficient quality measurements, the sensitivity and specificity were 96\% (95\% CI 89\%-99\%) and 97\% (95\% CI 91\%-99\%) for the PPG signal versus 95\% (95\% CI 88\%-98\%) and 97\% (95\% CI 91\%-99\%) for the single-lead ECG, respectively. False-positive results were mainly because of premature ectopic beats. PPG and single-lead ECG techniques yielded adequate signal quality in 196 subjects and a similar diagnosis in 98.0\% (192/196) subjects.

Conclusions
The FibriCheck AF algorithm can accurately detect AF on the basis of mobile phone PPG and single-lead ECG signals in a primary care convenience sample.},
  journal = {JMIR mHealth and uHealth},
  number = {3},
  pmcid = {PMC6456825},
  pmid = {30916656}
}

@article{Przybyla2018,
  title = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}: {{A}} User Study},
  shorttitle = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}},
  author = {Przyby{\l}a, Piotr and Brockmeier, Austin J. and Kontonatsios, Georgios and Pogam, Marie-Annick Le and McNaught, John and {\noopsort{elm}}von Elm, Erik and Nolan, Kay and Ananiadou, Sophia},
  year = {2018},
  volume = {9},
  pages = {470--488},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1311},
  abstract = {Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95\% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.},
  copyright = {\textcopyright{} 2018 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  file = {/Users/gerbrich/Zotero/storage/7FPGPSK2/Przyby≈Ça et al. - 2018 - Prioritising references for systematic reviews wit.pdf;/Users/gerbrich/Zotero/storage/U7KPBBJ4/jrsm.html},
  journal = {Research Synthesis Methods},
  language = {en},
  number = {3}
}

@inproceedings{Ramos2003,
  title = {Using Tf-Idf to Determine Word Relevance in Document Queries},
  booktitle = {Proceedings of the First Instructional Conference on Machine Learning},
  author = {Ramos, Juan and others},
  year = {2003},
  volume = {242},
  pages = {133--142},
  file = {/Users/gerbrich/Zotero/storage/YUB2N9D9/Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf},
  organization = {{Piscataway, NJ}}
}

@inproceedings{rayyan:14:cochrane,
  title = {Rayyan: A Systematic Reviews Web App for Exploring and Filtering Searches for Eligible Studies for {{Cochrane Reviews}}},
  author = {A, Elmagarmid and Z, Fedorowicz and H, Hammady and I, Ilyas and M, Khabsa and M, Ouzzani},
  pages = {9},
  crossref = {cochrane:14}
}

@article{rayyan:ML2015,
  title = {Learning to Identify Relevant Studies for Systematic Reviews Using Random Forest and External Information},
  author = {Khabsa, Madian and Elmagarmid, Ahmed and Ilyas, Ihab and Hammady, Hossam and Ouzzani, Mourad},
  year = {2015},
  pages = {1--18},
  publisher = {{Springer US}},
  issn = {0885-6125},
  doi = {10.1007/s10994-015-5535-7},
  journal = {Machine Learning},
  keywords = {Classification,Inclusion prediction,Systematic review},
  language = {English}
}

@inproceedings{Rehurek2010,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r{u}}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  month = may,
  pages = {45--50},
  publisher = {{ELRA}},
  address = {{Valletta, Malta}},
  language = {English}
}

@article{Reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde{}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archivePrefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/QWTQF9NF/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/gerbrich/Zotero/storage/SI4PQX5T/1908.html},
  journal = {arXiv:1908.10084 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@misc{RPubsDividedDifferences,
  title = {{{RPubs}} - {{Divided Differences Method}} of {{Polynomial Interpolation}}},
  file = {/Users/gerbrich/Zotero/storage/TJ2MARW3/divided-differences-polynomial-interpolation.html},
  howpublished = {https://rpubs.com/aaronsc32/divided-differences-polynomial-interpolation}
}

@article{Savitzky1964,
  title = {Smoothing and {{Differentiation}} of {{Data}} by {{Simplified Least Squares Procedures}}.},
  author = {Savitzky, Abraham. and Golay, M. J. E.},
  year = {1964},
  month = jul,
  volume = {36},
  pages = {1627--1639},
  issn = {0003-2700},
  doi = {10.1021/ac60214a047},
  file = {/Users/gerbrich/Zotero/storage/4TF9FIF9/Savitzky and Golay - 1964 - Smoothing and Differentiation of Data by Simplifie.pdf;/Users/gerbrich/Zotero/storage/9W9CLUQH/ac60214a047.html},
  journal = {Analytical Chemistry},
  number = {8}
}

@article{Schoot2018,
  title = {Bayesian {{PTSD}}-{{Trajectory Analysis}} with {{Informed Priors Based}} on a {{Systematic Literature Search}} and {{Expert Elicitation}}},
  author = {{\noopsort{schoot}}van de Schoot, Rens and Sijbrandij, Marit and Depaoli, Sarah and Winter, Sonja D. and Olff, Miranda and {\noopsort{loey}}van Loey, Nancy E.},
  year = {2018},
  month = mar,
  volume = {53},
  pages = {267--291},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1080/00273171.2017.1412293},
  abstract = {There is a recent increase in interest of Bayesian analysis. However, little effort has been made thus far to directly incorporate background knowledge via the prior distribution into the analyses. This process might be especially useful in the context of latent growth mixture modeling when one or more of the latent groups are expected to be relatively small due to what we refer to as limited data. We argue that the use of Bayesian statistics has great advantages in limited data situations, but only if background knowledge can be incorporated into the analysis via prior distributions. We highlight these advantages through a data set including patients with burn injuries and analyze trajectories of posttraumatic stress symptoms using the Bayesian framework following the steps of the WAMBS-checklist. In the included example, we illustrate how to obtain background information using previous literature based on a systematic literature search and by using expert knowledge. Finally, we show how to translate this knowledge into prior distributions and we illustrate the importance of conducting a prior sensitivity analysis. Although our example is from the trauma field, the techniques we illustrate can be applied to any field.},
  file = {/Users/gerbrich/Zotero/storage/MXLVZIJR/Schoot et al. - 2018 - Bayesian PTSD-Trajectory Analysis with Informed Pr.pdf;/Users/gerbrich/Zotero/storage/3DLPQCD2/00273171.2017.html},
  journal = {Multivariate Behavioral Research},
  keywords = {Bayesian statistics,Latent class analysis,Latent growth models,Mixture modeling,PTSD},
  note = {\_eprint: https://doi.org/10.1080/00273171.2017.1412293},
  number = {2},
  pmid = {29324055}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research}
}

@article{Settles,
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  pages = {67},
  file = {/Users/gerbrich/Zotero/storage/3PR6CDUT/Settles - Active Learning Literature Survey.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@article{Settles2012,
  title = {Active {{Learning}}},
  author = {Settles, Burr},
  year = {2012},
  month = jun,
  volume = {6},
  pages = {1--114},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00429ED1V01Y201207AIM018},
  file = {/Users/gerbrich/Zotero/storage/2BSTA3X9/Settles - 2012 - Active Learning.pdf},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  language = {en},
  number = {1}
}

@misc{Shapiro2018,
  title = {Shapiromatron/Hawc: 2018-{{Q3}}},
  shorttitle = {Shapiromatron/Hawc},
  author = {Shapiro, Andy and Addington, Josh and Thacker, Shane and Comeaux, Justin},
  year = {2018},
  month = sep,
  doi = {10.5281/zenodo.1414622},
  abstract = {2018 Q3 release.},
  file = {/Users/gerbrich/Zotero/storage/ESIYK5HH/1414622.html},
  howpublished = {Zenodo}
}

@article{Shemilt2014,
  title = {Pinpointing Needles in Giant Haystacks: Use of Text Mining to Reduce Impractical Screening Workload in Extremely Large Scoping Reviews},
  shorttitle = {Pinpointing Needles in Giant Haystacks},
  author = {Shemilt, Ian and Simon, Antonia and Hollands, Gareth J. and Marteau, Theresa M. and Ogilvie, David and O'Mara-Eves, Alison and Kelly, Michael P. and Thomas, James},
  year = {2014},
  volume = {5},
  pages = {31--49},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1093},
  abstract = {In scoping reviews, boundaries of relevant evidence may be initially fuzzy, with refined conceptual understanding of interventions and their proposed mechanisms of action an intended output of the scoping process rather than its starting point. Electronic searches are therefore sensitive, often retrieving very large record sets that are impractical to screen in their entirety. This paper describes methods for applying and evaluating the use of text mining (TM) technologies to reduce impractical screening workload in reviews, using examples of two extremely large-scale scoping reviews of public health evidence (choice architecture (CA) and economic environment (EE)). Electronic searches retrieved {$>$}800,000 (CA) and {$>$}1 million (EE) records. TM technologies were used to prioritise records for manual screening. TM performance was measured prospectively. TM reduced manual screening workload by 90\% (CA) and 88\% (EE) compared with conventional screening (absolute reductions of {$\approx$}430 000 (CA) and {$\approx$}378 000 (EE) records). This study expands an emerging corpus of empirical evidence for the use of TM to expedite study selection in reviews. By reducing screening workload to manageable levels, TM made it possible to assemble and configure large, complex evidence bases that crossed research discipline boundaries. These methods are transferable to other scoping and systematic reviews incorporating conceptual development or explanatory dimensions. \textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  copyright = {\textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/C42P4W4M/Shemilt et al. - 2014 - Pinpointing needles in giant haystacks use of tex.pdf;/Users/gerbrich/Zotero/storage/5QFKVQ8Z/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {scoping review methods,study selection,systematic review methods,text mining},
  language = {en},
  number = {1}
}

@article{Shemilt2016,
  title = {Use of Cost-Effectiveness Analysis to Compare the Efficiency of Study Identification Methods in Systematic Reviews},
  author = {Shemilt, Ian and Khan, Nada and Park, Sophie and Thomas, James},
  year = {2016},
  month = aug,
  volume = {5},
  pages = {140},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0315-4},
  abstract = {Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews.},
  file = {/Users/gerbrich/Zotero/storage/CZQI8A6B/Shemilt et al. - 2016 - Use of cost-effectiveness analysis to compare the .pdf},
  journal = {Systematic Reviews},
  language = {en},
  number = {1}
}

@misc{Shperber2019,
  title = {A Gentle Introduction to {{Doc2Vec}}},
  author = {Shperber, Gidi},
  year = {2019},
  month = nov,
  abstract = {TL;DR},
  file = {/Users/gerbrich/Zotero/storage/MKWGDB73/a-gentle-introduction-to-doc2vec-db3e8c0cce5e.html},
  howpublished = {https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e},
  journal = {Medium},
  language = {en}
}

@misc{SoftwareLicenses,
  title = {Software Licenses},
  file = {/Users/gerbrich/Zotero/storage/F28X225J/licenses.html},
  howpublished = {https://kbroman.org/pkg\_primer/pages/licenses.html}
}

@article{Stansfield2013,
  title = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research: A Case Study},
  shorttitle = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research},
  author = {Stansfield, Claire and Thomas, James and Kavanagh, Josephine},
  year = {2013},
  volume = {4},
  pages = {230--241},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1082},
  abstract = {Background Scoping reviews of research help determine the feasibility and the resource requirements of conducting a systematic review, and the potential to generate a description of the literature quickly is attractive. Aims To test the utility and applicability of an automated clustering tool to describe and group research studies to improve the efficiency of scoping reviews. Methods A retrospective study of two completed scoping reviews was conducted. This compared the groups and descriptive categories obtained by automatically clustering titles and abstracts with those that had originally been derived using traditional researcher-driven techniques. Results The clustering tool rapidly categorised research into themes, which were useful in some instances, but not in others. This provided a dynamic means to view each dataset. Interpretation was challenging where there were potentially multiple meanings of terms. Where relevant clusters were unambiguous, there was a high precision of relevant studies, although recall varied widely. Conclusions Policy-relevant scoping reviews are often undertaken rapidly, and this could potentially be enhanced by automation depending on the nature of the dataset and information sought. However, it is not a replacement for researcher-developed classification. The possibilities of further applications and potential for use in other types of review are discussed. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/B92KBE6S/jrsm.html},
  journal = {Research Synthesis Methods},
  keywords = {automatic clustering,automation,information storage and retrieval,lingo,mapping,methods,scoping reviews,text mining},
  language = {en},
  number = {3}
}

@article{Tamura2014,
  title = {Wearable {{Photoplethysmographic Sensors}}\textemdash{{Past}} and {{Present}}},
  author = {Tamura, Toshiyo and Maeda, Yuka and Sekine, Masaki and Yoshida, Masaki},
  year = {2014},
  month = jun,
  volume = {3},
  pages = {282--302},
  doi = {10.3390/electronics3020282},
  abstract = {Photoplethysmography (PPG) technology has been used to develop small, wearable, pulse rate sensors. These devices, consisting of infrared light-emitting diodes (LEDs) and photodetectors, offer a simple, reliable, low-cost means of monitoring the pulse rate noninvasively. Recent advances in optical technology have facilitated the use of high-intensity green LEDs for PPG, increasing the adoption of this measurement technique. In this review, we briefly present the history of PPG and recent developments in wearable pulse rate sensors with green LEDs. The application of wearable pulse rate monitors is discussed.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/gerbrich/Zotero/storage/3V3NP4RY/Tamura et al. - 2014 - Wearable Photoplethysmographic Sensors‚ÄîPast and Pr.pdf;/Users/gerbrich/Zotero/storage/UYTEUC5C/282.html},
  journal = {Electronics},
  keywords = {adaptive filter,green light,infrared light,least mean square algorithm,photoplethysmography,pulse rate,reflectance,transmittance},
  language = {en},
  number = {2}
}

@article{Tamura2019,
  title = {Current Progress of Photoplethysmography and {{SPO2}} for Health Monitoring},
  author = {Tamura, Toshiyo},
  year = {2019},
  month = feb,
  volume = {9},
  pages = {21--36},
  issn = {2093-985X},
  doi = {10.1007/s13534-019-00097-w},
  abstract = {A photoplethysmograph (PPG) is a simple medical device for monitoring blood flow and transportation of substances in the blood. It consists of a light source and a photodetector for measuring transmitted and reflected light signals. Clinically, PPGs are used to monitor the pulse rate, oxygen saturation, blood pressure, and blood vessel stiffness. Wearable unobtrusive PPG monitors are commercially available. Here, we review the principle issues and clinical applications of PPG for monitoring oxygen saturation.},
  file = {/Users/gerbrich/Zotero/storage/RZREIMU6/Tamura - 2019 - Current progress of photoplethysmography and SPO2 .pdf},
  journal = {Biomedical Engineering Letters},
  keywords = {Motion artifact,Photoplethysmograph (PPG),photoplethysmography,Pulse rate,Reflected light,Respiratory rate,RGB image,Transmitted light},
  language = {en},
  number = {1}
}

@article{Tamura2019a,
  title = {Current Progress of Photoplethysmography and {{SPO2}} for Health Monitoring},
  author = {Tamura, Toshiyo},
  year = {2019},
  month = feb,
  volume = {9},
  pages = {21--36},
  issn = {2093-985X},
  doi = {10.1007/s13534-019-00097-w},
  abstract = {A photoplethysmograph (PPG) is a simple medical device for monitoring blood flow and transportation of substances in the blood. It consists of a light source and a photodetector for measuring transmitted and reflected light signals. Clinically, PPGs are used to monitor the pulse rate, oxygen saturation, blood pressure, and blood vessel stiffness. Wearable unobtrusive PPG monitors are commercially available. Here, we review the principle issues and clinical applications of PPG for monitoring oxygen saturation.},
  file = {/Users/gerbrich/Zotero/storage/PQH3JFTN/Tamura - 2019 - Current progress of photoplethysmography and SPO2 .pdf},
  journal = {Biomedical Engineering Letters},
  keywords = {Motion artifact,Photoplethysmograph (PPG),Pulse rate,Reflected light,Respiratory rate,RGB image,Transmitted light},
  language = {en},
  number = {1}
}

@article{Thomas2011,
  title = {Applications of Text Mining within Systematic Reviews},
  author = {Thomas, James and McNaught, John and Ananiadou, Sophia},
  year = {2011},
  volume = {2},
  pages = {1--14},
  doi = {10.1002/jrsm.27},
  abstract = {Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.27},
  journal = {Research Synthesis Methods},
  keywords = {automatic summarization,document classification,document clustering,research synthesis,screening,searching,systematic review,term recognition,text mining},
  number = {1}
}

@article{Thomas2017,
  title = {Living Systematic Reviews: 2. {{Combining}} Human and Machine Effort},
  shorttitle = {Living Systematic Reviews},
  author = {Thomas, James and {Noel-Storr}, Anna and Marshall, Iain and Wallace, Byron and McDonald, Steven and Mavergames, Chris and Glasziou, Paul and Shemilt, Ian and Synnot, Anneliese and Turner, Tari and Elliott, Julian and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and {Kolakowsky-Hayner}, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"u}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie-Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and {Soares-Weiser}, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Mart{\'i}nez Garc{\'i}a, Laura and Synnot, Anneliese and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Thomas, James and Egan, Cathy and McDonald, Steve and Tritton, Roger and Elliott, Julian and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Turner, Tari and Foxlee, Ruth and Morley, Richard and {\noopsort{valkenhoef}}{van Valkenhoef}, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and {Noel-Storr}, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and Yepes Nu{\~n}ez, Juan Jos{\'e} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
  year = {2017},
  month = nov,
  volume = {91},
  pages = {31--37},
  issn = {0895-4356},
  doi = {10/gcqcxv},
  abstract = {New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (``crowds'') as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential\textemdash{}and limitations\textemdash{}of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine ``technologies'' are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.},
  file = {/Users/gerbrich/Zotero/storage/ESEJPVK8/Thomas et al. - 2017 - Living systematic reviews 2. Combining human and .pdf;/Users/gerbrich/Zotero/storage/J9B379VJ/S0895435617306042.html},
  journal = {Journal of Clinical Epidemiology},
  keywords = {Automation,Citizen science,Crowdsourcing,Machine learning,Systematic review,Text mining},
  language = {en}
}

@misc{TipsTricksPaulvanderlaken,
  title = {R Tips and Tricks \textendash{} Paulvanderlaken.Com},
  file = {/Users/gerbrich/Zotero/storage/X66NEX33/r-tips-and-tricks.html},
  howpublished = {https://paulvanderlaken.com/2018/05/21/r-tips-and-tricks/}
}

@inproceedings{Tomassetti2011,
  title = {Linked {{Data}} Approach for Selection Process Automation in {{Systematic Reviews}}},
  booktitle = {15th {{Annual Conference}} on {{Evaluation}} \& {{Assessment}} in {{Software Engineering}} ({{EASE}} 2011)},
  author = {Tomassetti, Federico Cesare Argentino and Rizzo, Giuseppe and Vetro', Antonio and Ardito, Luca and Torchiano, Marco and Morisio, Maurizio},
  year = {2011},
  pages = {31--35},
  publisher = {{IEE}},
  doi = {10.1049/ic.2011.0004},
  abstract = {Background: a systematic review identifies, evaluates and synthesizes the available literature on a given topic using scientific and repeatable methodologies. The significant workload required and the subjectivity bias could affect results. Aim: semi-automate the selection process to reduce the amount of manual work needed and the consequent subjectivity bias. Method: extend and enrich the selection of primary studies using the existing technologies in the field of Linked Data and text mining. We define formally the selection process and we also develop a prototype that implements it. Finally, we conduct a case study that simulates the selection process of a systematic literature published in literature. Results: the process presented in this paper could reduce the work load of 20\% with respect to the work load needed in the fully manually selection, with a recall of 100\%. Conclusions: the extraction of knowledge from scientific studies through Linked Data and text mining techniques could be used in the selection phase of the systematic review process to reduce the work load and subjectivity bias.},
  file = {/Users/gerbrich/Zotero/storage/8PJ2V6EQ/Tomassetti et al. - 2011 - Linked Data approach for selection process automat.pdf;/Users/gerbrich/Zotero/storage/8PT4A537/2381987.html},
  isbn = {978-1-84919-509-6},
  keywords = {dbpedia},
  language = {eng}
}

@article{Tong,
  title = {Support {{Vector Machine Active Learning}} with {{Applications}} to {{Text Classification}}},
  author = {Tong, Simon and Koller, Daphne},
  pages = {22},
  abstract = {Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.},
  file = {/Users/gerbrich/Zotero/storage/4ESDEWB5/Tong and Koller - Support Vector Machine Active Learning with Applic.pdf},
  language = {en}
}

@article{Tong2001,
  title = {Support Vector Machine Active Learning with Applications to Text Classification},
  author = {Tong, Simon and Koller, Daphne},
  year = {2001},
  volume = {2},
  pages = {45--66},
  file = {/Users/gerbrich/Zotero/storage/V4WAE7M6/Tong and Koller - 2001 - Support vector machine active learning with applic.pdf},
  journal = {Journal of machine learning research},
  keywords = {‚õî No DOI found},
  number = {Nov}
}

@article{Vandenberk2017,
  title = {Clinical {{Validation}} of {{Heart Rate Apps}}: {{Mixed}}-{{Methods Evaluation Study}}},
  shorttitle = {Clinical {{Validation}} of {{Heart Rate Apps}}},
  author = {Vandenberk, Thijs and Stans, Jelle and Mortelmans, Christophe and Haelst, Ruth Van and Schelvergem, Gertjan Van and Pelckmans, Caroline and Smeets, Christophe JP and Lanssens, Dorien and Canni{\`e}re, H{\'e}l{\`e}ne De and Storms, Valerie and Thijs, Inge M. and Vaes, Bert and Vandervoort, Pieter M.},
  year = {2017},
  volume = {5},
  pages = {e129},
  doi = {10.2196/mhealth.7254},
  abstract = {Background: Photoplethysmography (PPG) is a proven way to measure heart rate (HR). This technology is already available in smartphones, which allows measuring HR only by using the smartphone. Given the widespread availability of smartphones, this creates a scalable way to enable mobile HR monitoring. An essential precondition is that these technologies are as reliable and accurate as the current clinical (gold) standards. At this moment, there is no consensus on a gold standard method for the validation of HR apps. This results in different validation processes that do not always reflect the veracious outcome of comparison. Objective: The aim of this paper was to investigate and describe the necessary elements in validating and comparing HR apps versus standard technology. Methods: The FibriCheck (Qompium) app was used in two separate prospective nonrandomized studies. In the first study, the HR of the FibriCheck app was consecutively compared with 2 different Food and Drug Administration (FDA)-cleared HR devices: the Nonin oximeter and the AliveCor Mobile ECG. In the second study, a next step in validation was performed by comparing the beat-to-beat intervals of the FibriCheck app to a synchronized ECG recording. Results: In the first study, the HR (BPM, beats per minute) of 88 random subjects consecutively measured with the 3 devices showed a correlation coefficient of .834 between FibriCheck and Nonin, .88 between FibriCheck and AliveCor, and .897 between Nonin and AliveCor. A single way analysis of variance (ANOVA; P=.61 was executed to test the hypothesis that there were no significant differences between the HRs as measured by the 3 devices. In the second study, 20,298 (ms) R-R intervals (RRI)\textendash{}peak-to-peak intervals (PPI) from 229 subjects were analyzed. This resulted in a positive correlation (rs=.993, root mean square deviation [RMSE]=23.04 ms, and normalized root mean square error [NRMSE]=0.012) between the PPI from FibriCheck and the RRI from the wearable ECG. There was no significant difference (P=.92) between these intervals. Conclusions: Our findings suggest that the most suitable method for the validation of an HR app is a simultaneous measurement of the HR by the smartphone app and an ECG system, compared on the basis of beat-to-beat analysis. This approach could lead to more correct assessments of the accuracy of HR apps.  [JMIR Mhealth Uhealth 2017;5(8):e129]},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (},
  file = {/Users/gerbrich/Zotero/storage/UR8DU8WX/e129.html},
  journal = {JMIR mHealth and uHealth},
  language = {en},
  number = {8}
}

@article{Vandenberk2017a,
  title = {Clinical {{Validation}} of {{Heart Rate Apps}}: {{Mixed}}-{{Methods Evaluation Study}}},
  shorttitle = {Clinical {{Validation}} of {{Heart Rate Apps}}},
  author = {Vandenberk, Thijs and Stans, Jelle and Mortelmans, Christophe and Haelst, Ruth Van and Schelvergem, Gertjan Van and Pelckmans, Caroline and Smeets, Christophe JP and Lanssens, Dorien and Canni{\`e}re, H{\'e}l{\`e}ne De and Storms, Valerie and Thijs, Inge M. and Vaes, Bert and Vandervoort, Pieter M.},
  year = {2017},
  volume = {5},
  pages = {e129},
  doi = {10.2196/mhealth.7254},
  abstract = {Background: Photoplethysmography (PPG) is a proven way to measure heart rate (HR). This technology is already available in smartphones, which allows measuring HR only by using the smartphone. Given the widespread availability of smartphones, this creates a scalable way to enable mobile HR monitoring. An essential precondition is that these technologies are as reliable and accurate as the current clinical (gold) standards. At this moment, there is no consensus on a gold standard method for the validation of HR apps. This results in different validation processes that do not always reflect the veracious outcome of comparison. Objective: The aim of this paper was to investigate and describe the necessary elements in validating and comparing HR apps versus standard technology. Methods: The FibriCheck (Qompium) app was used in two separate prospective nonrandomized studies. In the first study, the HR of the FibriCheck app was consecutively compared with 2 different Food and Drug Administration (FDA)-cleared HR devices: the Nonin oximeter and the AliveCor Mobile ECG. In the second study, a next step in validation was performed by comparing the beat-to-beat intervals of the FibriCheck app to a synchronized ECG recording. Results: In the first study, the HR (BPM, beats per minute) of 88 random subjects consecutively measured with the 3 devices showed a correlation coefficient of .834 between FibriCheck and Nonin, .88 between FibriCheck and AliveCor, and .897 between Nonin and AliveCor. A single way analysis of variance (ANOVA; P=.61 was executed to test the hypothesis that there were no significant differences between the HRs as measured by the 3 devices. In the second study, 20,298 (ms) R-R intervals (RRI)\textendash{}peak-to-peak intervals (PPI) from 229 subjects were analyzed. This resulted in a positive correlation (rs=.993, root mean square deviation [RMSE]=23.04 ms, and normalized root mean square error [NRMSE]=0.012) between the PPI from FibriCheck and the RRI from the wearable ECG. There was no significant difference (P=.92) between these intervals. Conclusions: Our findings suggest that the most suitable method for the validation of an HR app is a simultaneous measurement of the HR by the smartphone app and an ECG system, compared on the basis of beat-to-beat analysis. This approach could lead to more correct assessments of the accuracy of HR apps.  [JMIR Mhealth Uhealth 2017;5(8):e129]},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (},
  file = {/Users/gerbrich/Zotero/storage/RB7T6CZN/Vandenberk et al. - 2017 - Clinical Validation of Heart Rate Apps Mixed-Meth.pdf;/Users/gerbrich/Zotero/storage/6JT5QWID/e129.html},
  journal = {JMIR mHealth and uHealth},
  language = {en},
  number = {8}
}

@article{vandeSchoot2017,
  title = {The {{GRoLTS}}-{{Checklist}}: {{Guidelines}} for Reporting on Latent Trajectory Studies},
  author = {{\noopsort{schoot}}{van de Schoot}, Rens and Sijbrandij, Marit and Winter, Sonja D. and Depaoli, Sarah and Vermunt, Jeroen K.},
  year = {2017},
  volume = {24},
  pages = {451--467},
  publisher = {{Routledge}},
  doi = {10/gdpcw9},
  eprint = {https://doi.org/10.1080/10705511.2016.1247646},
  file = {/Users/gerbrich/Zotero/storage/6VNXSNFK/van de Schoot et al. - 2017 - The GRoLTS-Checklist Guidelines for reporting on .pdf},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  number = {3}
}

@book{vanGent2018,
  title = {Analysing {{Noisy Driver Physiology Real}}-{{Time Using Off}}-the-{{Shelf Sensors}}: {{Heart Rate Analysis Software}} from the {{Taking}} the {{Fast Lane Project}}.},
  shorttitle = {Analysing {{Noisy Driver Physiology Real}}-{{Time Using Off}}-the-{{Shelf Sensors}}},
  author = {{\noopsort{gent}}{van Gent}, Paul and Farah, Haneen and Nes, Nicole and Arem, B.},
  year = {2018},
  month = nov,
  doi = {10.13140/RG.2.2.24895.56485},
  abstract = {We developed a heart rate analysis toolkit for use in collecting and analysing heart rate signals collected in noisy settings. In the paper we describe the development, functioning and (open source) availability of the software.

The paper has been submitted to the Journal of Open Research Software. Pre-print is available.},
  file = {/Users/gerbrich/Zotero/storage/DXVGNAGN/van Gent et al. - 2018 - Analysing Noisy Driver Physiology Real-Time Using .pdf}
}

@article{vanGent2019,
  title = {{{HeartPy}}: {{A}} Novel Heart Rate Algorithm for the Analysis of Noisy Signals},
  shorttitle = {{{HeartPy}}},
  author = {{\noopsort{gent}}{van Gent}, Paul and Farah, Haneen and {\noopsort{nes}}{van Nes}, Nicole and {\noopsort{arem}}{van Arem}, Bart},
  year = {2019},
  month = oct,
  volume = {66},
  pages = {368--378},
  issn = {1369-8478},
  doi = {10.1016/j.trf.2019.09.015},
  abstract = {Heart rate data are often collected in human factors studies, including those into vehicle automation. Advances in open hardware platforms and off-the-shelf photoplethysmogram (PPG) sensors allow the non-intrusive collection of heart rate data at very low cost. However, the signal is not trivial to analyse, since the morphology of PPG waveforms differs from electrocardiogram (ECG) waveforms and shows different noise patterns. Few validated open source available algorithms exist that handle PPG data well, as most of these algorithms are specifically designed for ECG data. In this paper we present the validation of a novel algorithm named HeartPy, useful for the analysis of heart rate data collected in noisy settings, such as when driving a car or when in a simulator. We benchmark the performance on two types of datasets and show that the developed algorithm performs well. Further research steps are discussed.},
  file = {/Users/gerbrich/Zotero/storage/RASADPVB/van Gent et al. - 2019 - HeartPy A novel heart rate algorithm for the anal.pdf;/Users/gerbrich/Zotero/storage/CTXNURCR/S1369847818306740.html},
  journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
  keywords = {Heart rate analysis,Human factors,Open source,Physiological signals,Signal analysis},
  language = {en}
}

@article{vanGent2019a,
  title = {{{HeartPy}}: {{A}} Novel Heart Rate Algorithm for the Analysis of Noisy Signals},
  shorttitle = {{{HeartPy}}},
  author = {{\noopsort{gent}}{van Gent}, Paul and Farah, Haneen and {\noopsort{nes}}{van Nes}, Nicole and {\noopsort{arem}}{van Arem}, Bart},
  year = {2019},
  month = oct,
  volume = {66},
  pages = {368--378},
  issn = {1369-8478},
  doi = {10.1016/j.trf.2019.09.015},
  abstract = {Heart rate data are often collected in human factors studies, including those into vehicle automation. Advances in open hardware platforms and off-the-shelf photoplethysmogram (PPG) sensors allow the non-intrusive collection of heart rate data at very low cost. However, the signal is not trivial to analyse, since the morphology of PPG waveforms differs from electrocardiogram (ECG) waveforms and shows different noise patterns. Few validated open source available algorithms exist that handle PPG data well, as most of these algorithms are specifically designed for ECG data. In this paper we present the validation of a novel algorithm named HeartPy, useful for the analysis of heart rate data collected in noisy settings, such as when driving a car or when in a simulator. We benchmark the performance on two types of datasets and show that the developed algorithm performs well. Further research steps are discussed.},
  file = {/Users/gerbrich/Zotero/storage/4LE97TE5/S1369847818306740.html},
  journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
  keywords = {Heart rate analysis,Human factors,Open source,Physiological signals,Signal analysis},
  language = {en}
}

@misc{VeritasHealthInnovation,
  title = {Covidence Systematic Review Software},
  address = {{Melbourne, Australia}},
  collaborator = {{Veritas Health Innovation}}
}

@article{Wallace2010,
  title = {Semi-Automated Screening of Biomedical Citations for Systematic Reviews},
  author = {Wallace, Byron C. and Trikalinos, Thomas A. and Lau, Joseph and Brodley, Carla and Schmid, Christopher H.},
  year = {2010},
  month = jan,
  volume = {11},
  pages = {55},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-55},
  abstract = {Systematic reviews address a specific clinical question by unbiasedly assessing and analyzing the pertinent literature. Citation screening is a time-consuming and critical step in systematic reviews. Typically, reviewers must evaluate thousands of citations to identify articles eligible for a given review. We explore the application of machine learning techniques to semi-automate citation screening, thereby reducing the reviewers' workload.},
  file = {/Users/gerbrich/Zotero/storage/SHT7NWMP/Wallace et al. - 2010 - Semi-automated screening of biomedical citations f.pdf;/Users/gerbrich/Zotero/storage/7RW8GCQ2/1471-2105-11-55.html},
  journal = {BMC Bioinformatics},
  keywords = {abstrackr},
  number = {1}
}

@inproceedings{Wallace2012,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  month = jan,
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/JHVMNGCY/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {abstrackr,active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@inproceedings{Wallace2012a,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: {{Abstrackr}}},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT}} International Health Informatics Symposium},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2110363.2110464},
  isbn = {978-1-4503-0781-9},
  keywords = {active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  numpages = {6},
  place = {Miami, Florida, USA},
  series = {{{IHI}} '12}
}

@inproceedings{Wallace2012b,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2012},
  month = jan,
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/NPDGJ92Z/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@misc{WelcomePackages,
  title = {Welcome {$\cdot$} {{R}} Packages},
  file = {/Users/gerbrich/Zotero/storage/T895U3XN/r-pkgs.had.co.nz.html},
  howpublished = {http://r-pkgs.had.co.nz/}
}

@article{Westgate2019,
  title = {Revtools: {{An R}} Package to Support Article Screening for Evidence Synthesis},
  author = {Westgate, Martin J.},
  year = {2019},
  doi = {10.1002/jrsm.1374},
  journal = {Research Synthesis Methods}
}

@article{Whitesides2004,
  title = {Whitesides' {{Group}}: {{Writing}} a {{Paper}}},
  shorttitle = {Whitesides' {{Group}}},
  author = {Whitesides, G. M.},
  year = {2004},
  month = aug,
  volume = {16},
  pages = {1375--1377},
  issn = {0935-9648, 1521-4095},
  doi = {10.1002/adma.200400767},
  file = {/Users/gerbrich/Zotero/storage/HPYVSRXS/Whitesides - 2004 - Whitesides' Group Writing a Paper.pdf},
  journal = {Advanced Materials},
  language = {en},
  number = {15}
}

@book{Wilke,
  title = {Fundamentals of {{Data Visualization}}},
  author = {Wilke, Claus O.},
  abstract = {A guide to making visualizations that accurately reflect the data, tell a story, and look professional.},
  file = {/Users/gerbrich/Zotero/storage/LFPWTGRW/dataviz.html}
}

@misc{Winter2020,
  title = {Additional {{Information}}: {{Bayesian PTSD}}-{{Trajectory Analysis}} with {{Informed Priors}}},
  author = {Winter, Sonja D. and {\noopsort{schoot}}{van de Schoot}, Rens},
  year = {2020},
  month = feb,
  publisher = {{OSF}}
}

@article{Yu2008,
  title = {{{GAPscreener}}: {{An}} Automatic Tool for Screening Human Genetic Association Literature in {{PubMed}} Using the Support Vector Machine Technique},
  author = {Yu, Wei and Clyne, Melinda and Dolan, Siobhan M. and Yesupriya, Ajay and Wulf, Anja and Liu, Tiebin and Khoury, Muin J. and Gwinn, Marta},
  year = {2008},
  month = apr,
  volume = {9},
  pages = {205},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-9-205},
  abstract = {Synthesis of data from published human genetic association studies is a critical step in the translation of human genome discoveries into health applications. Although genetic association studies account for a substantial proportion of the abstracts in PubMed, identifying them with standard queries is not always accurate or efficient. Further automating the literature-screening process can reduce the burden of a labor-intensive and time-consuming traditional literature search. The Support Vector Machine (SVM), a well-established machine learning technique, has been successful in classifying text, including biomedical literature. The GAPscreener, a free SVM-based software tool, can be used to assist in screening PubMed abstracts for human genetic association studies.},
  journal = {BMC Bioinformatics},
  number = {1}
}

@article{Yu2008a,
  title = {{{GAPscreener}}: {{An}} Automatic Tool for Screening Human Genetic Association Literature in {{PubMed}} Using the Support Vector Machine Technique},
  author = {Yu, Wei and Clyne, Melinda and Dolan, Siobhan M. and Yesupriya, Ajay and Wulf, Anja and Liu, Tiebin and Khoury, Muin J. and Gwinn, Marta},
  year = {2008},
  month = apr,
  volume = {9},
  pages = {205},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-9-205},
  abstract = {Synthesis of data from published human genetic association studies is a critical step in the translation of human genome discoveries into health applications. Although genetic association studies account for a substantial proportion of the abstracts in PubMed, identifying them with standard queries is not always accurate or efficient. Further automating the literature-screening process can reduce the burden of a labor-intensive and time-consuming traditional literature search. The Support Vector Machine (SVM), a well-established machine learning technique, has been successful in classifying text, including biomedical literature. The GAPscreener, a free SVM-based software tool, can be used to assist in screening PubMed abstracts for human genetic association studies.},
  journal = {BMC Bioinformatics},
  number = {1}
}

@dataset{Yu2017a,
  title = {Data Sets for {{FASTREAD}}},
  author = {Yu, Zhe and Kraft, Nicholas and Menzies, Tim},
  year = {2017},
  month = aug,
  publisher = {{Zenodo}},
  version = {v1.3}
}

@article{Yu2018,
  title = {Finding Better Active Learners for Faster Literature Reviews},
  author = {Yu, Zhe and Kraft, Nicholas and Menzies, Tim},
  year = {2018},
  month = mar,
  doi = {10.1007/s10664-017-9587-0},
  abstract = {Literature reviews can be time-consuming and tedious to complete. By cataloging and refactoring three state-of-the-art active learning techniques from evidence-based medicine and legal electronic discovery, this paper finds and implements FASTREAD, a faster technique for studying a large corpus of documents, combining and parametrizing the most efficient active learning algorithms. This paper assesses FASTREAD using datasets generated from existing SE literature reviews (Hall, Wahono, Radjenovi{\'c}, Kitchenham et al.). Compared to manual methods, FASTREAD lets researchers find 95\% relevant studies after reviewing an order of magnitude fewer papers. Compared to other state-of-the-art automatic methods, FASTREAD reviews 20\textendash{}50\% fewer studies while finding same number of relevant primary studies in a systematic literature review.},
  file = {/Users/gerbrich/Zotero/storage/MTLEK8XH/Yu et al. - 2018 - Finding better active learners for faster literatu.pdf},
  journal = {Empirical Software Engineering}
}

@article{Yu2018a,
  title = {Finding Better Active Learners for Faster Literature Reviews},
  author = {Yu, Zhe and Kraft, Nicholas A. and Menzies, Tim},
  year = {2018},
  month = mar,
  volume = {23},
  pages = {3161--3186},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9587-0},
  file = {/Users/gerbrich/Zotero/storage/ACKMBK39/Yu et al. - 2018 - Finding better active learners for faster literatu.pdf},
  journal = {Empirical Software Engineering},
  number = {6}
}

@article{Yu2019,
  title = {{{FAST2}}: {{An}} Intelligent Assistant for Finding Relevant Papers},
  shorttitle = {{{FAST2}}},
  author = {Yu, Zhe and Menzies, Tim},
  year = {2019},
  month = apr,
  volume = {120},
  pages = {57--71},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.11.021},
  abstract = {Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. Finding relevant papers can be hard due to the huge amount of candidates provided by search. FAST2 is a novel tool for assisting the researchers to find the next promising paper to read. This paper describes FAST2 and tests it on four large systematic literature review datasets. We show that FAST2 robustly optimizes the human effort to find most (95\%) of the relevant software engineering papers while also compensating for the errors made by humans during the review process. The effectiveness of FAST2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers\textemdash{}which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found\textemdash{}which helps the reviewer decide when to stop the review; (3) a novel human error correction algorithm\textemdash{}which corrects a majority of human misclassifications (labeling relevant papers as non-relevant or vice versa) without imposing too much extra human effort.},
  file = {/Users/gerbrich/Zotero/storage/R65D4TB5/Yu and Menzies - 2019 - FAST2 An intelligent assistant for finding releva.pdf;/Users/gerbrich/Zotero/storage/ITB4FS2M/S0957417418307413.html},
  journal = {Expert Systems with Applications},
  keywords = {Active learning,Literature reviews,Relevance feedback,Selection process,Semi-supervised learning,Text mining},
  language = {en}
}

@inproceedings{Zhang2004,
  title = {The {{Optimality}} of {{Naive Bayes}}},
  booktitle = {Proceedings of the {{Seventeenth International Florida Artificial Intelligence Research Society Conference}}, {{FLAIRS}} 2004},
  author = {Zhang, Harry},
  year = {2004},
  month = jan,
  volume = {2},
  abstract = {Naive Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classifica- tion is surprising, because the conditional independence assumption on which it is based, is rarely true in real- world applications. An open question is: what is the true reason for the surprisingly good performance of naive Bayes in classification? In this paper, we propose a novel explanation on the superb classification performance of naive Bayes. We show that, essentially, the dependence distribution; i.e., how the local dependence of a node distributes in each class, evenly or unevenly, and how the local dependen- cies of all nodes work together, consistently (support- ing a certain classification) or inconsistently (cancel- ing each other out), plays a crucial role. Therefore, no matter how strong the dependences among attributes are, naive Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences can- cel each other out. We propose and prove a sufficient and necessary conditions for the optimality of naive Bayes. Further, we investigate the optimality of naive Bayes under the Gaussian distribution. We present and prove a sufficient condition for the optimality of naive Bayes, in which the dependence between attributes do exist. This provides evidence that dependence among attributes may cancel out each other. In addition, we explore when naive Bayes works well.},
  file = {/Users/gerbrich/Zotero/storage/UP2XUMJR/Zhang - The Optimality of Naive Bayes.pdf},
  keywords = {model}
}

@misc{zotero-10,
  title = {{{JMU}} - {{Clinical Validation}} of {{Heart Rate Apps}}: {{Mixed}}-{{Methods Evaluation Study}} | {{Vandenberk}} | {{JMIR mHealth}} and {{uHealth}}},
  file = {/Users/gerbrich/Zotero/storage/FBB83KZF/e129.html},
  howpublished = {https://mhealth.jmir.org/2017/8/e129/}
}

@misc{zotero-172,
  title = {Resources to Learn {{Git}}},
  file = {/Users/gerbrich/Zotero/storage/43UL7T36/try.github.io.html},
  howpublished = {http://try.github.io/}
}

@misc{zotero-192,
  title = {Introduction to Cowplot},
  file = {/Users/gerbrich/Zotero/storage/MUCG8SDL/introduction.html},
  howpublished = {https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html}
}

@misc{zotero-196,
  title = {Scientific {{Journal}} and {{Sci}}-{{Fi Themed Color Palettes}} for Ggplot2},
  file = {/Users/gerbrich/Zotero/storage/VZB4KZVX/ggsci.html},
  howpublished = {https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html\#introduction}
}

@misc{zotero-206,
  title = {Adding {{GIF}} Animations - {{A}} Minimal Example Website Using Blogdown},
  howpublished = {https://blogdown-demo.rbind.io/2018/01/31/gif-animations/},
  language = {en-us}
}

@misc{zotero-208,
  title = {Adding {{GIF}} Animations - {{A}} Minimal Example Website Using Blogdown},
  howpublished = {https://blogdown-demo.rbind.io/2018/01/31/gif-animations/},
  language = {en-us}
}

@misc{zotero-238,
  title = {Knitr - {{LaTeX Reference}} and {{Guides}}},
  file = {/Users/gerbrich/Zotero/storage/NDBDZB9M/Knitr.html},
  howpublished = {https://learn.sharelatex.com/learn/Knitr}
}

@misc{zotero-254,
  title = {Machine Learning Algorithms for Systematic Review: Reducing Workload in a Preclinical Review of Animal Studies and Reducing Human Screening Error | {{Systematic Reviews}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/FTXIT6LW/s13643-019-0942-7.html},
  howpublished = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-0942-7}
}

@misc{zotero-263,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}},
  file = {/Users/gerbrich/Zotero/storage/GYHDPIKS/MANUAL.html},
  howpublished = {https://pandoc.org/MANUAL.html\#definition-lists}
}

@article{zotero-274,
  type = {Article}
}

@misc{zotero-298,
  title = {Is It Time to Trust the Robots? {{The}} Reliability and Usability of Machine Learning Tools for Screening in Systematic Reviews | {{Colloquium Abstracts}}},
  shorttitle = {Is It Time to Trust the Robots?},
  file = {/Users/gerbrich/Zotero/storage/S2WIJV9A/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening.html},
  howpublished = {/2019-santiago/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening},
  language = {en}
}

@misc{zotero-313,
  title = {{{EPPI}}-{{Reviewer}} 4: Systematic Review Software},
  file = {/Users/gerbrich/Zotero/storage/SYGD76RF/Default.html},
  howpublished = {https://eppi.ioe.ac.uk/CMS/Default.aspx?alias=eppi.ioe.ac.uk/cms/er4\&},
  keywords = {eppi}
}

@misc{zotero-32,
  title = {{{BMC Bioinformatics}}},
  abstract = {BMC Bioinformatics is part of the BMC series which publishes subject-specific journals focused on the needs of individual research communities across all areas ...},
  file = {/Users/gerbrich/Zotero/storage/I6FRGWY9/preparing-your-manuscript.html},
  howpublished = {https://bmcbioinformatics.biomedcentral.com/submission-guidelines/preparing-your-manuscript},
  journal = {BMC Bioinformatics},
  language = {en}
}

@misc{zotero-349,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review | {{Systematic Reviews}} | {{Full Text}}},
  file = {/Users/gerbrich/Zotero/storage/HC8TLNZ2/s13643-016-0263-z.html},
  howpublished = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-016-0263-z}
}

@misc{zotero-351,
  title = {Completing a Full Systematic Review in under Two Weeks: Processes, Barriers and Facilitators | {{The}} 26th {{Cochrane Colloquium}}},
  file = {/Users/gerbrich/Zotero/storage/6ZPXMXSL/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators.html},
  howpublished = {https://colloquium2019.cochrane.org/abstracts/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators},
  keywords = {2weeks}
}

@misc{zotero-358,
  title = {{{MDX SLR Tool}}},
  file = {/Users/gerbrich/Zotero/storage/P6GXB2WY/about.html},
  howpublished = {http://ta.mdx.ac.uk/slr/about/}
}

@misc{zotero-383,
  title = {{{SyRF}}: {{Systematic Review Facility}} | {{SyRF}}: {{Systematic Review Facility}}},
  file = {/Users/gerbrich/Zotero/storage/8ALRBI87/syrf.org.uk.html},
  howpublished = {http://syrf.org.uk/}
}

@misc{zotero-389,
  title = {6.2. {{Feature}} Extraction \textemdash{} Scikit-Learn 0.22.1 Documentation},
  file = {/Users/gerbrich/Zotero/storage/6Y956NI9/feature_extraction.html},
  howpublished = {https://scikit-learn.org/stable/modules/feature\_extraction.html}
}

@misc{zotero-436,
  title = {{{DistillerSR}}},
  address = {{Ottowa, Canada}},
  howpublished = {Evidence Partners}
}

@misc{zotero-5773,
  title = {({{PDF}}) {{How}} to {{Read Less}}: {{Better Machine Assisted Reading Methods}} for {{Systematic Literature Reviews}}},
  file = {/Users/gerbrich/Zotero/storage/5MHXQG65/311586326_How_to_Read_Less_Better_Machine_Assisted_Reading_Methods_for_Systematic_Literature_Re.html},
  howpublished = {https://www.researchgate.net/publication/311586326\_How\_to\_Read\_Less\_Better\_Machine\_Assisted\_Reading\_Methods\_for\_Systematic\_Literature\_Reviews}
}

@misc{zotero-77,
  title = {Smoothing and {{Differentiation}} of {{Data}} by {{Simplified Least Squares Procedures}}. | {{Analytical Chemistry}}},
  file = {/Users/gerbrich/Zotero/storage/KHJHHW5K/ac60214a047.html},
  howpublished = {https://pubs.acs.org/doi/abs/10.1021/ac60214a047}
}

@misc{zotero-84,
  title = {Style Guide {$\cdot$} {{Advanced R}}.},
  file = {/Users/gerbrich/Zotero/storage/5DSF5I9C/Style.html},
  howpublished = {http://adv-r.had.co.nz/Style.html}
}

@misc{zotero-90,
  title = {{{ResearchGate}}},
  file = {/Users/gerbrich/Zotero/storage/USBFJ2DS/download.html},
  howpublished = {https://www.researchgate.net/publication/328654252\_Analysing\_Noisy\_Driver\_Physiology\_Real-Time\_Using\_Off-the-Shelf\_Sensors\_Heart\_Rate\_Analysis\_Software\_from\_the\_Taking\_the\_Fast\_Lane\_Project/link/5bdab2c84585150b2b959d13/download}
}

@preamble{ "\newcommand{\noopsort}[1]{} " }

