---
title: "Manuscxript drafts"
author: "Gerbrich Ferdinands"
date: "1/14/2020"
output: pdf_document
bibliography: asreview.bib
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(knitr)
library(xtable)
library(kableExtra)
# load model parameters 
source("asr-parameters.R")
```


```{r}
# configurations table 
tab <- lapply(words, FUN = function(x) paste(x, collapse = ", "))
tab <- do.call(rbind, tab)
# remove row training data (28.02)
colnames(tab) <- c("Configurations")
tab <- tab[-4,]
```

```{r}
# combinations table
params[[4]] <- NULL # remove training data
combins <- expand.grid(params) %>%
  arrange(model, feature_extraction)

# rows to drop
droprows <- combins$model == "nb" & combins$feature_extraction != "tfidf"
combins <- combins[!droprows, ]
``` 
# Analysis strategy
Goal: evaluate performance of different models of the ASReview tool. 



# The software 
ASReview takes the following parameters/arguments: 

<!-- - a model -->
<!-- - a query strategy -->
<!-- - a balance strategy -->
<!-- - a feature extraction strategy -->

```{r}
kable(tab, "latex", booktabs = TRUE, col.names = "Configurations") %>%
  kable_styling(full_width = FALSE) %>%
  column_spec(2, width = "30em") 
```

Use these inputs to predict relevance of papers. 

## Stage 1: hyperparameter optimization
We are going to test 5 models on 5 different datasets. 

### Datasets

#### ptsd
#### ace
#### hall 
#### nagtegaal - van PhD van Lars
#### medische van Jan

### Models
- Naive Bayes 
- Random Forests
- Support Vecor Machine 
- Logistic Regression
- Dense Neural Network

Or, more specific: 

```{r}
tabbie <-
  combins %>%
  filter(query_strategy == "max",
         feature_extraction %in% c("doc2vec", "tfidf") & model %in% c("dense_nn","svm", "nb", "rf", "lr")) %>%
  select(-query_strategy)

tabbie <- tabbie[c(1,3,5,6,9),]
row.names(tabbie) <- NULL
tabbie %>%
  kable(col.names = names(words)[1:3], format = "latex")
```

The other parameters remain fixed over the 5 models:

- Query Strategy = max
- Balance Strategy = triple
- n_instances=10 (number of papers each query)
- n_prior_included = 5
- n_prior_excluded = 5

### Hyperparameters
Every model has its own set of hyperparameters:


### Optimization
The hyperparameters are optimized on the 5 datasets in three different ways: 

- 1 on 1: maximum performance 
- 4 on 1: cross-validation 
- 5 on 1: more data = more better? 

This results $(5+5+1)*5$ sets of hyperparameters. 

## Stage 2: simulation
for every for every model (5), for every dataset (5) and for every set of optimized hyperparameters (3), a simulation study is performed. From these $5*5*3=75$ simulation studies, performance of the different models is evaluated.


### Outcomes 
Several metrics are used to compare performance of different models over datasets, 

| Dataset   |   | Naive Bayes | Random Forests | Support Vector Machine | Logistic Regression | Dense Neural Network |  
| --------- | - | --- | --- | --- | --- | ---|
| ptsd      |   | ?   |     |     |    |     | 
| ace       |   | ?   |     |     |    |     | 
| hall      |   | ?   |     |     |    |     | 
| nagtegaal |   | ?   |     |     |    |     | 
| ....      |   | ?   |     |     |    |     | 


? How to compare outcomes of 3 different optimization strategies? 


# Appendix A - list of definitions
Machine learning algorithms cannot predict the relevance of abstracts from the raw texts as they are. 
The content of the texts needs to be transformed into numerical representations. The processs of transforming texts to numerical feature vectors is called word embeddings. 

A classical example of word embeddings is 'bag of words'. For each each text, the number of occurrences of each word is stored. This leads to n features, where n is the number of distinct words in the texts. [@scikit-learn]

Word embeddings allows ASReview to predict relevance of abstracts from the features of abstracts of which relevance is known. 

corpus = all the text: 

ASReview implements several feature extraction strategies. The following will be compared: 

The model is typically a learning algorithm used to predict the relevance of text.


Active learning = 
increasing classification performance with every query. 
The query strategy determines the way unlabeled papers are queried to the researcher. 

[@modAL2018]


### Feature Extraction Strategies 
split_ta = overall hyperparameter 

#### TF-IDF
The bag-of-words method is simplistic and will highly value often occuring but otherwise meaningless words such as "and". 

Term-frequency Inverse Document Frequency [@Ramos2003] circumvents this problem by adjusting a term frequency in a text with the inverse docuement frequency, the frequency of a given word in the entire corpus.

##### hyperparameters 
    ngram_max: int
            Can use up to ngrams up to ngram_max. For example in the case of
            ngram_max=2, monograms and bigrams could be used.


#### Doc2Vec
Predicts words from context. 
Aims at capturing the relations between word (man-woman, king-queen).
[@Le2014]. Using a neural network. 

using Continuous Bag-of-Words (CBOW), Skip-Gram model, ....
Word vector _W_ and extra: document vector _D_, trained to predict words in the text.


From gensim [@Rehurek2010].

        Arguments
        ---------
        vector_size: int
            Output size of the vector.
        epochs: int
            Number of epochs to train the doc2vec model.
        min_count: int
            Minimum number of occurences for a word in the corpus for it to
            be included in the model.
        workers: int
            Number of threads to train the model with.
        window: int
            Maximum distance over which word vectors influence each other.
        dm_concat: int
            Whether to concatenate word vectors or not.
            See paper for more detail.
        dm: int
            Model to use.
            0: Use distribute bag of words (DBOW).
            1: Use distributed memory (DM).
            2: Use both of the above with half the vector size and concatenate
            them.
        dbow_words: int
            Whether to train the word vectors using the skipgram metho
            
            
#### SBERT

BERT-base model with mean-tokens pooling [@Reimers2019]

#### embeddingIdf 
This model averages the weighted word vectors of all the words in the text,
in order to get a single feature vector for each text. The weights are provided by the inverse document frequencies

### Models 
#### Naive Bayes 
Naive Bayes assumes all features are independent given the class value. [@Zhang2004] 

ASReview uses the ` MultinomialNB` from the scikit-learn package [@scikit-learn], that implements the naive Bayes algorithm for multinomially distributed data. 
` nb`

Hyperparameters

  * alpha - accounts for features not present in learning samples and prevents zero probabilities in further computations. 

#### Random Forests
A number of decision trees are fit on bootstrapped samples of the original data, 
[@Breiman2001]
RandomForestClassifier from sklearn 

Arguments
        ---------
        n_estimators: int
            Number of estimators.
        max_features: int
            Number of features in the model.
        class_weight: float
            Class weight of the inclusions.
        random_state: int, RandomState
            Set the random state of the RNG.
        """

#### Support Vector Machine

#### Logistic Regression 

#### __Dense Neural Network__

### Query Strategies 
* Max - Choose the most likely samples to be included according to the model
* Uncertainty - choose the most uncertain samples according to the model (i.e. closest to 0.5 probability) [@Lewis1994] 
* Random - randomly selects abstracts with no regard to model assigned probabilities. 
* Cluster - Use clustering after feature extraction on the dataset. Then the highest probabilities within random clusters are sampled

The following combinations are simulated: 

* cluster
* max
* cluster * random 
* cluster * uncertainty
* max * cluster
* max * random
* max * uncertainty

### Balance Strategies 

### amount of training data

* n_instances = number of papers queried each query
* n_queries = number of queries
* n_prior_included: 5
* n_prior_excluded:

# Combinations 

This leads to  `r nrow(combins)` combinations of configurations. 

- Naive bayes only goes with tfidf feature extraction.
- For the feature extraction strategies we will focus on doc2vec and tfidf. (but will compute all 4)
- This leads to 3 * 7  * 4 * 3 + 1 * 7 * 1 * 3 =  273 combinations. 

See appendix A for a table containing all 273 combinations. 

# Performance metrics
Tradeoff: identifying all relevant papers and reducing workload. 

What is more important: recall or precision? 

Recall more highly valued than precision. 

What about class imbalance? 


#### RRF
Amount of relevant references found after having screened a certain percentage of the total number of abstracts.


#### Work saved over sampling (WSS)
Indicates how much time can be saved, at a given level of recall.
WSS is in terms of the percentage of abstracts that don't have to be screened by the researcher. 
Typically, WSS is measured at a recall of 0.95. 

$$\texttt{WSS} = \frac{TN + FN}{N} - (1- recall ) $$ 

#### Raoul
#### Utility? 
#### F-measure 
#### ROC/AUC



# Appendix B - combinations

```{r}
names(combins) <- c("Model", "Query Strategy", "Feature extraction strategy") #, "Training data [included/excluded]")
combins %>%
  kable(row.names = FALSE, format = "latex", longtable = TRUE, booktabs = TRUE) %>%
  kable_styling(latex_options = c("repeat_header")) 
```


# References 