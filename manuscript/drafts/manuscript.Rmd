---
title: "Manuscript drafts"
author: "Gerbrich Ferdinands"
date: "1/14/2020"
output: 
  pdf_document:
    citation_package: biblatex
bibliography: asreview.bib

---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(knitr)
library(xtable)
library(kableExtra)
library(captioner)
library(glue)
library(rbbt)

# load model parameters 
source("asr-parameters.R")

#pathRdata <- "../../R/data/"
pathRdata <- "data/"

```


```{r}
# configurations table 
tab <- lapply(words, FUN = function(x) paste(x, collapse = ", "))
tab <- do.call(rbind, tab)
# remove row training data (28.02)
colnames(tab) <- c("Configurations")
tab <- tab[-4,]
```

```{r}
# combinations table
params[[4]] <- NULL # remove training data
combins <- expand.grid(params) %>%
  arrange(model, feature_extraction)

# rows to drop
droprows <- combins$model == "nb" & combins$feature_extraction != "tfidf"
combins <- combins[!droprows, ]
``` 


```{r all tables, results = 'hide'}
table_nums <- captioner(prefix = "Table")
# table  on datasets statistics
table_nums(name = "datasets", caption = "Statistics on datasets from original systematic reviews.")
```

#### Introduction 
#### Methods
A convenience sample of 5 existing systematic reviews on varying topics was collected.

#### Results
#### Discussion


\newpage
# Introduction
Systematic Reviews (SR's) are booming - but they are a lot of work
Various machien learning tools have been proposed to reduce workload in abstract screening. 

- objectives - to demonstrate effectiveness of ml algorithm in reducing abstract classification for systematic reviews 
- justification - 
- background 
- guidance to reader 
- summary/conclusion

A SR can be divided into phases. 
Everything starts with a **systematic search**, leading to
then citation screening is performed, then full-text screening
[@PRISMA-PGroup2015]


What must be the objective of our tool?:
It is the tedious task citation screening part where loads of time can be saved. 

models are designed in a 'realistic' way (you have some inclusions)

Selecting papers is a two-step process: abstract & fulltext screening


RQ1 

## active learning for systematic reviews 
achine learning algorithms cannot predict the relevance of abstracts from the raw texts as they are. 
The content of the texts needs to be transformed into numerical representations. The processs of transforming texts to numerical feature vectors is called word embeddings. 

A classical example of word embeddings is 'bag of words'. For each each text, the number of occurrences of each word is stored. This leads to n features, where n is the number of distinct words in the texts. [@scikit-learn]

Word embeddings allows ASReview to predict relevance of abstracts from the features of abstracts of which relevance is known. 

corpus = all the text: 

ASReview implements several feature extraction strategies. The following will be compared: 

The model is typically a learning algorithm used to predict the relevance of text.

Active learning = 
increasing classification performance with every query. 
The query strategy determines the way unlabeled papers are queried to the researcher. 

[@modAL2018]

- pool unlabeled abstracts $\mathcal{U}$ 
- labeled data set $\mathcal{L}$, 
- instance $x$, label $y$

- utility measure $\phi_A(\cdot)$
- $x^*_A$ best query instance according to $\phi_A(\cdot)$ 

```{}
while 

```

## Background
[@Yu2018a], [@Yu2019] simulated 32 svm classifiers, on software engineering. 
A popular classifier is SVM 
In terms of Yu et al, we adopt .CT. 

Our extensions is that we try different classifiers, on more datasets. 

\newpage
# Methods
Goal: evaluate performance of different models of the ASReview tool. 
The screening process is simulated using ASReview, seeing if the original inclusions replicate. 
What would happen if the citation screening would have been performed using asreview? 

## Datasets

The algorithm will be tested on five systematic reviews from various research areas. 
test datasets serve as systematic search results, then perform active learning to detect inclusions. 

#### ace 
A dataset from a collection of systematic reviews on drug efficacy from the medical sciences [@Cohen2006]. 
This dataset is on a systematic review 

efficacy of Angiotensin-converting enzyme (ACE) inhibitors.
The ace dataset comes from a study on reducing workload in systematic reviews  [@Cohen2006].
The ace dataset is on the efficacy of 

drug class 
The ACEInhibitors dataset from the study by [@Cohen2006].
The study includes several data sets from the medical sciences, one of them is on ACE inhibitors.

a machine learning-based citation classification tool to reduce workload in systematic reviews of drug class efficacy. 

WSS@95% = 56.61 in [@Cohen2006]. (5x2 crossvalidation). Can we beat this? 
The data

#### software 
A review on fault prediction in software engineering by [@Hall2012]. 
The dataset is reviewed from [@Yu2018a] who collected datasets on literature reviews from the software engineering field. 

#### nudging
review [@Nagtegaal2019]
The data [@Nagtegaal2019a]

Difference in 18 inclusions = systematic reviews. to exclude/include? 

#### ptsd
A review [@vandeSchoot2018] on longitudinal studies on posttraumatic stress symptoms assessed after exposure to trauma. 
The corresponding dataset [@Winter2020].  
 
#### Wilson
The review [@Appenzeller-Herzog2019]
The dataset [@Appenzeller-Herzog2020]

Statistics on the SRs can be found in `r table_nums("datasets", display = "cite")`. 
All datasets accompanying the systematic reviews are openly published. 
The datasets contain information on all citations obtained in the search strategy and which citations were included in the systematic review. 

For every SR, the raw datafiles were preprocessed into a test dataset.

These test datasets contain authors, title, abstract and annotation of whether the entry was included in the final review or not (0/1). 

Entries with missing abstracts were removed. 
Duplicate entries were removed. 
Preprocessing scripts can be found on the GitHub^[https://github.com/GerbrichFerdinands/asreview-thesis]

```{r}
# datasets table
load(paste0(pathRdata, "datasets_table.Rdata"))

nstudies <- function(all, datastage, col){
}

nostudies <- function(all, set, stage){
  sapply(all, function(x) x[set, stage])
}
inclrate <- function(all, set){
  sapply(all, function(x) round(x[set,"incl"]/x[set,"search"]*100,2))
}


datastats <- 
  tibble(Dataset = names(all), 
       #Citation = NA, # maybe add footnote citation with kableExtra i.o. this.
       # paper 
       `candidates_paper` = nostudies(all, "paper", "search"), 
       #`fulltext_paper` = nostudies(all, "paper", "ftext"),
       `incl_paper` = nostudies(all, "paper", "incl"),
       `inclrate_paper` = inclrate(all, "paper"),
       # test set 
       `candidates_test` =  nostudies(all, "test", "search"), 
       #`fulltext_test` =  nostudies(all, "test", "ftext"), 
       `incl_test` =  nostudies(all, "test", "incl"), 
       `inclrate_test` = inclrate(all, "test")
       ) 
```


`r table_nums("datasets")`
```{r}
datastats %>%
  kable(format = "latex",
        booktabs = TRUE,
        col.names = c("Dataset", rep(c("Candidate studies", 
                                       #"Studies selected for fulltext screening", 
                                       "Final inclusions", 
                                       "Inclusion rate (%)"),2))) %>%
  kable_styling(full_width = TRUE) %>%
  add_header_above(c("", "Original study" = 3, "Test collection" = 3))
```

The inclusion rate is ... data is imbalanced. 
what is the philosophy
False negatives must be avoided ... 
The cost of a false negative outweighs the cost of a false positive. 
Note that we assume the oracle/original user to hold the truth. 
This is of course not always the case. 

## Models 
Now, we build five different active learning models. 

we have a model $M$ using a classifier $c$ with (hyper)parameters $h$, performing an automated systematic review on SR dataset $d$. 

We are going to test 5 models on 5 different datasets. 

classifier - feature extraction - query strategy - balance strategy

BTMD - 

#### Classifiers
- Naive Bayes (B)
- Random Forests (R)
- Support Vecor Machine (S)
- Logistic Regression (L)
- Dense Neural Network (N)

#### Feature extraction strategies
To compute relevancy of a paper, we use the title and abstract 
- baseline model: 


### Balance strategies
triple = 
The other parameters remain fixed over the 5 models:


- Query Strategy = max
- Balance Strategy = triple
- n_instances=10 (number of papers each query)
- n_prior_included = 5
- n_prior_excluded = 5

## Simulations
The original systematic review is reproduced using five different models.

A simulation is of one model on one dataset. 

The model starts with 5 inclusions and 5 exclusions, randomly selected. The simulation is repeated for 10 trials $t$. 
We use 5 inclusions and exclusions as we assume the researcher has some prior knowledge on this. 
The researcher has some prior knowledge about the pool, some papers ought to be included in the SR
for every for every model (5), for every dataset (5) and for every set of optimized hyperparameters (3), a simulation study consisting trials is performed. From these $5*5*3=75$ simulation studies, performance of the different models is evaluated.

Every simulation study consists of 10 trials, to account for the randomness of prior inclusions and exclusions. Results are aggregated (?)

assumptions
 
 - decisions of the original SR are **ground truth** (benchmark) (oracle)
 - binary classifications: relevant/irrelevant 

### Optimizing Hyperparameters 
Every model has its own hyperparameters. 
For every model, the hyperparameters are optimized three times, arriving at three versions of the model: 


# The software 
ASReview takes the following parameters/arguments: 

<!-- - a model -->
<!-- - a query strategy -->
<!-- - a balance strategy -->
<!-- - a feature extraction strategy -->

```{r}
tab %>%
kable(format = "latex", 
      booktabs = TRUE, 
      col.names = "Configurations") %>%
  kable_styling(full_width = TRUE)
```

Use these inputs to predict relevance of papers. 

### Stage 1: hyperparameter optimization

Or, more specific: 

```{r}
tabbie <-
  combins %>%
  filter(query_strategy == "max",
         feature_extraction %in% c("doc2vec", "tfidf") & model %in% c("dense_nn","svm", "nb", "rf", "lr")) %>%
  select(-query_strategy)

tabbie <- tabbie[c(1,3,5,6,9),]
row.names(tabbie) <- NULL

tabbie %>%
  kable(col.names = names(words)[c(1,3)], 
        format = "latex",
        booktabs = TRUE)
```



### Hyperparameters
Every model has its own set of hyperparameters:

### Optimization
The hyperparameters are optimized on the 5 datasets in three different ways: 

- 1 on 1: maximum performance 

$$ d = D $$
- 4 on 1: cross-validation 
$$ d \notin D$$ 

$$ D = {1, 2, 3, 4}$$

- 5 on 1: more data = more better? 

$$ d \in D $$

This results $(5+5+1)*5$ sets of hyperparameters. 



### Outcomes 
For each model, 
Several metrics are used to compare performance of different models over datasets, 

| Dataset   |   | Naive Bayes | Random Forests | Support Vector Machine | Logistic Regression | Dense Neural Network |  
| --------- | - | --- | --- | --- | --- | ---|
| ptsd      |   | ?   |     |     |    |     | 
| ace       |   | ?   |     |     |    |     | 
| hall      |   | ?   |     |     |    |     | 
| nagtegaal |   | ?   |     |     |    |     | 
| ....      |   | ?   |     |     |    |     | 


? How to compare outcomes of 3 different optimization strategies? 

## Evaluation

\newpage

# Results

\newpage
# Discussion


\newpage
# Appendix A - list of definitions


### Feature Extraction Strategies 
split_ta = overall hyperparameter 

#### TF-IDF
The bag-of-words method is simplistic and will highly value often occuring but otherwise meaningless words such as "and". 

Term-frequency Inverse Document Frequency [@Ramos2003] circumvents this problem by adjusting a term frequency in a text with the inverse docuement frequency, the frequency of a given word in the entire corpus.

##### hyperparameters 
    ngram_max: int
            Can use up to ngrams up to ngram_max. For example in the case of
            ngram_max=2, monograms and bigrams could be used.


#### Doc2Vec
Predicts words from context. 
Aims at capturing the relations between word (man-woman, king-queen).
[@Le2014]. Using a neural network. 

using Continuous Bag-of-Words (CBOW), Skip-Gram model, ....
Word vector _W_ and extra: document vector _D_, trained to predict words in the text.


From gensim [@Rehurek2010].

        Arguments
        ---------
        vector_size: int
            Output size of the vector.
        epochs: int
            Number of epochs to train the doc2vec model.
        min_count: int
            Minimum number of occurences for a word in the corpus for it to
            be included in the model.
        workers: int
            Number of threads to train the model with.
        window: int
            Maximum distance over which word vectors influence each other.
        dm_concat: int
            Whether to concatenate word vectors or not.
            See paper for more detail.
        dm: int
            Model to use.
            0: Use distribute bag of words (DBOW).
            1: Use distributed memory (DM).
            2: Use both of the above with half the vector size and concatenate
            them.
        dbow_words: int
            Whether to train the word vectors using the skipgram metho
            
            
#### SBERT

BERT-base model with mean-tokens pooling [@Reimers2019]

#### embeddingIdf 
This model averages the weighted word vectors of all the words in the text,
in order to get a single feature vector for each text. The weights are provided by the inverse document frequencies

### Models 
#### Naive Bayes 
Naive Bayes assumes all features are independent given the class value. [@Zhang2004] 

ASReview uses the ` MultinomialNB` from the scikit-learn package [@scikit-learn], that implements the naive Bayes algorithm for multinomially distributed data. 
` nb`

Hyperparameters

  * alpha - accounts for features not present in learning samples and prevents zero probabilities in further computations. 

#### Random Forests
A number of decision trees are fit on bootstrapped samples of the original data, 
[@Breiman2001]
RandomForestClassifier from sklearn 

Arguments
        ---------
        n_estimators: int
            Number of estimators.
        max_features: int
            Number of features in the model.
        class_weight: float
            Class weight of the inclusions.
        random_state: int, RandomState
            Set the random state of the RNG.
        """

#### Support Vector Machine

#### Logistic Regression 

#### __Dense Neural Network__

### Query Strategies 
* Max - Choose the most likely samples to be included according to the model
* Uncertainty - choose the most uncertain samples according to the model (i.e. closest to 0.5 probability) [@Lewis1994] 
* Random - randomly selects abstracts with no regard to model assigned probabilities. 
* Cluster - Use clustering after feature extraction on the dataset. Then the highest probabilities within random clusters are sampled

The following combinations are simulated: 

* cluster
* max
* cluster * random 
* cluster * uncertainty
* max * cluster
* max * random
* max * uncertainty

### Balance Strategies 

### amount of training data

* n_instances = number of papers queried each query
* n_queries = number of queries
* n_prior_included: 5
* n_prior_excluded:

# Combinations 

This leads to  `r nrow(combins)` combinations of configurations. 

- Naive bayes only goes with tfidf feature extraction.
- For the feature extraction strategies we will focus on doc2vec and tfidf. (but will compute all 4)
- This leads to 3 * 7  * 4 * 3 + 1 * 7 * 1 * 3 =  273 combinations. 

See appendix A for a table containing all 273 combinations. 

# Performance metrics
Tradeoff: identifying all relevant papers and reducing workload. 

What is more important: recall or precision? 

Recall more highly valued than precision. 

What about class imbalance? 


#### RRF
Amount of relevant references found after having screened a certain percentage of the total number of abstracts.


#### Work saved over sampling (WSS)
Indicates how much time can be saved, at a given level of recall.
WSS is in terms of the percentage of abstracts that don't have to be screened by the researcher. 
Typically, WSS is measured at a recall of 0.95. Reasonable because..

$$\texttt{WSS} = \frac{TN + FN}{N} - (1- recall) $$ 

#### Raoul
#### Utility? 
#### F-measure 
#### ROC/AUC

Is performance related to some characteristic (n, inclusion rate, ...)

## Cross-validation
Should give an accurate estimate of maximum performance / future systematic reviews to be performed. 


# Appendix B - combinations

```{r}
names(combins) <- c("Model", "Query Strategy", "Feature extraction strategy") #, "Training data [included/excluded]")
combins %>%
  kable(row.names = FALSE, format = "latex", longtable = TRUE, booktabs = TRUE) %>%
  kable_styling(latex_options = c("repeat_header")) 
```


# References 