[
["index.html", "A Minimal Book Example 1 thesis Ferdinands 1.1 Scripts", " A Minimal Book Example Yihui Xie 2020-02-18 1 thesis Ferdinands Goal: evaluate performance of different models of the ASReview tool. 1.0.1 Stage 1: hyperparameter optimization We are testing 5 models on 5 different datasets. Every model has its own set of hyperparameters. The hyperparameters are optimized on the 5 datasets in three different ways: 1 on 1: maximum performance 4 on 1: cross-validation 5 on 1: more data = more better? This results \\((5+5+1)*5\\) sets of hyperparameters. 1.0.2 Stage 2: simulation for every for every model (5), for every dataset (5) and for every set of optimized hyperparameters (3), a simulation study is performed. From these \\(5*5*3=75\\) simulation studies, performance of the different models is evaluated. 1.1 Scripts simulatie: script die optimalisatie genereert (config files.) script die .ini’s voor simulatie genereert. "],
["strategy.html", "2 Analysis strategy 2.1 Datasets 2.2 Models 2.3 Hyperparameters", " 2 Analysis strategy 2.1 Datasets ptsd ace hall nagtegaal - van PhD van Lars medische van Jan 2.2 Models (input table here?) Naive Bayes Random Forests Support Vecor Machine (doc2vec) Logistic Regression Dense Neural Network (doc2vec) The other parameters remain fixed over the 5 models: Feature extraction = tf-idf Query Strategy = max Balance Strategy = triple n_instances=10 (number of papers each query) n_prior_included = 5 n_prior_excluded = 5 mix_ratio = 0.95 (95% max, 5% random) 2.3 Hyperparameters "],
["my-first-simulation.html", "3 My first simulation 3.1 Methods 3.2 Results 3.3 Console output 3.4 Notes", " 3 My first simulation 3.1 Methods A simulation study was performed on the Nagtegaal dataset, using a model with the following configurations: Model = Naive Bayes Query Strategy = max_random Balance Strategy = Double n_instances=10 (number of papers each query) n_papers=2000 (shouldn’t I do all?) n_prior_included = 5 n_prior_excluded = 5 mix_ratio = 0.95 (95% max, 5% random) Hyperparameters default optimized Model alpha 3.822 3.511844 Balance a 2.155 0.254892 alpha 0.94 1.459081 b 0.789 0.394437 Feature ngram_max 1 2 split_ta 0 1 For the sake of evaluating the optimized hyperparameters, two simulations of five runs each were compared: one with default hyperparameters and with optimized hyperparameters. 3.2 Results Explanation of the plots come from the asreview-visualization repository. The optimized hyperparameters do not perform better than the default ones, this is probably due to the fact that the default hyperparameters have already been optimized in the past. It is therefore to know for which models this has already been done and which not! 3.2.0.1 Inclusions This figure shows the number/percentage of included papers found as a function of the number/percentage of papers reviewed. Initial included/excluded papers are subtracted so that the line always starts at (0,0). The quicker the line goes to a 100%, the better the performance. In the beginning, the model with default parameters finds inclusions quicker than the model with optimized hyperparamters. Only after reviewing 50% of the papers, the optimized hyperparameters outperform the default ones. 3.2.0.2 Discovery This figure shows the distribution of the number of papers that have to be read before discovering each inclusion. Not every paper is equally hard to find. The closer to the left, the better. 3.2.0.3 Limits This figure shows how many papers need to be read with a given criterion. A criterion is expressed as “after reading y % of the papers, at most an average of z included papers have been not been seen by the reviewer, if he is using max sampling.”. Here, y is shown on the y-axis, while three values of z are plotted as three different lines with the same color. The three values for z are 0.1, 0.5 and 2.0. The quicker the lines touch the black (y=x) line, the better. 3.3 Console output 3120 iterations ran overnight. 2191 was best performing with a loss of 0.1124 3.4 Notes Optimizing the hyperparameters with the Nagtegaal set and then running a simulation on the Nagtegaal dataset is using the data twice. This leads to overfitting. A next approach could be to perform some way of cross-validation, e.g. split the datasets in train and test datasets. There are two main optimization modes: passive and active learning. The first is used here and is relatively fast, the second is more computationally expensive. Of primary interest is the comparison of different model configurations in predictive performance. A simulation study can be performed with all possible configurations using the default hyperparameters. The results could be used to select model configurations that could possibly benefit from hyperparameter sreening. Second, we could investigate how much is to gain in predictive performance from optimizing the hyperparameters. For this, some cross-validation strategy should be used. Optimization can consist of two steps: first, optimization through passive learning can be performed, from which the best performing models can be selected for the second step: optimization through passive learning. 3.4.1 Possible Research Questions Which model configurations have good predictive performance? for what kind of data sets and under which circumstances? Does optimization of hyperparameters lead to substantial gain in predictive performance? How much and why? How do the hyperparameters relate to one another? What is the optimal way to tune the hyperparameters? to determine by cross-validation for example: optimize over a large number of datasets? or a different strategy? … … "],
["list-of-definitions.html", "A List of definitions", " A List of definitions "],
["references.html", "References", " References "]
]
